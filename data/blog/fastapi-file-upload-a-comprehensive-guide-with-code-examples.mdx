---
title: 'FastAPI File Upload: A Comprehensive Guide with Code Examples'
date: '2024-10-27'
lastmod: '2024-10-27'
tags: ['fastapi', 'python', 'file upload', 'api', 'backend']
draft: false
summary: 'Learn how to handle file uploads in FastAPI, including single and multiple file uploads, file size limitations, and how to save files to disk or cloud storage. This comprehensive guide provides code examples and best practices for building robust file upload functionality in your FastAPI applications.'
authors: ['default']
---

# FastAPI File Upload: A Comprehensive Guide with Code Examples

File upload functionality is a common requirement for many web applications. Whether you're building a social media platform, a document management system, or an image processing service, you'll need to handle file uploads efficiently and securely.  FastAPI, a modern, fast (high-performance), web framework for building APIs with Python, provides a simple and elegant way to implement file uploads.  This comprehensive guide will walk you through the process, covering single file uploads, multiple file uploads, file size limitations, and saving files to disk or cloud storage.

## Why FastAPI for File Uploads?

FastAPI shines in handling file uploads due to several reasons:

*   **Ease of Use:**  FastAPI simplifies the process of handling file uploads with intuitive syntax and built-in features.
*   **Performance:** FastAPI leverages asynchronous programming (async/await) to handle file uploads efficiently, resulting in better performance.
*   **Type Hints:**  FastAPI utilizes Python's type hints for data validation, ensuring that uploaded files meet your specified requirements.
*   **Automatic Documentation:**  FastAPI automatically generates interactive API documentation using OpenAPI and Swagger UI, including support for file upload endpoints.
*   **Integration:** FastAPI integrates seamlessly with other Python libraries and frameworks, making it easy to build complex file processing pipelines.

## Prerequisites

Before we begin, ensure you have the following installed:

*   **Python 3.7+:** FastAPI requires Python 3.7 or higher.
*   **FastAPI:**  Install using pip: `pip install fastapi`
*   **Uvicorn:** An ASGI server for running FastAPI applications: `pip install uvicorn`

## Single File Upload

Let's start with the simplest scenario: uploading a single file.

```python
from fastapi import FastAPI, File, UploadFile
from typing import Annotated

app = FastAPI()

@app.post("/uploadfile/")
async def create_upload_file(file: Annotated[UploadFile, File()]):
    return {"filename": file.filename}
```

**Explanation:**

1.  **Import necessary modules:** `FastAPI`, `File`, and `UploadFile` from `fastapi`. We also use `Annotated` from `typing` to correctly annotate the `file` parameter.
2.  **Create a FastAPI app:** `app = FastAPI()` initializes the FastAPI application.
3.  **Define the endpoint:**  `@app.post("/uploadfile/")` defines a POST endpoint at `/uploadfile/`.
4.  **Declare the `file` parameter:**  `file: UploadFile = File(...)` declares a parameter named `file` of type `UploadFile`. `File(...)` indicates that this parameter is a file to be uploaded.  `UploadFile` is a FastAPI class that represents the uploaded file.
5.  **Access file metadata:**  Inside the function, you can access the file's filename using `file.filename`. Other properties like `file.content_type` are also available.
6.  **Return the filename:**  The function returns a JSON response containing the filename.

**Running the code:**

1.  Save the code as `main.py`.
2.  Run the application using Uvicorn: `uvicorn main:app --reload`

You can now test the endpoint using a tool like `curl` or an API testing client like Postman.  In Postman, set the method to POST, the URL to `http://127.0.0.1:8000/uploadfile/`, and in the "Body" tab, select `form-data`.  Add a key named `file` (same as the parameter in your function), set the type to `File`, and select the file you want to upload.

**Important considerations for file handling:**

*   The `UploadFile` object does *not* automatically save the file to disk. You need to explicitly read the file content and save it.  We'll cover that in the next section.

## Saving the Uploaded File to Disk

To save the uploaded file to your server's file system, you'll need to read the file content and write it to a file. Here's an example:

```python
from fastapi import FastAPI, File, UploadFile, HTTPException
from typing import Annotated
import os
import shutil

app = FastAPI()

UPLOAD_DIR = "uploads"  # Directory to store uploaded files

if not os.path.exists(UPLOAD_DIR):
    os.makedirs(UPLOAD_DIR)

@app.post("/uploadfile/")
async def create_upload_file(file: Annotated[UploadFile, File()]):
    try:
        file_path = os.path.join(UPLOAD_DIR, file.filename)
        with open(file_path, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error saving file: {e}")
    finally:
        await file.close()

    return {"filename": file.filename, "filepath": file_path}
```

**Explanation:**

1.  **Import necessary modules:**  `os` for file system operations and `shutil` for efficient file copying. `HTTPException` for raising errors.
2.  **Define upload directory:** `UPLOAD_DIR` specifies the directory where you want to save the uploaded files. Create this directory if it doesn't exist.
3.  **Read file content and save to disk:**
    *   We construct the full file path using `os.path.join(UPLOAD_DIR, file.filename)`.
    *   We open the file path in binary write mode (`"wb"`) using a `with` statement to ensure the file is closed properly.
    *   `shutil.copyfileobj(file.file, buffer)` efficiently copies the content of the uploaded file (accessed through `file.file`, which is a file-like object) to the opened file on disk.
4.  **Error handling:**  A `try...except` block is used to catch potential errors during file saving.  If an error occurs, an `HTTPException` is raised with a 500 status code and a detailed error message.
5.  **File closing:** The `finally` block ensures that the uploaded file is closed (`await file.close()`), even if an error occurs. This is important to release resources.
6.  **Return filepath:** The function returns the filename and the saved filepath.

**Important Considerations:**

*   **File naming:**  It's crucial to implement proper file naming strategies to avoid name collisions and potential security vulnerabilities.  Consider using unique identifiers or timestamps in the filenames.
*   **Security:**  Sanitize filenames to prevent directory traversal attacks.  For example, remove or replace characters like `..` and `/`.
*   **File size limits:**  Implement file size limitations to prevent abuse and resource exhaustion.  We'll cover that next.
*   **Asynchronous operations:** While `shutil.copyfileobj` is blocking, in a production environment, you might consider using asynchronous file writing operations for even better performance, especially for large files.

## File Size Limitations

Limiting file sizes is essential to prevent denial-of-service attacks and manage storage resources.  FastAPI doesn't directly enforce file size limits. You need to implement this yourself.  Here's one way to do it:

```python
from fastapi import FastAPI, File, UploadFile, HTTPException
from typing import Annotated
import os
import shutil

app = FastAPI()

UPLOAD_DIR = "uploads"
MAX_FILE_SIZE = 10 * 1024 * 1024  # 10MB

if not os.path.exists(UPLOAD_DIR):
    os.makedirs(UPLOAD_DIR)

@app.post("/uploadfile/")
async def create_upload_file(file: Annotated[UploadFile, File()]):
    try:
        file_size = 0
        for chunk in iter(lambda: await file.read(1024), b''): # Read in chunks
            file_size += len(chunk)
            if file_size > MAX_FILE_SIZE:
                raise HTTPException(status_code=413, detail="File size exceeds the limit.")

            file_path = os.path.join(UPLOAD_DIR, file.filename)
            with open(file_path, "wb") as buffer:
                file.file.seek(0)  # Reset file pointer to the beginning
                shutil.copyfileobj(file.file, buffer)
            break  # Stop after first chunk, we just check filesize.
    except HTTPException as e:
        await file.close()  # Close the file to prevent resource leaks
        raise e
    except Exception as e:
        await file.close()
        raise HTTPException(status_code=500, detail=f"Error saving file: {e}")
    finally:
        await file.close()

    return {"filename": file.filename, "filepath": file_path}
```

**Explanation:**

1.  **Define `MAX_FILE_SIZE`:**  Sets the maximum allowed file size in bytes.
2.  **Check file size before saving:** We now read the file in chunks to check the file size, raising `HTTPException` if it exceeds `MAX_FILE_SIZE`.
    *   `file.read(1024)`: Reads the file in 1024-byte chunks. Reading in chunks is more memory-efficient, especially for large files.
    *   We calculate the `file_size`.
    *   If `file_size` exceeds `MAX_FILE_SIZE`, we raise an `HTTPException` with status code 413 (Request Entity Too Large).
3.  **Reset file pointer:** After reading chunks to check the file size, we use `file.file.seek(0)` to reset the file pointer to the beginning of the file. This ensures that `shutil.copyfileobj` reads the entire file content correctly.

**Key improvements for production:**

* **Stream saving:** Instead of reading the entire file into memory to calculate its size and *then* writing to disk, stream the data to disk chunk-by-chunk.  Check the cumulative size as you write. This significantly reduces memory usage. The current implementation just peeks at the first chunk.
* **Error handling:** Added error handling for the HTTPException to ensure that the file is properly closed to prevent resource leaks.

## Multiple File Uploads

FastAPI makes handling multiple file uploads straightforward.

```python
from fastapi import FastAPI, File, UploadFile
from typing import List, Annotated
import os
import shutil

app = FastAPI()

UPLOAD_DIR = "uploads"

if not os.path.exists(UPLOAD_DIR):
    os.makedirs(UPLOAD_DIR)


@app.post("/uploadfiles/")
async def create_upload_files(files: List[Annotated[UploadFile, File()]]):
    file_paths = []
    for file in files:
        try:
            file_path = os.path.join(UPLOAD_DIR, file.filename)
            with open(file_path, "wb") as buffer:
                shutil.copyfileobj(file.file, buffer)
            file_paths.append(file_path)
        except Exception as e:
            return {"message": f"There was an error uploading {file.filename}: {e}"}
        finally:
            await file.close()

    return {"filenames": [file.filename for file in files], "file_paths": file_paths}
```

**Explanation:**

1.  **Use `List[UploadFile]`:** The `files` parameter is now a `List[UploadFile]`, indicating that it's a list of uploaded files.  We use `typing.List` for type hinting.
2.  **Iterate through the files:**  We iterate through the `files` list and process each file individually, saving them to disk as before.
3.  **Return a list of filenames:** The function returns a list of filenames in the response.

**Testing:**

In Postman, set the method to POST, the URL to `http://127.0.0.1:8000/uploadfiles/`, and in the "Body" tab, select `form-data`. Add multiple keys named `files` (same as the parameter in your function), set the type to `File` for each, and select the files you want to upload.  The backend expects keys with the *same name*, unlike single file uploads.

## Uploading to Cloud Storage (e.g., AWS S3)

While saving files to disk is simple, it's often better to store uploaded files in cloud storage services like AWS S3, Google Cloud Storage, or Azure Blob Storage. Here's an example of uploading files to AWS S3 using the `boto3` library:

First, install `boto3`: `pip install boto3`

```python
from fastapi import FastAPI, File, UploadFile, HTTPException
from typing import Annotated
import boto3
import os
from io import BytesIO

app = FastAPI()

S3_BUCKET_NAME = "your-s3-bucket-name"  # Replace with your bucket name
S3_ACCESS_KEY = "your-s3-access-key"  # Replace with your access key
S3_SECRET_KEY = "your-s3-secret-key"  # Replace with your secret key
S3_REGION = "your-s3-region"  # Replace with your region

s3 = boto3.client(
    "s3",
    aws_access_key_id=S3_ACCESS_KEY,
    aws_secret_access_key=S3_SECRET_KEY,
    region_name=S3_REGION
)


@app.post("/uploadtos3/")
async def create_upload_file_s3(file: Annotated[UploadFile, File()]):
    try:
        contents = await file.read()
        file_object = BytesIO(contents)
        s3.upload_fileobj(file_object, S3_BUCKET_NAME, file.filename)

        return {"filename": file.filename, "message": "File uploaded successfully to S3"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error uploading to S3: {e}")
    finally:
        await file.close()
```

**Explanation:**

1.  **Install `boto3`:**  Make sure you have `boto3` installed.
2.  **Configure S3 credentials:** Replace the placeholder values for `S3_BUCKET_NAME`, `S3_ACCESS_KEY`, `S3_SECRET_KEY`, and `S3_REGION` with your actual AWS credentials. **Never hardcode your credentials directly in your code in a production environment. Use environment variables or a secrets management system.**
3.  **Create an S3 client:** `boto3.client("s3", ...)` creates an S3 client using your credentials.
4.  **Read file content:** `contents = await file.read()` reads the entire file content into memory.  For very large files, consider streaming the file directly to S3 instead.
5.  **Create in-memory file object:** The contents are converted into an in-memory file-like object using `BytesIO`.
6.  **Upload to S3:** `s3.upload_fileobj(file_object, S3_BUCKET_NAME, file.filename)` uploads the file to your S3 bucket.

**Important Considerations for S3 Uploads:**

*   **Security:**  Protect your AWS credentials carefully. Use IAM roles and policies to grant limited access to your S3 bucket.
*   **Streaming:**  For large files, consider using `s3.upload_fileobj` with a streamable file object to avoid loading the entire file into memory.  `aiobotocore` can enable asynchronus operations.
*   **Error Handling:**  Implement robust error handling to handle network issues, permission errors, and other potential problems.
*   **Pre-signed URLs:**  Consider using pre-signed URLs to allow clients to upload files directly to S3 without exposing your AWS credentials.

## Conclusion

FastAPI provides a clean and efficient way to handle file uploads in your Python APIs.  By understanding the concepts covered in this guide, including single and multiple file uploads, file size limitations, saving to disk, and uploading to cloud storage, you can build robust and scalable file upload functionality for your applications.  Remember to prioritize security, error handling, and efficient resource management when implementing file uploads in a production environment.