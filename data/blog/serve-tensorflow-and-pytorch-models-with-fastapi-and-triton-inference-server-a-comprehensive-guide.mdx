---
title: 'Serve TensorFlow & PyTorch Models with FastAPI & Triton Inference Server: A Comprehensive Guide'
date: '2024-10-27'
lastmod: '2024-10-27'
tags:
  [
    'machine learning',
    'mlops',
    'tensorflow',
    'pytorch',
    'fastapi',
    'triton inference server',
    'model serving',
    'api',
    'deployment',
    'inference',
  ]
draft: false
summary: 'Learn how to deploy your TensorFlow and PyTorch machine learning models using FastAPI and Triton Inference Server for scalable and efficient inference. This comprehensive guide includes code examples and best practices for serving ML models in production.'
authors: ['default']
---

# Serve TensorFlow & PyTorch Models with FastAPI & Triton Inference Server: A Comprehensive Guide

Deploying machine learning models to production can be a complex and challenging task. Choosing the right tools and architecture is crucial for building scalable, reliable, and efficient inference services. This guide provides a comprehensive walkthrough of how to serve TensorFlow and PyTorch models using FastAPI and Triton Inference Server. We'll cover everything from setting up your environment to building a robust API endpoint.

## Why FastAPI and Triton Inference Server?

Before diving into the implementation details, let's understand why FastAPI and Triton Inference Server are excellent choices for model serving:

- **FastAPI:** A modern, high-performance web framework for building APIs with Python 3.7+ based on standard Python type hints. Its key benefits include:

  - **Speed:** Built on Starlette and Pydantic, offering impressive performance.
  - **Ease of Use:** Intuitive API design with automatic data validation and serialization.
  - **Automatic Documentation:** Generates interactive API documentation (Swagger UI, ReDoc) out-of-the-box.
  - **Asynchronous Support:** Makes it suitable for handling concurrent requests efficiently.

- **Triton Inference Server:** A high-performance inference server developed by NVIDIA. It's designed to optimize and deploy AI models at scale in production. Its main advantages are:
  - **Model Management:** Supports multiple models and frameworks (TensorFlow, PyTorch, ONNX Runtime, TensorRT) on a single server.
  - **Dynamic Batching:** Aggregates multiple inference requests into a single batch to improve GPU utilization.
  - **Concurrent Execution:** Handles multiple concurrent requests efficiently.
  - **Health Endpoint:** Provides a health endpoint to monitor the health of the server and models.
  - **Metrics:** Exposes Prometheus metrics for monitoring performance.
  - **Framework Agnostic:** Supports various frameworks including TensorFlow, PyTorch, ONNX Runtime, and TensorRT.

By combining FastAPI's ease of use and API capabilities with Triton's high-performance inference, you can create a powerful and efficient model serving solution.

## Prerequisites

Before you begin, ensure you have the following installed:

- **Python 3.7+:** Python version 3.7 or later is required.
- **Docker:** Triton Inference Server is typically deployed within a Docker container.
- **pip:** Python package installer.

## Step 1: Setting up the Environment

Create a new project directory and a virtual environment:

```bash
mkdir fastapi-triton
cd fastapi-triton
python3 -m venv venv
source venv/bin/activate  # On Linux/macOS
# venv\Scripts\activate  # On Windows
```

Install the necessary Python packages:

```bash
pip install fastapi uvicorn requests tritonclient[all]
```

- `fastapi`: The FastAPI framework.
- `uvicorn`: An ASGI server for running FastAPI applications.
- `requests`: For making HTTP requests to the Triton Inference Server.
- `tritonclient[all]`: NVIDIA's Triton Inference Server client library.

## Step 2: Preparing Your ML Model

This guide demonstrates serving both TensorFlow and PyTorch models. Let's create a simple model for demonstration purposes.

### TensorFlow Model

```plaintext
# tf_model.py
import tensorflow as tf

# Define a simple Keras model
model = tf.keras.models.Sequential([
  tf.keras.layers.Dense(10, activation='relu', input_shape=(10,)),
  tf.keras.layers.Dense(1)
])

# Compile the model
model.compile(optimizer='adam', loss='mse')

# Save the model in SavedModel format
tf.saved_model.save(model, 'models/tensorflow_model/1')
```

### PyTorch Model

```plaintext
# pytorch_model.py
import torch
import torch.nn as nn

# Define a simple PyTorch model
class SimpleModel(nn.Module):
  def __init__(self, input_size, hidden_size, output_size):
    super(SimpleModel, self).__init__()
    self.fc1 = nn.Linear(input_size, hidden_size)
    self.relu = nn.ReLU()
    self.fc2 = nn.Linear(hidden_size, output_size)

  def forward(self, x):
    out = self.fc1(x)
    out = self.relu(out)
    out = self.fc2(out)
    return out

# Instantiate the model
input_size = 10
hidden_size = 10
output_size = 1
model = SimpleModel(input_size, hidden_size, output_size)

# Save the model
torch.save(model.state_dict(), 'models/pytorch_model/1/model.pt')
```

Run these scripts to create your models and save them in the `models` directory:

```bash
mkdir -p models/tensorflow_model/1
mkdir -p models/pytorch_model/1

python tf_model.py
python pytorch_model.py
```

The `1` in the directory path represents the version number of the model. Triton supports versioning, allowing you to deploy updated models without interrupting service.

## Step 3: Configuring Triton Inference Server

Triton Inference Server requires a configuration file (`config.pbtxt`) for each model to define its input and output schemas. Let's create configuration files for both models.

### TensorFlow Configuration (`models/tensorflow_model/config.pbtxt`)

```protobuf
name: "tensorflow_model"
platform: "tensorflow_savedmodel"
max_batch_size: 16
input [
  {
    name: "dense_input"
    data_type: TYPE_FP32
    dims: [ 10 ]
  }
]
output [
  {
    name: "dense_1"
    data_type: TYPE_FP32
    dims: [ 1 ]
  }
]
```

- `name`: The name of the model, which will be used in the API requests.
- `platform`: Specifies the framework used to build the model. `tensorflow_savedmodel` is used for TensorFlow SavedModel format.
- `max_batch_size`: The maximum batch size that the model can handle. Set to 0 to disable batching.
- `input`: Defines the input tensor(s) of the model, including the name, data type (`TYPE_FP32` for float32), and dimensions. The input name `"dense_input"` can be identified using `model.summary()` from tensorflow or tensorboard.
- `output`: Defines the output tensor(s) of the model, including the name, data type, and dimensions. The output name `"dense_1"` can be identified using `model.summary()` from tensorflow or tensorboard.

### PyTorch Configuration (`models/pytorch_model/config.pbtxt`)

```protobuf
name: "pytorch_model"
platform: "pytorch_libtorch"
max_batch_size: 16
input [
  {
    name: "input__0"
    data_type: TYPE_FP32
    dims: [ 10 ]
  }
]
output [
  {
    name: "output__0"
    data_type: TYPE_FP32
    dims: [ 1 ]
  }
]
```

- `name`: The name of the model.
- `platform`: Specifies the framework used to build the model. `pytorch_libtorch` is used for PyTorch models saved using `torch.save`.
- `max_batch_size`: The maximum batch size that the model can handle.
- `input`: Defines the input tensor(s). The input name `"input__0"` can be inferred when tracing model, or if it has a single input node, Triton assigns a default name of "input\_\_0".
- `output`: Defines the output tensor(s). The output name `"output__0"` can be inferred when tracing model, or if it has a single output node, Triton assigns a default name of "output\_\_0".

## Step 4: Running Triton Inference Server with Docker

Download the Triton Inference Server Docker image:

```bash
docker pull nvcr.io/nvidia/tritonserver:24.02-py3
```

Run the Triton Inference Server in a Docker container, mounting the `models` directory:

```bash
docker run --gpus all --rm -p8000:8000 -p8001:8001 -p8002:8002 -v$(pwd)/models:/models nvcr.io/nvidia/tritonserver:24.02-py3 tritonserver --model-repository=/models --grpc-inference-port=8001 --http-inference-port=8000 --metrics-port=8002
```

- `--gpus all`: Allows the container to access all GPUs on the host machine. Remove this if you don't have a GPU or want to use CPU inference.
- `-p 8000:8000 -p 8001:8001 -p 8002:8002`: Maps the container's ports to the host machine.
  - `8000`: HTTP inference port.
  - `8001`: GRPC inference port.
  - `8002`: Metrics port.
- `-v $(pwd)/models:/models`: Mounts the `models` directory on the host machine to the `/models` directory inside the container. This allows Triton to access your models.
- `tritonserver --model-repository=/models`: Specifies the location of the model repository inside the container.
- `--grpc-inference-port=8001`: Configures the gRPC inference port.
- `--http-inference-port=8000`: Configures the HTTP inference port.
- `--metrics-port=8002`: Configures the metrics port.

Wait for the server to start. You should see messages in the logs indicating that the models have been loaded successfully. You can check the server status by visiting `http://localhost:8000/v2/health/ready` in your browser or using `curl`.

## Step 5: Building the FastAPI Application

Now, let's create a FastAPI application to interact with the Triton Inference Server.

```plaintext
# main.py
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import requests
import numpy as np
import json

app = FastAPI()

TRITON_SERVER_URL = "http://localhost:8000"  # Adjust if your Triton server is running elsewhere

class PredictionRequest(BaseModel):
    input_data: list

@app.post("/predict/tensorflow")
async def predict_tensorflow(request: PredictionRequest):
    """
    Endpoint for making predictions using the TensorFlow model served by Triton Inference Server.
    """
    try:
        input_data = np.array(request.input_data, dtype=np.float32)
        if input_data.shape != (10,):
            raise HTTPException(status_code=400, detail="Input data must be a list of 10 numbers.")

        payload = {
            "inputs": [
                {
                    "name": "dense_input",
                    "shape": [1, 10],
                    "datatype": "FP32",
                    "data": input_data.reshape(1, 10).tolist(), # Reshape to (1, 10) for batching
                }
            ]
        }

        response = requests.post(
            f"{TRITON_SERVER_URL}/v2/models/tensorflow_model/versions/1/infer",
            headers={"Content-Type": "application/json"},
            data=json.dumps(payload),
        )

        response.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)

        results = response.json()
        predictions = results["outputs"][0]["data"]
        return {"predictions": predictions}

    except requests.exceptions.RequestException as e:
        raise HTTPException(status_code=500, detail=f"Error communicating with Triton Server: {e}")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"An unexpected error occurred: {e}")

@app.post("/predict/pytorch")
async def predict_pytorch(request: PredictionRequest):
    """
    Endpoint for making predictions using the PyTorch model served by Triton Inference Server.
    """
    try:
        input_data = np.array(request.input_data, dtype=np.float32)
        if input_data.shape != (10,):
            raise HTTPException(status_code=400, detail="Input data must be a list of 10 numbers.")

        payload = {
            "inputs": [
                {
                    "name": "input__0",
                    "shape": [1, 10],
                    "datatype": "FP32",
                    "data": input_data.reshape(1, 10).tolist(),
                }
            ]
        }

        response = requests.post(
            f"{TRITON_SERVER_URL}/v2/models/pytorch_model/versions/1/infer",
            headers={"Content-Type": "application/json"},
            data=json.dumps(payload),
        )

        response.raise_for_status()

        results = response.json()
        predictions = results["outputs"][0]["data"]
        return {"predictions": predictions}

    except requests.exceptions.RequestException as e:
        raise HTTPException(status_code=500, detail=f"Error communicating with Triton Server: {e}")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"An unexpected error occurred: {e}")


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8080)
```

Key components of the FastAPI application:

- `PredictionRequest`: A Pydantic model to define the expected input format for the API. It expects a list of numbers representing the input data.
- `/predict/tensorflow` and `/predict/pytorch` Endpoints: These are the API endpoints for making predictions using the TensorFlow and PyTorch models, respectively.
- `TRITON_SERVER_URL`: The URL of the Triton Inference Server. Make sure to adjust this if your Triton server is running on a different host or port.
- **Request Payload:** The code constructs the request payload in the format expected by the Triton Inference Server's HTTP API. The payload includes:
  - `name`: The name of the input tensor as defined in the `config.pbtxt` file.
  - `shape`: The shape of the input tensor. We are sending a single input (batch size of 1) with the input shape (1, 10).
  - `datatype`: The data type of the input tensor (e.g., `"FP32"` for float32).
  - `data`: The actual input data, reshaped to match the expected shape.
- **Error Handling:** The code includes error handling to catch potential issues such as invalid input data, communication errors with the Triton Server, and other unexpected errors. It returns appropriate HTTP error responses with informative error messages.

## Step 6: Running the FastAPI Application

Run the FastAPI application:

```bash
python main.py
```

This will start the server at `http://0.0.0.0:8080`. You can access the automatic API documentation at `http://localhost:8080/docs`.

## Step 7: Testing the API

You can test the API using `curl`, `Postman`, or any other HTTP client. Here's an example using `curl`:

**TensorFlow Model:**

```bash
curl -X POST -H "Content-Type: application/json" -d '{"input_data": [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0]}' http://localhost:8080/predict/tensorflow
```

**PyTorch Model:**

```bash
curl -X POST -H "Content-Type: application/json" -d '{"input_data": [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0]}' http://localhost:8080/predict/pytorch
```

You should receive a JSON response containing the predictions from the respective model.

## Advanced Considerations

- **Batching:** Triton supports dynamic batching, which can significantly improve throughput by aggregating multiple inference requests into a single batch. To enable batching, set the `max_batch_size` in the `config.pbtxt` file to a value greater than 0 and modify your FastAPI application to handle batched input.
- **Model Versioning:** Triton allows you to deploy multiple versions of a model simultaneously. You can specify the version to use in the API request by including the `versions/<version_number>` in the URL (e.g., `/v2/models/tensorflow_model/versions/2/infer`).
- **GPU vs. CPU Inference:** By default, Triton will use all available GPUs. If you want to use CPU inference, remove the `--gpus all` flag from the `docker run` command or configure specific GPUs using the `CUDA_VISIBLE_DEVICES` environment variable.
- **Health Checks:** Regularly monitor the health of both the FastAPI application and the Triton Inference Server. Triton provides a `/v2/health/ready` endpoint to check its readiness.
- **Monitoring and Logging:** Implement robust monitoring and logging to track the performance of your model serving system and identify potential issues. Triton exposes Prometheus metrics for monitoring. Configure FastAPI to log requests and errors.
- **Authentication and Authorization:** Secure your API endpoints using appropriate authentication and authorization mechanisms.
- **Input Data Validation:** Thoroughly validate the input data to ensure it conforms to the expected format and range. This can help prevent errors and improve the reliability of your system. FastAPI's Pydantic integration makes this easy.
- **Model Optimization:** Optimize your models for inference to improve performance. This may involve techniques such as quantization, pruning, and graph optimization. Consider using TensorRT for further optimization with NVIDIA GPUs.
- **Load Balancing:** For high-traffic applications, consider deploying multiple instances of the Triton Inference Server behind a load balancer to distribute the load and improve availability.
- **gRPC:** While this guide focuses on the HTTP interface, Triton also supports gRPC. gRPC is often preferred for its efficiency and support for streaming. You can use the `tritonclient` library to interact with Triton using gRPC.

## Conclusion

This guide provided a comprehensive overview of how to serve TensorFlow and PyTorch models using FastAPI and Triton Inference Server. By following these steps, you can build a robust, scalable, and efficient model serving solution for your machine learning applications. Remember to tailor the configuration and implementation to your specific needs and continuously monitor and optimize your system for optimal performance. Good luck!
