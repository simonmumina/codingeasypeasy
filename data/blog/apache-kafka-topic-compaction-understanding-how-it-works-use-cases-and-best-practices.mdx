---
title: 'Apache Kafka Topic Compaction: Understanding How It Works, Use Cases, and Best Practices'
date: '2024-10-26'
lastmod: '2024-10-27'
tags:
  [
    'apache kafka',
    'kafka',
    'topic compaction',
    'data streaming',
    'message queues',
    'event sourcing',
    'kafka architecture',
    'kafka best practices',
    'data management',
  ]
draft: false
summary: 'Dive deep into Apache Kafka topic compaction, learning how it works, when to use it, and best practices. Optimize your Kafka topics for performance and storage efficiency.'
authors: ['default']
---

# Apache Kafka Topic Compaction: Understanding How It Works, Use Cases, and Best Practices

Apache Kafka is a powerful distributed streaming platform widely used for building real-time data pipelines and streaming applications. One of the key features that contributes to its versatility is _topic compaction_. This blog post will provide a comprehensive guide to Kafka topic compaction, explaining its purpose, mechanics, use cases, and best practices.

## What is Kafka Topic Compaction?

Kafka topic compaction is a data retention mechanism that allows Kafka to retain only the _latest_ known value for each key within a topic partition. Unlike traditional time-based retention, where Kafka deletes messages after a specified period, topic compaction selectively removes older messages based on their key. This means that if a key appears multiple times within a partition, only the last occurrence of that key (along with its associated value) will be retained.

**Key Differences from Time-Based Retention:**

| Feature            | Time-Based Retention                                | Topic Compaction                                        |
| ------------------ | --------------------------------------------------- | ------------------------------------------------------- |
| Retention Criteria | Age of the message                                  | Key of the message (latest value for each key retained) |
| Data Deletion      | Messages older than the configured time are deleted | Older messages for the same key are deleted             |
| Data Loss          | All data older than the retention period is lost    | Only older values for the same key are removed          |
| Use Cases          | Logging, audit trails, short-term event storage     | Change data capture, stateful stream processing         |

## How Does Topic Compaction Work?

Kafka uses a background process called the _log cleaner_ to compact topics. The log cleaner periodically scans the topic partitions and identifies messages with duplicate keys. It then removes the older messages, leaving only the most recent message for each key. The process can be broken down into the following steps:

1. **Log Segmentation:** Kafka stores data in segments. Each segment is a log file.
2. **Marking for Deletion:** The log cleaner identifies older messages for the same key. These aren't immediately deleted.
3. **Garbage Collection (Compaction):** Periodically, Kafka compacts these segments, physically removing the marked messages. It essentially creates a new, cleaned segment.
4. **Index Updates:** Kafka updates its indexes to point to the latest offsets for each key.

**Deleting Keys (Tombstones):**

To signal the deletion of a key entirely, Kafka introduces the concept of _tombstones_. A tombstone is a message with a key but a _null_ value. When the log cleaner encounters a tombstone for a specific key, it removes all messages associated with that key from the compacted log, effectively deleting the key.

## When to Use Kafka Topic Compaction

Topic compaction is particularly useful in scenarios where you need to maintain a consistent, up-to-date snapshot of data, rather than the entire history of events. Here are some common use cases:

- **Change Data Capture (CDC):** Capture changes from a database and stream them to Kafka. Compaction ensures that only the latest state of each record is maintained. This is ideal for rebuilding read models or updating caches.

- **Stateful Stream Processing:** When building stateful stream processing applications using Kafka Streams or similar frameworks, topic compaction allows you to persist the state of your application in a compact form. This is crucial for fault tolerance and application recovery.

- **Key-Value Stores:** Kafka can be used as a simple key-value store by enabling topic compaction. This allows you to store and retrieve the latest value associated with each key.

- **Configuration Management:** Store configuration settings in Kafka. Topic compaction ensures that only the most recent configuration is available.

- **Materialized Views:** Create materialized views of data stored in Kafka. Topic compaction ensures that the views are always up-to-date with the latest data.

## Advantages of Topic Compaction

- **Reduced Storage Costs:** By removing obsolete messages, topic compaction significantly reduces the amount of storage required to store data in Kafka.
- **Faster Recovery:** When recovering from failures, applications only need to replay the compacted log, which is much smaller than the full log.
- **Improved Performance:** Reading from a compacted topic is faster because there are fewer messages to process.
- **Data Consistency:** Ensures that consumers always have access to the latest state of the data.

## Configuring Topic Compaction in Kafka

To enable topic compaction for a Kafka topic, you need to configure the following properties:

- **`cleanup.policy=compact`**: This property tells Kafka to use topic compaction as the cleanup policy for the topic.
- **`delete.retention.ms`**: This property controls how long tombstones (messages with null values) are retained before being deleted. A longer retention period ensures that consumers have enough time to see the tombstone and remove the corresponding data from their state.
- **`min.cleanable.dirty.ratio`**: This property controls how much "dirty" data (data eligible for compaction) must be in a log segment before it is eligible for compaction. A lower value triggers compaction more frequently.
- **`min.compaction.lag.ms`**: This property specifies the minimum time a message must remain in the log before it is eligible for compaction. This helps prevent compaction from running too frequently on rapidly changing data.

**Creating a Compacted Topic using Kafka CLI:**

```bash
kafka-topics --create --topic my-compacted-topic --bootstrap-server localhost:9092 \
  --replication-factor 1 --partitions 1 \
  --config cleanup.policy=compact \
  --config delete.retention.ms=86400000  # 24 hours
```

**Updating an Existing Topic to Use Compaction (Careful!):**

**Warning:** Switching an existing topic to compaction can have unintended consequences. Ensure consumers are prepared to handle tombstones and understand the potential for data loss if older messages are removed that they were relying on.

```bash
kafka-configs --alter --entity-type topics --entity-name my-existing-topic --bootstrap-server localhost:9092 \
  --add-config cleanup.policy=compact,delete.retention.ms=86400000
```

**Example: Producing and Consuming Data with Compaction**

Let's illustrate with a simple Python example using the `kafka-python` library:

**Producer (producing data with updates and a tombstone):**

```plaintext
from kafka import KafkaProducer
import json

producer = KafkaProducer(
    bootstrap_servers=['localhost:9092'],
    value_serializer=lambda v: json.dumps(v).encode('utf-8')
)

# Produce some updates for a key
producer.send('my-compacted-topic', key=b'user1', value={'name': 'Alice', 'age': 30})
producer.send('my-compacted-topic', key=b'user1', value={'name': 'Alice', 'age': 31})
producer.send('my-compacted-topic', key=b'user2', value={'name': 'Bob', 'age': 25})

# Produce a tombstone to delete user1
producer.send('my-compacted-topic', key=b'user1', value=None) # Value is None for a tombstone

producer.flush() # Ensure all messages are sent
```

**Consumer (consuming data and handling tombstones):**

```plaintext
from kafka import KafkaConsumer
import json

consumer = KafkaConsumer(
    'my-compacted-topic',
    bootstrap_servers=['localhost:9092'],
    auto_offset_reset='earliest',  # Start from the beginning
    enable_auto_commit=True,
    value_deserializer=lambda v: json.loads(v.decode('utf-8')) if v else None #Handle None values
)

user_data = {}

for message in consumer:
    key = message.key.decode('utf-8')
    value = message.value

    if value is None:
        # Tombstone: Remove the key
        if key in user_data:
            del user_data[key]
        print(f"Key '{key}' deleted (tombstone)")

    else:
        # Update the user data
        user_data[key] = value
        print(f"Updated data for key '{key}': {value}")

    print(f"Current User Data: {user_data}")

```

This example demonstrates how to produce updates for keys and then delete a key using a tombstone. The consumer handles both regular updates and tombstone messages to maintain a consistent view of the data. Pay close attention to the `value_deserializer` in the consumer, which handles `None` values, which are tombstone messages.

## Best Practices for Using Topic Compaction

- **Choose the Right Retention Policy:** Consider the trade-offs between storage costs, recovery time, and data consistency when choosing a retention policy.
- **Monitor Compaction Performance:** Monitor the performance of the log cleaner to ensure that compaction is not impacting the overall performance of your Kafka cluster. Metrics like `kafka.log.LogCleanerManager.cleanable-ratio` are important to watch.
- **Consider Consumer Compatibility:** Ensure that your consumers are able to handle tombstones and understand the implications of data deletion.
- **Testing is Crucial:** Thoroughly test your applications with topic compaction enabled to ensure that they behave as expected. Pay particular attention to recovery scenarios.
- **Partitioning Strategy:** The effectiveness of compaction depends on the partitioning strategy. Distribute keys that are frequently updated evenly across partitions to maximize compaction efficiency.
- **Avoid Over-Compaction:** Setting a low `min.cleanable.dirty.ratio` may lead to frequent compaction, consuming resources unnecessarily. Balance the frequency with the actual need for space reclamation.

## When _Not_ to Use Topic Compaction

While topic compaction is a powerful tool, it's not appropriate for all scenarios. Avoid using it when:

- **You need the full history of events:** If you require a complete audit trail or need to analyze historical data, topic compaction is not suitable. Use time-based retention instead.
- **Keys are rarely updated:** If your data is mostly append-only and keys are rarely updated, the benefits of topic compaction will be minimal.
- **Consumers require sequential processing of all events:** Compaction changes the ordering of events, potentially breaking consumers that rely on the precise order of messages.

## Conclusion

Apache Kafka topic compaction is a valuable tool for optimizing storage and improving performance in specific use cases, particularly those involving change data capture, stateful stream processing, and key-value storage. By understanding how it works and following best practices, you can effectively leverage topic compaction to build efficient and reliable Kafka applications. Remember to carefully consider the trade-offs and test your applications thoroughly before deploying topic compaction in production.
