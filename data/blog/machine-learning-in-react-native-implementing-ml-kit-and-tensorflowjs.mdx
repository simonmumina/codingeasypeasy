---
title: 'Machine Learning in React Native: Implementing ML Kit & TensorFlow.js'
date: '2024-10-27'
lastmod: '2024-10-27'
tags:
  [
    'react-native',
    'machine-learning',
    'mlkit',
    'tensorflowjs',
    'mobile-development',
    'javascript',
    'ai',
    'artificial-intelligence',
  ]
draft: false
summary: 'A comprehensive guide to implementing machine learning in React Native using ML Kit for native performance and TensorFlow.js for browser-compatible models. Learn how to integrate and utilize pre-trained models for tasks like image recognition, text analysis, and more.'
authors: ['default']
---

# Machine Learning in React Native: Implementing ML Kit & TensorFlow.js

Machine learning (ML) is rapidly transforming mobile applications, offering features like image recognition, natural language processing, and personalized recommendations. React Native, with its cross-platform capabilities, provides an excellent environment to bring these ML functionalities to both iOS and Android users. This article dives into implementing machine learning in React Native using two popular frameworks: ML Kit (specifically for native device features) and TensorFlow.js (for leveraging browser-based ML models).

## Why Machine Learning in React Native?

- **Cross-Platform Development:** Write code once and deploy it to both iOS and Android.
- **Native Performance (ML Kit):** Leverage device-specific hardware for optimized performance, particularly for tasks like image processing.
- **Flexibility (TensorFlow.js):** Run pre-trained models directly in JavaScript, enabling compatibility with a wider range of models and easier deployment.
- **Enhanced User Experience:** Provide richer and more intelligent features to your users.

## Choosing Between ML Kit and TensorFlow.js

The choice between ML Kit and TensorFlow.js depends on your specific needs:

- **ML Kit:**
  - **Pros:** Native performance, easy-to-use APIs for common tasks (image labeling, text recognition, face detection, etc.). Optimized for mobile devices.
  - **Cons:** Limited model customization, requires more native code integration. Can be platform specific with installation and configuration.
- **TensorFlow.js:**
  - **Pros:** Runs in JavaScript, highly flexible with model loading and customization, large community support. Browser-compatible models can be used.
  - **Cons:** Performance might be slightly slower than native implementations, especially for complex models.

For tasks where native performance is critical (e.g., real-time object detection), ML Kit is often the better choice. For tasks where model customization and portability are more important, TensorFlow.js might be preferable.

## Implementing ML Kit in React Native

ML Kit offers a collection of ready-to-use APIs for common ML tasks. While direct JavaScript bindings are not always available for all features, you can use native modules to access ML Kit's functionality from your React Native application.

### Setting Up the Native Modules (Android)

1.  **Install React Native Firebase:** While ML Kit is part of Firebase, you don't need the full Firebase SDK. Just the ML Kit dependency.

    ```plaintext
    npm install @react-native-firebase/app @react-native-firebase/ml-kit
    cd ios && pod install && cd ..
    ```

2.  **Android Configuration:** (Usually automatically linked, but verify)

    - Ensure your `android/build.gradle` includes the necessary repositories:

      ```plaintext
      allprojects {
          repositories {
              google()
              mavenCentral()
          }
      }
      ```

3.  **Android Code (Native Module):** Create a Java class to bridge the ML Kit functionality to React Native. Let's create a simple example for text recognition. Create a new file named `MyMLKitModule.java` in `android/app/src/main/java/<your_package_name>`. Replace `<your_package_name>` with your app's package name.

    ```plaintext
    package com.<your_package_name>;

    import com.facebook.react.bridge.ReactApplicationContext;
    import com.facebook.react.bridge.ReactContextBaseJavaModule;
    import com.facebook.react.bridge.ReactMethod;
    import com.facebook.react.bridge.Promise;

    import android.graphics.Bitmap;
    import android.graphics.BitmapFactory;
    import android.net.Uri;

    import androidx.annotation.NonNull;

    import com.google.android.gms.tasks.OnFailureListener;
    import com.google.android.gms.tasks.OnSuccessListener;
    import com.google.mlkit.vision.common.InputImage;
    import com.google.mlkit.vision.text.Text;
    import com.google.mlkit.vision.text.TextRecognition;
    import com.google.mlkit.vision.text.TextRecognizer;
    import com.google.mlkit.vision.text.latin.TextRecognizerOptions;

    import java.io.IOException;
    import java.io.InputStream;


    public class MyMLKitModule extends ReactContextBaseJavaModule {

        private static ReactApplicationContext reactContext;

        MyMLKitModule(ReactApplicationContext context) {
            super(context);
            reactContext = context;
        }

        @Override
        public String getName() {
            return "MyMLKitModule";
        }

        @ReactMethod
        public void recognizeText(String imageUriString, Promise promise) {
            try {
                Uri imageUri = Uri.parse(imageUriString);
                InputStream imageStream = reactContext.getContentResolver().openInputStream(imageUri);
                Bitmap bitmap = BitmapFactory.decodeStream(imageStream);


                if (bitmap != null) {
                    InputImage image = InputImage.fromBitmap(bitmap, 0);
                    TextRecognizer recognizer = TextRecognition.getClient(new TextRecognizerOptions.Builder().build());

                    recognizer.process(image)
                            .addOnSuccessListener(new OnSuccessListener<Text>() {
                                @Override
                                public void onSuccess(Text result) {
                                    promise.resolve(result.getText());
                                }
                            })
                            .addOnFailureListener(new OnFailureListener() {
                                @Override
                                public void onFailure(@NonNull Exception e) {
                                    promise.reject("Text Recognition Error", e.getMessage());
                                }
                            });
                } else {
                    promise.reject("Bitmap Error", "Failed to decode bitmap from URI");
                }
            } catch (IOException e) {
                promise.reject("Image Error", e.getMessage());
            }
        }
    }
    ```

4.  **Register the Module:** Create a `MyMLKitPackage.java` file in the same directory:

    ```plaintext
    package com.<your_package_name>;

    import com.facebook.react.ReactPackage;
    import com.facebook.react.bridge.NativeModule;
    import com.facebook.react.bridge.ReactApplicationContext;
    import com.facebook.react.uimanager.ViewManager;

    import java.util.ArrayList;
    import java.util.Collections;
    import java.util.List;

    public class MyMLKitPackage implements ReactPackage {

        @Override
        public List<NativeModule> createNativeModules(ReactApplicationContext reactContext) {
            List<NativeModule> modules = new ArrayList<>();

            modules.add(new MyMLKitModule(reactContext));

            return modules;
        }

        @Override
        public List<ViewManager> createViewManagers(ReactApplicationContext reactContext) {
            return Collections.emptyList();
        }
    }
    ```

5.  **Register the Package in `MainApplication.java`:**

    ```plaintext
    package com.<your_package_name>;

    import android.app.Application;

    import com.facebook.react.ReactApplication;
    import com.facebook.react.ReactNativeHost;
    import com.facebook.react.ReactPackage;
    import com.facebook.react.shell.MainReactPackage;
    import com.facebook.soloader.SoLoader;

    import java.util.Arrays;
    import java.util.List;

    public class MainApplication extends Application implements ReactApplication {

      private final ReactNativeHost mReactNativeHost = new ReactNativeHost(this) {
        @Override
        public boolean getUseDeveloperSupport() {
          return BuildConfig.DEBUG;
        }

        @Override
        protected List<ReactPackage> getPackages() {
          return Arrays.<ReactPackage>asList(
              new MainReactPackage(),
                  new MyMLKitPackage()  // Add this line
          );
        }

        @Override
        protected String getJSMainModuleName() {
          return "index";
        }
      };

      @Override
      public ReactNativeHost getReactNativeHost() {
        return mReactNativeHost;
      }

      @Override
      public void onCreate() {
        super.onCreate();
        SoLoader.init(this, /* native exopackage */ false);
      }
    }
    ```

6.  **Add Permissions to `AndroidManifest.xml`:**

    ```xml
    <uses-permission android:name="android.permission.CAMERA"/>
    <uses-permission android:name="android.permission.READ_EXTERNAL_STORAGE" />
    <uses-permission android:name="android.permission.WRITE_EXTERNAL_STORAGE" />
    ```

### Setting Up the Native Modules (iOS)

1.  **Install React Native Firebase:** Same as Android.

    ```plaintext
    npm install @react-native-firebase/app @react-native-firebase/ml-kit
    cd ios && pod install && cd ..
    ```

2.  **iOS Code (Native Module):** Create a header file (`MyMLKitModule.h`) and implementation file (`MyMLKitModule.m`) in your project's `ios` directory.

    **MyMLKitModule.h:**

    ```objectivec
    #import <React/RCTBridgeModule.h>

    @interface MyMLKitModule : NSObject <RCTBridgeModule>

    @end
    ```

    **MyMLKitModule.m:**

    ```objectivec
    #import "MyMLKitModule.h"
    #import <React/RCTLog.h>
    #import <MLKitVision/MLKitVision.h>
    #import <MLKitVisionText/MLKitVisionText.h>
    #import <MLKitVisionTextCommon/MLKitVisionTextCommon.h>


    @implementation MyMLKitModule

    RCT_EXPORT_MODULE();

    RCT_REMAP_METHOD(recognizeText,
                     imagePath:(NSString *)imagePath
                     resolver:(RCTPromiseResolveBlock)resolve
                     rejecter:(RCTPromiseRejectBlock)reject)
    {

        UIImage *image = [UIImage imageWithContentsOfFile:imagePath];

        if (!image) {
            reject(@"Image Error", @"Failed to load image from path", nil);
            return;
        }

        VisionImage *visionImage = [[VisionImage alloc] initWithImage:image];
        visionImage.orientation = image.imageOrientation;

        NSError *error = nil;
        TextRecognizer *textRecognizer = [TextRecognizer textRecognizer];

        [textRecognizer processImage:visionImage
                             completion:^(Text *_Nullable result,
                                          NSError *_Nullable error) {
            if (error != nil) {
                reject(@"Text Recognition Error", error.localizedDescription, error);
                return;
            }

            NSString *recognizedText = result.text;
            resolve(recognizedText);
        }];
    }

    @end
    ```

3.  **Bridge to React Native:**

    - **Create a Bridging Header:** If you don't have one already, create a bridging header file (`YourProjectName-Bridging-Header.h`) in your `ios` directory. Xcode will prompt you to create one if it doesn't exist when you import Objective-C code into Swift/Objective-C++.
    - **Import React/RCTBridgeModule.h:** In your bridging header, add the following:

      ```objectivec
      #import <React/RCTBridgeModule.h>
      #import <MLKitVision/MLKitVision.h>
      #import <MLKitVisionText/MLKitVisionText.h>
      #import <MLKitVisionTextCommon/MLKitVisionTextCommon.h>
      ```

    - **Update Build Settings:** Go to your project settings in Xcode, select your target, go to "Build Settings," and search for "Objective-C Bridging Header." Set the path to your bridging header file (e.g., `$(SRCROOT)/YourProjectName/YourProjectName-Bridging-Header.h`).
    - **Add framework search paths:** In Build Settings search for "Framework Search Paths" and add the path where your Firebase ML Kit frameworks reside, this can be under `$(SRCROOT)/Pods/MLKitVision/Frameworks` and `$(SRCROOT)/Pods/MLKitVisionText/Frameworks`, make sure to specify those paths as recursive (`**`).

4.  **Add Camera Usage Description:**

    Add the `NSCameraUsageDescription` key to your `Info.plist` to request camera access.

    ```xml
    <key>NSCameraUsageDescription</key>
    <string>This app needs camera access to scan text.</string>
    ```

### Using the Native Module in React Native

```plaintext
import React, { useState } from 'react'
import { View, Button, Image, Text, Alert, Platform } from 'react-native'
import * as ImagePicker from 'react-native-image-picker'
import { NativeModules } from 'react-native'

const { MyMLKitModule } = NativeModules

const App = () => {
  const [imageSource, setImageSource] = useState(null)
  const [recognizedText, setRecognizedText] = useState('')

  const chooseImage = async () => {
    const options = {
      mediaType: 'photo',
      includeBase64: false,
      maxHeight: 2000,
      maxWidth: 2000,
    }

    ImagePicker.launchImageLibrary(options, (response) => {
      if (response.didCancel) {
        console.log('User cancelled image picker')
      } else if (response.error) {
        console.log('ImagePicker Error: ', response.error)
      } else {
        const source = response.assets[0].uri
        setImageSource(source)
      }
    })
  }

  const captureImage = async () => {
    const options = {
      mediaType: 'photo',
      includeBase64: false,
      maxHeight: 2000,
      maxWidth: 2000,
    }

    ImagePicker.launchCamera(options, (response) => {
      if (response.didCancel) {
        console.log('User cancelled image picker')
      } else if (response.error) {
        console.log('ImagePicker Error: ', response.error)
      } else {
        const source = response.assets[0].uri
        setImageSource(source)
      }
    })
  }

  const recognizeTextHandler = async () => {
    if (!imageSource) {
      Alert.alert('Please select an image first.')
      return
    }

    try {
      const text = await MyMLKitModule.recognizeText(imageSource)
      setRecognizedText(text)
    } catch (e) {
      console.error(e)
      Alert.alert('Error', e.message)
    }
  }

  return (
    <View style={{ flex: 1, justifyContent: 'center', alignItems: 'center' }}>
      <Button title="Choose Image" onPress={chooseImage} />
      <Button title="Capture Image" onPress={captureImage} />
      {imageSource && (
        <Image source={{ uri: imageSource }} style={{ width: 200, height: 200, marginTop: 20 }} />
      )}
      <Button title="Recognize Text" onPress={recognizeTextHandler} disabled={!imageSource} />
      {recognizedText ? (
        <Text style={{ marginTop: 20 }}>Recognized Text: {recognizedText}</Text>
      ) : null}
    </View>
  )
}

export default App
```

**Explanation:**

- We import the native module `MyMLKitModule`.
- The `recognizeTextHandler` function calls the `recognizeText` method of the native module, passing the image URI.
- The recognized text is then displayed in the UI.
- Uses `react-native-image-picker` to select the image.

**Important Considerations for ML Kit:**

- **Error Handling:** Implement robust error handling in both your native module and React Native code.
- **Permissions:** Request necessary permissions (camera, storage) from the user.
- **Image Paths:** Note that the image URI passed to the native module needs to be a file path on iOS. On Android, the URI can be directly parsed using the content resolver.
- **Performance:** Optimize image processing for mobile devices. Consider resizing images before processing.
- **Thread Safety:** Ensure your native module methods are thread-safe.
- **Asynchronous Operations:** Use promises or callbacks to handle asynchronous operations correctly.

## Implementing TensorFlow.js in React Native

TensorFlow.js allows you to run pre-trained models directly in your React Native application, using JavaScript. This is a great option for tasks where native performance is not as critical, or when you want to use a wider range of models that are readily available in TensorFlow format.

### Installing TensorFlow.js

```plaintext
npm install @tensorflow/tfjs @tensorflow/tfjs-react-native react-native-unimodules expo-gl expo-gl-cpp expo-modules-core
```

**Expo Installation** (if using Expo):

If you are using Expo, you'll also need to install the following packages (and likely run `expo prebuild`):

```plaintext
expo install @tensorflow/tfjs @tensorflow/tfjs-react-native react-native-unimodules expo-gl expo-gl-cpp expo-modules-core
```

If you're using Expo, you'll also need to modify your `app.json` (or `app.config.js`) to add the following plugin. This is necessary for Expo to properly bundle the required native modules.

```plaintext
{
  "expo": {
    "plugins": [["expo-gl", { "importFrom": "react-native-unimodules" }]]
  }
}
```

### Initializing TensorFlow.js

Before using TensorFlow.js, you need to initialize it within your React Native application.

```plaintext
import React, { useState, useEffect } from 'react'
import { View, Text, ActivityIndicator } from 'react-native'
import * as tf from '@tensorflow/tfjs'
import * as tfjsWasm from '@tensorflow/tfjs-backend-wasm'
import { bundleResourceIO, decodeJpeg } from '@tensorflow/tfjs-react-native'

const App = () => {
  const [isTfReady, setIsTfReady] = useState(false)

  useEffect(() => {
    async function prepare() {
      try {
        // Wait for tf to be ready.
        await tf.ready()

        // Register WebGL backend.  Ensure expo-gl is installed.
        // WebGL can cause issues.  Try the WASM backend if you have problems.
        // tf.setBackend('webgl');

        // Register WASM backend. Recommended as the default.
        tfjsWasm.setWasmPaths(
          `https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-wasm@${tfjsWasm.version_wasm}/dist/`
        )
        tf.registerBackend('wasm', () => new tfjsWasm.WasmBackendFactory())
        tf.setBackend('wasm')

        console.log('TensorFlow.js version:', tf.version.tfjs)
        console.log('Backend:', tf.getBackend())

        setIsTfReady(true)
      } catch (e) {
        console.log('Error initializing TensorFlow:', e)
      }
    }
    prepare()
  }, [])

  if (!isTfReady) {
    return (
      <View style={{ flex: 1, justifyContent: 'center', alignItems: 'center' }}>
        <ActivityIndicator size="large" />
        <Text>Loading TensorFlow.js...</Text>
      </View>
    )
  }

  return (
    <View style={{ flex: 1, justifyContent: 'center', alignItems: 'center' }}>
      <Text>TensorFlow.js is ready!</Text>
    </View>
  )
}

export default App
```

**Explanation:**

- We import the necessary TensorFlow.js modules.
- We use the `useEffect` hook to initialize TensorFlow.js when the component mounts.
- `tf.ready()` ensures that TensorFlow.js is fully loaded before we proceed.
- We set the TensorFlow.js backend to WebGL (hardware acceleration) or WASM (WebAssembly, generally more stable on mobile). Choose the best option for your device.
- We set `isTfReady` to `true` when TensorFlow.js is ready.

### Loading a Pre-Trained Model (MobileNet)

Now, let's load a pre-trained model like MobileNet for image classification. We'll use a bundled image for simplicity. For real applications, you'd use an image from the camera or gallery.

```plaintext
import React, { useState, useEffect } from 'react'
import { View, Text, ActivityIndicator, Image, Button, Alert } from 'react-native'
import * as tf from '@tensorflow/tfjs'
import * as tfjsWasm from '@tensorflow/tfjs-backend-wasm'
import { bundleResourceIO, decodeJpeg } from '@tensorflow/tfjs-react-native'
import * as ImagePicker from 'react-native-image-picker'

// Import the image you want to classify (you'll need to add it to your project)
import image from './assets/cat.jpg' // Replace with your image path

const App = () => {
  const [isTfReady, setIsTfReady] = useState(false)
  const [model, setModel] = useState(null)
  const [predictions, setPredictions] = useState(null)
  const [selectedImage, setSelectedImage] = useState(null) // Store the URI of the chosen image

  useEffect(() => {
    async function prepare() {
      try {
        await tf.ready()
        tfjsWasm.setWasmPaths(
          `https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-wasm@${tfjsWasm.version_wasm}/dist/`
        )
        tf.registerBackend('wasm', () => new tfjsWasm.WasmBackendFactory())
        tf.setBackend('wasm')

        // Load the MobileNet model
        console.log('Loading mobilenet...')
        const mobilenetModel = await tf.loadGraphModel(
          bundleResourceIO(
            require('./assets/model/model.json'), // Path to your model.json file
            require('./assets/model/group1-shard1of1.bin') // Path to your weights file
          )
        )
        setModel(mobilenetModel)
        console.log('Mobilenet loaded!')

        setIsTfReady(true)
      } catch (e) {
        console.log('Error initializing TensorFlow:', e)
        Alert.alert('Error', `TensorFlow initialization failed: ${e.message}`) // Better error handling
      }
    }
    prepare()
  }, [])

  // Add image selection
  const chooseImage = async () => {
    const options = {
      mediaType: 'photo',
      includeBase64: false,
      maxHeight: 2000,
      maxWidth: 2000,
    }

    ImagePicker.launchImageLibrary(options, (response) => {
      if (response.didCancel) {
        console.log('User cancelled image picker')
      } else if (response.error) {
        console.log('ImagePicker Error: ', response.error)
      } else {
        const source = response.assets[0].uri
        setSelectedImage(source) // Store the selected image URI
      }
    })
  }

  // Function to convert URI to Tensor
  const uriToTensor = async (uri) => {
    try {
      const imgB64 = await fetch(uri)
      const imgBuffer = await imgB64.arrayBuffer()
      const imgTensor = decodeJpeg(new Uint8Array(imgBuffer))
      return imgTensor
    } catch (e) {
      console.log('Error converting URI to Tensor:', e)
      Alert.alert('Error', `Image conversion to tensor failed: ${e.message}`) // More helpful alert
      return null
    }
  }

  const classifyImage = async () => {
    if (!model) {
      Alert.alert('Model not loaded yet!')
      return
    }

    if (!selectedImage) {
      Alert.alert('Please choose an image first!')
      return
    }

    try {
      const imageTensor = await uriToTensor(selectedImage) // Load selected image
      if (!imageTensor) return

      // Resize and normalize the image tensor
      const resizedImage = tf.image.resizeBilinear(imageTensor, [224, 224])
      const normalizedImage = resizedImage.div(255.0).expandDims(0) // Add batch dimension

      // Make the prediction
      const prediction = await model.predict(normalizedImage)
      const values = await prediction.data()
      const probabilities = Array.from(values)

      // Get top 5 predictions (for demonstration purposes)
      const topK = Array(5).fill({ index: 0, value: 0 }) // Initialize with zero values
      for (let i = 0; i < probabilities.length; i++) {
        for (let j = 0; j < 5; j++) {
          if (probabilities[i] > topK[j].value) {
            topK.splice(j, 0, { index: i, value: probabilities[i] })
            topK.pop()
            break
          }
        }
      }

      setPredictions(topK) // Set the predictions
    } catch (e) {
      console.log('Error classifying image:', e)
      Alert.alert('Classification Error', `Error during image classification: ${e.message}`) // Useful error
    }
  }

  if (!isTfReady) {
    return (
      <View style={{ flex: 1, justifyContent: 'center', alignItems: 'center' }}>
        <ActivityIndicator size="large" />
        <Text>Loading TensorFlow.js and model...</Text>
      </View>
    )
  }

  return (
    <View style={{ flex: 1, justifyContent: 'center', alignItems: 'center' }}>
      <Text>TensorFlow.js is ready!</Text>
      <Button title="Choose Image" onPress={chooseImage} />
      {selectedImage && (
        <Image source={{ uri: selectedImage }} style={{ width: 200, height: 200, marginTop: 20 }} />
      )}
      <Button title="Classify Image" onPress={classifyImage} disabled={!model || !selectedImage} />

      {predictions && (
        <View>
          <Text>Predictions:</Text>
          {predictions.map((pred, index) => (
            <Text key={index}>
              Class {pred.index}: {pred.value.toFixed(3)}
            </Text>
          ))}
        </View>
      )}
    </View>
  )
}

export default App
```

**Explanation:**

1.  **Model Loading:**

    - We load the `model.json` and `group1-shard1of1.bin` files, which contain the MobileNet model definition and weights. These files need to be placed in your project's `assets` folder (create it if it doesn't exist). Ensure you've correctly configured `metro.config.js` to include binary files (see below).
    - `bundleResourceIO` is used to load the model from the bundled resources.

2.  **Image Preprocessing:**

    - We convert the image to a TensorFlow.js tensor using `decodeJpeg`.
    - The image is resized to 224x224 pixels (the input size for MobileNet).
    - The image is normalized by dividing by 255.0.
    - A batch dimension is added using `expandDims(0)`.

3.  **Prediction:**

    - We call `model.predict()` to get the model's predictions.
    - The predictions are processed to extract the top 5 predicted classes and their probabilities.

4.  **UI:**
    - The UI displays the image and the top 5 predictions.

**Important Considerations for TensorFlow.js:**

- **Model Size:** TensorFlow.js models can be large, which can impact your app's size and download time. Consider using quantized models to reduce their size.
- **Performance:** TensorFlow.js performance can vary depending on the device and the model. Experiment with different backends (WebGL, WASM) to find the best performance. WASM is often a good choice for mobile.
- **Memory Management:** Be mindful of memory usage, especially when working with large images and models. Dispose of tensors when you're finished with them to prevent memory leaks (use `tensor.dispose()`).
- **Security:** Be careful when loading models from external sources, as they could contain malicious code.
- **Async Operations:** Loading and running models are asynchronous operations. Use `async/await` to handle them properly.
- **Loading custom Models**: When loading custom models you must include a `metro.config.js` file that allows the bundler to package the model file and any weights files, the following is an example:

```js
const { getDefaultConfig } = require('metro-config')

module.exports = (async () => {
  const {
    resolver: { sourceExts, assetExts },
  } = await getDefaultConfig()
  return {
    transformer: {
      babelTransformerPath: require.resolve('react-native-svg-transformer'),
    },
    resolver: {
      assetExts: assetExts.filter((ext) => ext !== 'svg'),
      sourceExts: [
        ...sourceExts,
        'svg',
        'glb',
        'gltf',
        'bin',
        'mtl',
        'obj',
        'ts',
        'tsx',
        'jsx',
        'js',
      ],
    },
  }
})()
```

This adds `.bin` to be included as an asset.

## Conclusion

Implementing machine learning in React Native opens up a world of possibilities for creating intelligent and engaging mobile applications. By combining the native performance of ML Kit with the flexibility of TensorFlow.js, you can choose the best approach for each specific task. Remember to carefully consider performance, model size, and security when integrating ML into your React Native projects. This guide provides a solid foundation for building powerful ML-powered React Native applications. Good luck!
