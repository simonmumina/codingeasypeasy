---
title: 'ML Model Monitoring with Flask: A Comprehensive Guide (Python)'
date: '2024-10-26'
lastmod: '2024-10-26'
tags:
  [
    'machine learning',
    'ml monitoring',
    'flask',
    'python',
    'model deployment',
    'drift detection',
    'performance monitoring',
    'data science',
    'api',
  ]
draft: false
summary: 'Learn how to implement robust ML model monitoring using Flask in Python. Monitor model performance, detect data drift, and set up alerts to maintain model accuracy and reliability.'
authors: ['default']
---

# ML Model Monitoring with Flask: A Comprehensive Guide (Python)

Maintaining the health and accuracy of deployed machine learning models is crucial for ensuring reliable and consistent performance. Without proper monitoring, models can degrade over time due to changes in data patterns (data drift), concept drift, or infrastructure issues. This blog post provides a comprehensive guide on how to implement robust ML model monitoring using Flask, a lightweight and flexible Python web framework. We will cover key aspects like performance monitoring, data drift detection, alerting mechanisms, and practical code examples to get you started.

## Why is ML Model Monitoring Important?

ML models are not static entities. They are trained on specific datasets and assumptions about the real-world environment. Over time, the data distribution the model sees in production can deviate significantly from the training data, leading to a decline in model accuracy and reliability. Here's why monitoring is essential:

- **Performance Degradation:** Identifying and addressing performance drops before they impact business outcomes.
- **Data Drift Detection:** Recognizing changes in the input data distribution, indicating potential problems with data pipelines or shifts in user behavior.
- **Concept Drift Detection:** Recognizing changes in the relationship between input features and the target variable.
- **Early Warning System:** Setting up alerts for anomalies and potential issues before they escalate.
- **Compliance and Auditing:** Maintaining a record of model performance and data characteristics for regulatory compliance.
- **Improved Model Retraining:** Using monitoring data to inform retraining strategies and improve future model versions.

## Key Components of ML Model Monitoring

A comprehensive ML model monitoring solution typically includes the following components:

1.  **Data Collection:** Gathering data about model inputs, predictions, and actual outcomes.
2.  **Performance Metrics Calculation:** Computing key performance indicators (KPIs) such as accuracy, precision, recall, F1-score, AUC, RMSE, etc.
3.  **Data Drift Detection:** Identifying significant changes in the distribution of input features. Common techniques include:
    - **Kolmogorov-Smirnov (KS) Test:** Compares the distributions of two continuous variables.
    - **Chi-Squared Test:** Compares the distributions of two categorical variables.
    - **Population Stability Index (PSI):** Quantifies the difference between two distributions.
4.  **Alerting System:** Sending notifications when performance metrics fall below acceptable thresholds or data drift is detected.
5.  **Visualization and Reporting:** Creating dashboards and reports to visualize model performance and data characteristics.
6.  **Logging:** Storing monitoring data for analysis and auditing.

## Implementing ML Model Monitoring with Flask

Let's build a basic Flask application to demonstrate how to implement ML model monitoring. We'll focus on performance monitoring and data drift detection, using a simple example with a pre-trained model.

**Prerequisites:**

- Python 3.6 or higher
- Flask
- Scikit-learn
- Pandas
- NumPy
- SciPy
- Alibi Detect (for drift detection)

Install the necessary packages:

```plaintext
pip install flask scikit-learn pandas numpy scipy alibi-detect
```

**1. Define the Model (Simplified Example):**

We'll use a dummy model for demonstration purposes. In a real-world scenario, you would replace this with your actual trained ML model.

```plaintext
# model.py
import numpy as np
from sklearn.linear_model import LogisticRegression
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import joblib

def train_model():
    # Generate synthetic data
    np.random.seed(42)
    X = np.random.rand(100, 5)
    y = (X[:, 0] + X[:, 1] > 1).astype(int)  # Simple classification rule

    # Split data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Standardize features
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    # Train a Logistic Regression model
    model = LogisticRegression()
    model.fit(X_train, y_train)

    # Save the model and scaler
    joblib.dump(model, 'model.joblib')
    joblib.dump(scaler, 'scaler.joblib')

if __name__ == '__main__':
    train_model()

```

**2. Create the Flask Application (app.py):**

This Flask application will:

- Load the pre-trained model.
- Define an API endpoint to receive data for prediction.
- Calculate performance metrics.
- Detect data drift.
- Log monitoring data.

```plaintext
# app.py
from flask import Flask, request, jsonify
import joblib
import numpy as np
import pandas as pd
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from alibi_detect.drift import KolmogorovSmirnovDrift, ChiSquareDrift
import logging
import os
import json

app = Flask(__name__)

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Load the trained model and scaler
try:
    model = joblib.load('model.joblib')
    scaler = joblib.load('scaler.joblib')
    logging.info("Model and scaler loaded successfully.")
except FileNotFoundError as e:
    logging.error(f"Error loading model or scaler: {e}")
    raise e

# Global variables for storing training data statistics (for drift detection)
training_data = None
try:
    # Load the data used to train the model for calculating statistics. Replace this with your actual training data.
    X = np.random.rand(100, 5) # Generating dummy training data.  Replace with your actual training data loading mechanism.
    training_data = pd.DataFrame(X)
    logging.info("Training data loaded successfully for drift detection.")
except Exception as e:
    logging.error(f"Error loading training data for drift detection: {e}")
    training_data = None

# --- Performance Monitoring ---
def calculate_metrics(y_true, y_pred):
    """Calculates performance metrics."""
    accuracy = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred, zero_division=0)
    recall = recall_score(y_true, y_pred, zero_division=0)
    f1 = f1_score(y_true, y_pred, zero_division=0)
    return {'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1}


# --- Data Drift Detection ---
def detect_drift(current_data, training_data):
    """Detects data drift using Kolmogorov-Smirnov test for numerical features and Chi-Square for categorical."""

    drift_results = {}

    if training_data is None:
        logging.warning("Training data not available. Skipping drift detection.")
        return {"error": "Training data not available. Skipping drift detection."}

    numerical_features = training_data.select_dtypes(include=np.number).columns.tolist()
    categorical_features = training_data.select_dtypes(exclude=np.number).columns.tolist()

    # Initialize the KS test only once, outside of the loop.
    ks_drift = KolmogorovSmirnovDrift(p_val=0.05) #set p_val

    for feature in numerical_features:
        try:
            drift_test = ks_drift.predict(training_data[feature].values, current_data[feature].values) #Pass the data to predict.
            drift_results[feature] = {"data_drift": bool(drift_test['data']['is_drift'][0]), "p_value": drift_test['data']['p_val'][0]}
        except Exception as e:
             logging.error(f"Error performing KS test on {feature}: {e}")
             drift_results[feature] = {"error": str(e)} # Error message

    #Chi-squared test
    chi2_drift = ChiSquareDrift(p_val=0.05) # set p_val

    for feature in categorical_features:
        try:
            drift_test = chi2_drift.predict(training_data[feature].values, current_data[feature].values) #Pass the data to predict.
            drift_results[feature] = {"data_drift": bool(drift_test['data']['is_drift'][0]), "p_value": drift_test['data']['p_val'][0]}

        except Exception as e:
            logging.error(f"Error performing Chi-Square test on {feature}: {e}")
            drift_results[feature] = {"error": str(e)} # Error message

    return drift_results


# --- Prediction Endpoint ---
@app.route('/predict', methods=['POST'])
def predict():
    """API endpoint for making predictions and monitoring performance."""
    try:
        data = request.get_json()
        logging.info(f"Received data: {data}")

        # Input validation (minimal)
        if not isinstance(data, dict) or not all(key in data for key in ['features', 'target']): # Expecting data in the format {"features": [...], "target": actual_value}
            return jsonify({'error': 'Invalid input data format.  Expected format: {"features": [...], "target": actual_value}'}), 400

        features = data['features']
        target = data['target'] # Actual label for evaluation

        # Convert features to a NumPy array and scale the data
        features = np.array(features).reshape(1, -1) # Reshape to a 2D array for single sample prediction
        features_scaled = scaler.transform(features)


        # Make prediction
        prediction = model.predict(features_scaled)[0] # Access the prediction element not the whole array.


        # Calculate performance metrics
        metrics = calculate_metrics([target], [prediction])  # Pass targets and predictions as lists

        # Prepare data for drift detection. Convert to DataFrame
        current_data_point = pd.DataFrame([data['features']], columns=training_data.columns if training_data is not None else None) # ensures that it is a dataframe with the same column name
        # Drop non numerical columns before detecting drift if needed.  Adapt to your use case.
        #current_data_point = current_data_point.select_dtypes(include=np.number)

        drift_results = detect_drift(current_data_point, training_data)

        # Log the data, predictions, metrics, and drift results. Persist to database or file.
        log_data = {
            'features': data['features'],
            'target': target,
            'prediction': int(prediction),  # Convert prediction to int for JSON serialization
            'metrics': metrics,
            'drift_results': drift_results
        }

        logging.info(f"Prediction: {prediction}, Metrics: {metrics}, Drift Results: {drift_results}")

        # Store monitoring data to a log file
        with open('monitoring_log.txt', 'a') as f:
            f.write(json.dumps(log_data) + '\n')

        return jsonify({'prediction': int(prediction), 'metrics': metrics, 'drift_results': drift_results})

    except Exception as e:
        logging.error(f"Error processing request: {e}")
        return jsonify({'error': str(e)}), 500

if __name__ == '__main__':
    app.run(debug=True, host='0.0.0.0', port=5000)

```

**Explanation:**

- **Model Loading:** The Flask application loads the pre-trained model and scaler using `joblib`.
- **`predict` Endpoint:** This endpoint receives a JSON payload containing the input features (`features`) and the actual target value (`target`).
- **Input Validation:** Basic input validation is performed to ensure the data is in the expected format.
- **Prediction:** The input features are transformed using the loaded scaler and passed to the model for prediction.
- **Performance Metrics:** The `calculate_metrics` function computes accuracy, precision, recall, and F1-score by comparing the predicted value with the actual target.
- **Data Drift Detection:** The `detect_drift` function implements drift detection using `alibi-detect`. It uses the Kolmogorov-Smirnov test for numerical features and Chi-squared test for categorical features. This section assumes you have access to training data to compare against. Adapt the `training_data` loading mechanism to load your data. Also adapt the feature types to perform the appropiate drift test. It's important to know the feature types for accurate drift detection.
- **Logging:** The input data, predictions, performance metrics, and drift detection results are logged to `monitoring_log.txt`. In a real-world application, you would typically store this data in a database or a dedicated monitoring system.
- **Error Handling:** The code includes error handling to catch exceptions and return appropriate error responses.

**3. Run the Application:**

```plaintext
python app.py
```

This will start the Flask application on `http://0.0.0.0:5000`.

**4. Send a Prediction Request:**

Use `curl` or `Postman` to send a POST request to the `/predict` endpoint:

```plaintext
curl -X POST -H "Content-Type: application/json" -d '{"features": [0.6, 0.7, 0.2, 0.9, 0.3], "target": 1}' http://0.0.0.0:5000/predict
```

Replace `[0.6, 0.7, 0.2, 0.9, 0.3]` with your actual feature values and `1` with the correct target value.

**Expected Response:**

```plaintext
{
  "prediction": 1,
  "metrics": {
    "accuracy": 1.0,
    "precision": 1.0,
    "recall": 1.0,
    "f1": 1.0
  },
  "drift_results": {
    "0": {
      "data_drift": false,
      "p_value": 0.9861471645628705
    },
    "1": {
      "data_drift": false,
      "p_value": 0.4371751796239593
    },
    "2": {
      "data_drift": false,
      "p_value": 0.3159146728155623
    },
    "3": {
      "data_drift": false,
      "p_value": 0.5511619902103448
    },
    "4": {
      "data_drift": false,
      "p_value": 0.8827485635418377
    }
  }
}
```

The response includes the model's prediction, the calculated performance metrics, and the data drift detection results. You can observe if the data drift is detected or not using the `"data_drift": true/false` result.

**5. Check the monitoring_log.txt file**

Open the file to see the data being logged. Each line contains a JSON object representing the data, prediction, metrics and drift results.

## Advanced Monitoring Techniques

- **Concept Drift Detection:** Techniques like the Page-Hinkley test or the Drift Detection Method (DDM) can be used to detect changes in the relationship between input features and the target variable. These often require tracking the performance of the model over time and looking for statistically significant changes.
- **Explainable AI (XAI):** Using XAI techniques like SHAP or LIME can help understand the factors driving model predictions and identify potential biases or anomalies.
- **Custom Metrics:** Define custom metrics that are specific to your business problem and track them over time.
- **Advanced Alerting:** Integrate with alerting systems like PagerDuty or Slack to receive notifications when critical issues are detected.
- **Shadow Deployment:** Deploy the new version of the model alongside the existing model and compare their performance before fully replacing the old model.

## Integrating with Monitoring Tools

While this guide provides a basic implementation, you can integrate your Flask application with dedicated ML monitoring tools such as:

- **Evidently AI:** Open-source library for data and model monitoring.
- **Arize AI:** End-to-end ML observability platform.
- **WhyLabs:** ML monitoring platform.
- **Fiddler AI:** ML model performance management.
- **Datadog:** Infrastructure and application monitoring with ML capabilities.
- **Prometheus and Grafana:** Powerful open-source monitoring and alerting tools.

These tools offer features such as:

- Automated data and model profiling.
- Drift detection algorithms.
- Performance monitoring dashboards.
- Alerting and incident management.
- Root cause analysis.

## Conclusion

Implementing robust ML model monitoring is crucial for maintaining the accuracy, reliability, and trustworthiness of your deployed models. By using Flask, you can easily create an API to collect data, calculate performance metrics, detect data drift, and set up alerts. This guide provides a solid foundation for building a comprehensive ML model monitoring solution tailored to your specific needs. Remember to continuously adapt and improve your monitoring strategy as your models and data evolve. Don't forget to consider concept drift and integrate with dedicated monitoring tools for enhanced capabilities.
