---
title: 'Monitoring FastAPI Applications with Prometheus: A Comprehensive Guide'
date: '2024-01-26'
lastmod: '2024-01-26'
tags: ['fastapi', 'prometheus', 'monitoring', 'metrics', 'python', 'api', 'observability']
draft: false
summary: 'Learn how to add Prometheus metrics to your FastAPI applications for robust monitoring and observability. This comprehensive guide provides code examples and best practices for collecting and exposing metrics.'
authors: ['default']
---

# Monitoring FastAPI Applications with Prometheus: A Comprehensive Guide

In today's microservices-driven world, observability is crucial for maintaining the health and performance of your applications. Prometheus, a popular open-source monitoring and alerting toolkit, provides a powerful way to collect and analyze metrics. This guide will walk you through adding Prometheus metrics to your FastAPI applications, enabling you to gain valuable insights into their behavior.

## Why Use Prometheus with FastAPI?

- **Real-time Monitoring:** Prometheus allows you to monitor your FastAPI application's performance in real-time, identifying bottlenecks and potential issues before they impact users.
- **Historical Analysis:** Store and analyze metrics over time to identify trends, optimize resource usage, and plan for future growth.
- **Alerting:** Configure alerts based on metric thresholds to receive notifications when your application's performance deviates from expected norms.
- **Integration:** Seamlessly integrates with other monitoring tools like Grafana for visualization and dashboards.
- **Scalability:** Prometheus is designed to handle large volumes of metrics from numerous sources.

## Prerequisites

Before we begin, ensure you have the following installed:

- **Python 3.7+:** FastAPI requires Python 3.7 or higher.
- **FastAPI:** `pip install fastapi`
- **Uvicorn (or similar ASGI server):** `pip install uvicorn`
- **prometheus-client:** `pip install prometheus-client`

## Step-by-Step Guide to Adding Prometheus Metrics

### 1. Installing the Prometheus Client Library

The `prometheus-client` library provides the necessary tools to create and expose Prometheus metrics. We already covered this in the prerequisites, but just to be clear:

```bash
pip install prometheus-client
```

### 2. Creating and Registering Metrics

Let's start by creating some basic metrics within your FastAPI application. We'll use a `Counter` to track the number of requests, a `Histogram` to measure request latency, and a `Gauge` to monitor the number of active requests.

```plaintext
from fastapi import FastAPI, Request
from prometheus_client import Counter, Histogram, Gauge, make_asgi_app
import time

app = FastAPI()

# Define Prometheus metrics
REQUEST_COUNT = Counter(
    "http_requests_total", "Total number of HTTP requests.", ["method", "path"]
)
REQUEST_LATENCY = Histogram(
    "http_request_duration_seconds", "HTTP request latency in seconds.", ["method", "path"]
)
ACTIVE_REQUESTS = Gauge("http_active_requests_total", "Total number of active HTTP requests.")


# Middleware to track requests and latency
@app.middleware("http")
async def metrics_middleware(request: Request, call_next):
    ACTIVE_REQUESTS.inc()
    start_time = time.time()
    try:
        response = await call_next(request)
    except Exception as e:
        raise e
    finally:
        process_time = time.time() - start_time
        REQUEST_COUNT.labels(request.method, request.url.path).inc()
        REQUEST_LATENCY.labels(request.method, request.url.path).observe(process_time)
        ACTIVE_REQUESTS.dec()
        return response


@app.get("/")
async def root():
    return {"message": "Hello World"}

@app.get("/items/{item_id}")
async def read_item(item_id: int):
    return {"item_id": item_id}

# Expose metrics endpoint
metrics_app = make_asgi_app()
app.mount("/metrics", metrics_app)
```

**Explanation:**

- **Import necessary libraries:** We import `FastAPI`, `Request` from FastAPI, and `Counter`, `Histogram`, `Gauge`, `make_asgi_app` from `prometheus_client`.
- **Define Metrics:**
  - `REQUEST_COUNT`: A `Counter` that increments for each HTTP request. We label it with the HTTP method and path to differentiate between different endpoints.
  - `REQUEST_LATENCY`: A `Histogram` that tracks the duration of each HTTP request. We also label it with the HTTP method and path. Histograms provide buckets for analyzing the distribution of latencies.
  - `ACTIVE_REQUESTS`: A `Gauge` that tracks the number of requests currently being processed.
- **Metrics Middleware:**
  - The `metrics_middleware` is applied to all incoming requests.
  - `ACTIVE_REQUESTS.inc()`: Increments the `ACTIVE_REQUESTS` gauge at the beginning of the request.
  - `start_time = time.time()`: Records the start time of the request.
  - `try...except...finally`: Ensures the `ACTIVE_REQUESTS.dec()` is always called, even if an exception occurs during request processing.
  - `process_time = time.time() - start_time`: Calculates the request duration.
  - `REQUEST_COUNT.labels(...).inc()`: Increments the `REQUEST_COUNT` counter with the appropriate labels.
  - `REQUEST_LATENCY.labels(...).observe(process_time)`: Observes the request duration in the `REQUEST_LATENCY` histogram.
  - `ACTIVE_REQUESTS.dec()`: Decrements the `ACTIVE_REQUESTS` gauge at the end of the request.
- **Example Endpoints:** `/` and `/items/{item_id}` are simple example endpoints to generate traffic.
- **Expose Metrics Endpoint:** `app.mount("/metrics", metrics_app)` mounts the Prometheus metrics endpoint at `/metrics`. The `make_asgi_app()` function creates an ASGI application that exposes the collected metrics in the Prometheus text format.

### 3. Running Your FastAPI Application

Save the code above to a file (e.g., `main.py`) and run it using Uvicorn:

```bash
uvicorn main:app --reload
```

The `--reload` flag enables automatic reloading when you make changes to the code.

### 4. Accessing the Metrics

Open your web browser and navigate to `http://localhost:8000/metrics`. You should see a text-based output containing the Prometheus metrics, similar to this:

```
# HELP http_requests_total Total number of HTTP requests.
# TYPE http_requests_total counter
http_requests_total{method="GET",path="/"} 1.0
# HELP http_request_duration_seconds HTTP request latency in seconds.
# TYPE http_request_duration_seconds histogram
http_request_duration_seconds_bucket{method="GET",path="/",le="0.005"} 1.0
http_request_duration_seconds_bucket{method="GET",path="/",le="0.01"} 1.0
http_request_duration_seconds_bucket{method="GET",path="/",le="0.025"} 1.0
http_request_duration_seconds_bucket{method="GET",path="/",le="0.05"} 1.0
http_request_duration_seconds_bucket{method="GET",path="/",le="0.1"} 1.0
http_request_duration_seconds_bucket{method="GET",path="/",le="0.25"} 1.0
http_request_duration_seconds_bucket{method="GET",path="/",le="0.5"} 1.0
http_request_duration_seconds_bucket{method="GET",path="/",le="1.0"} 1.0
http_request_duration_seconds_bucket{method="GET",path="/",le="2.5"} 1.0
http_request_duration_seconds_bucket{method="GET",path="/",le="5.0"} 1.0
http_request_duration_seconds_bucket{method="GET",path="/",le="10.0"} 1.0
http_request_duration_seconds_bucket{method="GET",path="/",le="+inf"} 1.0
http_request_duration_seconds_sum{method="GET",path="/"} 0.000123456
http_request_duration_seconds_count{method="GET",path="/"} 1.0
# HELP http_active_requests_total Total number of active HTTP requests.
# TYPE http_active_requests_total gauge
http_active_requests_total 0.0
```

This is the data that Prometheus will scrape.

### 5. Configuring Prometheus to Scrape Metrics

Now, you need to configure Prometheus to scrape metrics from your FastAPI application. Create a `prometheus.yml` configuration file with the following content (adjust the target address if your application is running on a different host or port):

```yaml
global:
  scrape_interval: 15s # Scrape every 15 seconds.
  evaluation_interval: 15s # Evaluate rules every 15 seconds.

scrape_configs:
  - job_name: 'fastapi'
    static_configs:
      - targets: ['localhost:8000']
    metrics_path: /metrics
```

**Explanation:**

- **`scrape_interval`**: Defines how often Prometheus scrapes metrics from the targets.
- **`evaluation_interval`**: Defines how often Prometheus evaluates alerting rules.
- **`scrape_configs`**: Contains a list of scrape jobs.
  - **`job_name`**: A name for the scrape job (e.g., 'fastapi').
  - **`static_configs`**: Defines the targets to scrape.
    - **`targets`**: A list of target addresses (e.g., `['localhost:8000']`).
  - **`metrics_path`**: Specifies the path where the metrics are exposed (e.g., `/metrics`).

### 6. Running Prometheus

Download and install Prometheus from the official website: [https://prometheus.io/download/](https://prometheus.io/download/)

Start Prometheus using the following command, pointing it to your `prometheus.yml` configuration file:

```bash
./prometheus --config.file=prometheus.yml
```

(Adjust the command based on your operating system and installation directory).

### 7. Visualizing Metrics with Grafana (Optional)

While you can query Prometheus directly using its expression language (PromQL), Grafana provides a much more user-friendly interface for visualizing and analyzing metrics.

1.  **Install Grafana:** Follow the instructions on the Grafana website: [https://grafana.com/docs/grafana/latest/installation/](https://grafana.com/docs/grafana/latest/installation/)
2.  **Add Prometheus as a Data Source:**
    - Open Grafana in your web browser (usually at `http://localhost:3000`).
    - Go to "Configuration" -> "Data sources".
    - Click "Add data source".
    - Select "Prometheus".
    - Enter the Prometheus URL (e.g., `http://localhost:9090`).
    - Click "Save & test".
3.  **Create a Dashboard:**

    - Click the "+" icon in the left sidebar and select "Dashboard".
    - Click "Add new panel".
    - Select your Prometheus data source.
    - Use PromQL queries to visualize the metrics you created. For example, to graph the request rate, you could use the following query:

      ```promql
      rate(http_requests_total{method="GET",path="/"}[5m])
      ```

      This query calculates the rate of requests to the root path (`/`) over the last 5 minutes.

      Another example to graph the request duration, you could use the following query:

      ```promql
      histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket[5m])) by (le, method, path))
      ```

      This query calculates the 95th percentile of the request duration over the last 5 minutes. It provides insights into the typical latency experienced by users.

    - Configure the panel title, axes, and other options as desired.
    - Save the dashboard.

## Best Practices

- **Use Meaningful Metric Names and Labels:** Choose names that clearly describe what the metric represents. Use labels to add dimensions to your metrics, allowing you to filter and aggregate data (e.g., by endpoint, HTTP method, status code).
- **Don't Expose Sensitive Data:** Be careful not to include sensitive information in your metrics.
- **Choose the Right Metric Type:** Select the appropriate metric type (Counter, Gauge, Histogram, Summary) based on the nature of the data you're collecting.
- **Regularly Review and Refine Your Metrics:** As your application evolves, you may need to add, remove, or modify your metrics to ensure you're collecting the most relevant data.
- **Use Alerting:** Set up alerts based on metric thresholds to receive notifications when your application's performance deviates from expected norms.
- **Consider Custom Metrics:** While the basic metrics we covered are useful, you may need to create custom metrics to track application-specific events or behaviors. For example, you might want to track the number of successful transactions, the number of failed logins, or the usage of a specific feature.

## Advanced Considerations

- **Using a Prometheus Exporter:** For more complex applications, you might consider using a dedicated Prometheus exporter. Exporters are standalone applications that collect metrics from a specific system or service and expose them in the Prometheus format.
- **Service Discovery:** In dynamic environments (e.g., Kubernetes), service discovery is essential for automatically discovering and monitoring your application instances. Prometheus supports various service discovery mechanisms.
- **Remote Write:** For long-term storage and analysis of metrics, you can use Prometheus's remote write feature to send metrics to external storage systems.
- **Aggregation and Federation:** Prometheus provides features for aggregating and federating metrics from multiple Prometheus instances.

## Conclusion

Adding Prometheus metrics to your FastAPI applications is a crucial step towards achieving robust monitoring and observability. By following the steps outlined in this guide, you can gain valuable insights into your application's performance, identify potential issues, and optimize its behavior. Remember to choose meaningful metric names and labels, regularly review your metrics, and leverage Grafana for powerful visualization and analysis. Happy monitoring!
