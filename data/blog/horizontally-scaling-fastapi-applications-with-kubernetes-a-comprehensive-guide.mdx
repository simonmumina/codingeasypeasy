---
title: 'Horizontally Scaling FastAPI Applications with Kubernetes: A Comprehensive Guide'
date: '2024-10-27'
lastmod: '2024-10-27'
tags:
  [
    'fastapi',
    'kubernetes',
    'k8s',
    'horizontal scaling',
    'docker',
    'cicd',
    'uvicorn',
    'gunicorn',
    'load balancing',
    'microservices',
    'python',
  ]
draft: false
summary: 'Learn how to horizontally scale your FastAPI application using Kubernetes. This comprehensive guide covers Dockerization, deployment configurations, load balancing, and best practices for building resilient and scalable Python microservices.'
authors: ['default']
---

# Horizontally Scaling FastAPI Applications with Kubernetes: A Comprehensive Guide

FastAPI has emerged as a popular framework for building high-performance Python APIs. As your application grows, handling increased traffic and ensuring high availability becomes crucial. This is where horizontal scaling comes in, and Kubernetes (K8s) provides a powerful platform to achieve it. This guide walks you through the process of horizontally scaling a FastAPI application using Kubernetes, covering everything from Dockerizing your application to configuring deployments and services.

## Why Horizontal Scaling and Kubernetes?

- **Horizontal Scaling:** Instead of upgrading a single server (vertical scaling), horizontal scaling involves adding more servers to handle the load. This offers better resilience, scalability, and cost-effectiveness.

- **Kubernetes (K8s):** Kubernetes is a container orchestration system that automates the deployment, scaling, and management of containerized applications. It simplifies the process of managing multiple instances of your application and provides features like self-healing, load balancing, and automated rollouts.

## Prerequisites

- **Basic understanding of Docker:** You should be familiar with Dockerfiles and containerization.
- **Basic understanding of Kubernetes:** Knowledge of Pods, Deployments, Services, and Namespaces is helpful.
- **Docker installed on your machine.**
- **kubectl installed on your machine.**
- **A Kubernetes cluster:** This can be a local cluster (Minikube, Kind), a cloud-based cluster (AWS EKS, Google GKE, Azure AKS), or any other Kubernetes environment.

## Step 1: Creating a Simple FastAPI Application

Let's start with a basic FastAPI application. Create a file named `main.py`:

```plaintext
# main.py
from fastapi import FastAPI
import os

app = FastAPI()

@app.get("/")
async def root():
    return {"message": "Hello World from FastAPI!", "pod_name": os.environ.get("HOSTNAME", "unknown")}
```

This simple app has a single endpoint `/` that returns a "Hello World" message and the pod's hostname (useful for identifying which instance is handling the request).

## Step 2: Dockerizing the FastAPI Application

To run our application in Kubernetes, we need to containerize it using Docker. Create a file named `Dockerfile` in the same directory as `main.py`:

```dockerfile
# Dockerfile
FROM python:3.9-slim-buster

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

And a `requirements.txt` file:

```
# requirements.txt
fastapi
uvicorn[standard]
```

**Explanation:**

- `FROM python:3.9-slim-buster`: Uses a slim Python 3.9 base image.
- `WORKDIR /app`: Sets the working directory inside the container.
- `COPY requirements.txt .`: Copies the requirements file.
- `RUN pip install --no-cache-dir -r requirements.txt`: Installs the dependencies. The `--no-cache-dir` flag prevents caching, reducing image size.
- `COPY . .`: Copies the entire application code into the container.
- `CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]`: Specifies the command to run the application using Uvicorn, an ASGI server. We use `0.0.0.0` to allow the container to listen on all interfaces, and port `8000` (which will be important later).

Now, build the Docker image:

```plaintext
docker build -t fastapi-app .
```

You can run the image locally to test it:

```plaintext
docker run -p 8000:8000 fastapi-app
```

Open your browser and go to `http://localhost:8000` to see the "Hello World" message.

## Step 3: Creating Kubernetes Deployment and Service

Now, let's define the Kubernetes deployment and service configurations. Create a file named `deployment.yaml`:

```plaintext
# deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: fastapi-deployment
spec:
  replicas: 3 # Start with 3 replicas for scaling
  selector:
    matchLabels:
      app: fastapi-app
  template:
    metadata:
      labels:
        app: fastapi-app
    spec:
      containers:
        - name: fastapi-container
          image: fastapi-app # Replace with your image name if pushed to registry
          ports:
            - containerPort: 8000
          env:
            - name: HOSTNAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
          resources:
            requests:
              cpu: '100m' # Request 100 millicores
              memory: '128Mi' # Request 128 MiB of memory
            limits:
              cpu: '200m' # Limit to 200 millicores
              memory: '256Mi' # Limit to 256 MiB of memory
```

And a file named `service.yaml`:

```plaintext
# service.yaml
apiVersion: v1
kind: Service
metadata:
  name: fastapi-service
spec:
  type: LoadBalancer # Expose the service externally (NodePort is another option)
  selector:
    app: fastapi-app
  ports:
    - protocol: TCP
      port: 80
      targetPort: 8000
```

**Explanation:**

- **Deployment (deployment.yaml):**
  - `apiVersion: apps/v1` and `kind: Deployment`: Defines a Deployment resource.
  - `replicas: 3`: Specifies that we want 3 replicas of our application running. This is the key to horizontal scaling. You can increase this number to scale your application.
  - `selector`: Matches pods with the label `app: fastapi-app`.
  - `template`: Defines the pod specification:
    - `image: fastapi-app`: Specifies the Docker image to use. **Important:** If you've pushed your image to a registry like Docker Hub, replace `fastapi-app` with the full image name (e.g., `your-dockerhub-username/fastapi-app:latest`).
    - `containerPort: 8000`: Exposes port 8000 inside the container.
    - `env`: Sets the environment variable `HOSTNAME` from the pod's metadata. This allows the application to identify which pod it's running in.
    - `resources`: Defines the resource requests and limits for the container. This helps Kubernetes schedule the pods effectively and prevent resource contention. Adjust these values based on your application's needs.
- **Service (service.yaml):**
  - `apiVersion: v1` and `kind: Service`: Defines a Service resource.
  - `type: LoadBalancer`: Creates a LoadBalancer service. This will provision an external load balancer (if supported by your Kubernetes environment) and expose the application externally. If you're using a local cluster like Minikube, you might need to use `NodePort` instead.
  - `selector`: Matches pods with the label `app: fastapi-app`.
  - `ports`:
    - `port: 80`: The port on which the service will be exposed.
    - `targetPort: 8000`: The port on which the application is listening inside the container.

## Step 4: Deploying to Kubernetes

Apply the deployment and service configurations:

```plaintext
kubectl apply -f deployment.yaml
kubectl apply -f service.yaml
```

Check the status of the deployment and service:

```plaintext
kubectl get deployments
kubectl get services
kubectl get pods
```

Wait until the pods are running and the service has an external IP address (if you're using `LoadBalancer`). If you're using Minikube, you can use the command `minikube service fastapi-service` to get the URL.

Once the service is running, you can access your FastAPI application through the external IP or URL provided by Kubernetes. Refresh the page multiple times. You should see the `pod_name` change, indicating that the load balancer is distributing traffic across the different replicas.

## Step 5: Scaling the Application

To scale the application horizontally, you can simply change the `replicas` field in the `deployment.yaml` file and apply the changes:

```plaintext
kubectl apply -f deployment.yaml
```

Alternatively, you can use the `kubectl scale` command:

```plaintext
kubectl scale deployment fastapi-deployment --replicas=5
```

This will increase the number of replicas to 5. Verify the change using `kubectl get deployments` and `kubectl get pods`.

## Step 6: Implementing Health Checks (Readiness and Liveness Probes)

To ensure Kubernetes knows when your application is healthy and ready to receive traffic, implement health checks. Add the following to your `deployment.yaml` under the `containers` section:

```plaintext
containers:
  - name: fastapi-container
    image: fastapi-app
    ports:
      - containerPort: 8000
    readinessProbe:
      httpGet:
        path: /
        port: 8000
      initialDelaySeconds: 5
      periodSeconds: 10
    livenessProbe:
      httpGet:
        path: /
        port: 8000
      initialDelaySeconds: 15
      periodSeconds: 10
```

**Explanation:**

- **`readinessProbe`:** Checks if the application is ready to receive traffic. If the probe fails, the pod will be removed from the service's endpoints until it passes again.
- **`livenessProbe`:** Checks if the application is still running. If the probe fails, Kubernetes will restart the pod.
- **`httpGet`:** Performs an HTTP GET request to the specified path and port. In this case, we're using the root path `/`, but you might want to create a dedicated health check endpoint in your application for more sophisticated checks.
- **`initialDelaySeconds`:** The number of seconds to wait after the container has started before the probes are initiated.
- **`periodSeconds`:** How often (in seconds) to perform the probe.

Apply the changes to your deployment:

```plaintext
kubectl apply -f deployment.yaml
```

## Step 7: Setting Resource Limits and Requests

We've already included the `resources` section in the `deployment.yaml`. Let's revisit it. It's crucial to set resource requests and limits for your containers to ensure efficient resource utilization and prevent resource contention.

```plaintext
resources:
  requests:
    cpu: '100m' # Request 100 millicores
    memory: '128Mi' # Request 128 MiB of memory
  limits:
    cpu: '200m' # Limit to 200 millicores
    memory: '256Mi' # Limit to 256 MiB of memory
```

- **`requests`:** Specifies the minimum amount of resources that the container requires. Kubernetes uses this information to schedule the pod on a node that has enough available resources.
- **`limits`:** Specifies the maximum amount of resources that the container can use. If the container tries to exceed these limits, it will be throttled (CPU) or killed (memory).

**Important:** Choose appropriate values for requests and limits based on your application's resource consumption. Too low, and your application might suffer from performance issues. Too high, and you might be wasting resources.

## Step 8: Implementing Auto-Scaling (Horizontal Pod Autoscaler - HPA)

While manually scaling the application is possible, Kubernetes offers an automated solution called Horizontal Pod Autoscaler (HPA). HPA automatically adjusts the number of pods in a deployment based on observed CPU utilization or other metrics.

Create a file named `hpa.yaml`:

```plaintext
# hpa.yaml
apiVersion: autoscaling/v2beta2
kind: HorizontalPodAutoscaler
metadata:
  name: fastapi-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: fastapi-deployment
  minReplicas: 3
  maxReplicas: 10
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70
```

**Explanation:**

- `scaleTargetRef`: Specifies the deployment that the HPA will manage.
- `minReplicas`: The minimum number of replicas.
- `maxReplicas`: The maximum number of replicas.
- `metrics`: Defines the metrics that the HPA will use to determine when to scale. In this example, we're using CPU utilization.
  - `averageUtilization: 70`: The HPA will try to maintain an average CPU utilization of 70% across all pods.

Apply the HPA configuration:

```plaintext
kubectl apply -f hpa.yaml
```

Check the status of the HPA:

```plaintext
kubectl get hpa
```

To trigger auto-scaling, you need to generate load on your application. You can use tools like `hey` or `loadtest` to simulate traffic.

```plaintext
# Example using hey (install with go install github.com/rakyll/hey@latest)
hey -n 1000 -c 100 http://<your-service-ip>
```

This command sends 1000 requests with a concurrency of 100 to your service. Monitor the HPA status using `kubectl get hpa` to see if the number of replicas is increasing.

## Step 9: Choosing the Right Web Server: Uvicorn vs Gunicorn

In our Dockerfile, we used Uvicorn as the web server. While Uvicorn is a great choice for development and simple deployments, Gunicorn often provides better performance in production environments, especially when combined with Uvicorn workers.

Here's how to integrate Gunicorn:

First, update your `requirements.txt`:

```
fastapi
gunicorn
uvicorn[standard]
```

Then, modify your `Dockerfile`:

```dockerfile
# Dockerfile (Modified for Gunicorn)
FROM python:3.9-slim-buster

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

CMD ["gunicorn", "main:app", "--workers", "3", "--worker-class", "uvicorn.workers.UvicornWorker", "--bind", "0.0.0.0:8000"]
```

**Explanation:**

- We're now using `gunicorn` as the main command.
- `--workers 3`: Specifies the number of worker processes. A good starting point is 2-4 workers per CPU core.
- `--worker-class uvicorn.workers.UvicornWorker`: Specifies that we want to use Uvicorn workers with Gunicorn.
- `--bind 0.0.0.0:8000`: Binds Gunicorn to port 8000.

Rebuild your Docker image:

```plaintext
docker build -t fastapi-app .
```

Redeploy your application to Kubernetes using the updated image.

**Benefits of Gunicorn with Uvicorn Workers:**

- **Improved Concurrency:** Gunicorn manages multiple worker processes, allowing your application to handle multiple requests concurrently.
- **Better Resource Utilization:** Gunicorn can distribute requests across multiple CPU cores.
- **Process Management:** Gunicorn provides process management features, such as automatic restarts in case of worker failures.

## Step 10: Monitoring and Logging

Effective monitoring and logging are crucial for understanding your application's performance and identifying potential issues.

- **Monitoring:** Use tools like Prometheus and Grafana to collect and visualize metrics about your application's performance, resource utilization, and request latency.
- **Logging:** Implement structured logging in your application and use a centralized logging system like Elasticsearch, Fluentd, and Kibana (EFK stack) or Loki to collect and analyze logs from all your pods.

## Step 11: CI/CD Pipeline

Automate the process of building, testing, and deploying your application using a CI/CD pipeline. Tools like Jenkins, GitLab CI, GitHub Actions, or CircleCI can be used to create automated workflows that build your Docker image, run tests, and deploy the application to your Kubernetes cluster.

## Conclusion

Horizontally scaling your FastAPI application with Kubernetes is a powerful way to ensure high availability, handle increased traffic, and improve performance. This guide has covered the essential steps, from Dockerizing your application to configuring deployments, services, and auto-scaling. By following these best practices, you can build resilient and scalable Python microservices that can handle the demands of modern applications. Remember to continuously monitor and optimize your application's performance to ensure it meets your specific needs.
