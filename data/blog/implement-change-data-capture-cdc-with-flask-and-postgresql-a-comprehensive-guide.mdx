---
title: 'Implement Change Data Capture (CDC) with Flask and PostgreSQL: A Comprehensive Guide'
date: '2024-01-01'
lastmod: '2024-10-27'
tags:
  [
    'flask',
    'postgresql',
    'cdc',
    'change-data-capture',
    'database',
    'python',
    'real-time',
    'event-driven',
    'trigger',
    'logical-replication',
  ]
draft: false
summary: 'Learn how to implement Change Data Capture (CDC) in your Flask application using PostgreSQL.  This comprehensive guide covers triggers, logical replication, and polling approaches with detailed code examples and best practices for building real-time data pipelines.'
authors: ['default']
---

# Implement Change Data Capture (CDC) with Flask and PostgreSQL: A Comprehensive Guide

Change Data Capture (CDC) is a powerful technique for capturing and tracking data changes in a database as they occur. It allows you to build real-time data pipelines, update caches, trigger notifications, and perform various other data-driven tasks. This blog post provides a comprehensive guide on implementing CDC with Flask and PostgreSQL, exploring different approaches and providing practical code examples.

## Why Use Change Data Capture?

Traditional methods of data integration, like batch processing, often involve periodic full refreshes, which can be inefficient and lead to data latency. CDC overcomes these limitations by:

- **Real-time updates:** Data changes are captured and propagated almost immediately.
- **Reduced latency:** Enables timely updates to downstream systems.
- **Minimal impact on source database:** Avoids resource-intensive full table scans.
- **Event-driven architecture:** Facilitates building event-driven applications.

## CDC Implementation Approaches with Flask and PostgreSQL

There are several approaches to implementing CDC with Flask and PostgreSQL. We'll explore the most common and practical methods:

1.  **Database Triggers:** Using PostgreSQL triggers to record changes in a separate audit table.
2.  **Logical Replication:** Leveraging PostgreSQL's logical replication capabilities.
3.  **Polling (Less Recommended):** Regularly querying the database for changes based on a timestamp or sequence number.

### 1. CDC with Database Triggers

This approach involves creating database triggers that are fired whenever data is inserted, updated, or deleted in a target table. These triggers then record the changes in a separate audit table.

**Advantages:**

- Simple to implement for basic CDC needs.
- Doesn't require significant changes to the existing database setup.

**Disadvantages:**

- Can impact database performance if triggers are not optimized.
- Adds complexity to database schema management.
- Doesn't capture the exact state of the data before the change (pre-image) without additional effort.

**Implementation Steps:**

**a. Define the Target Table:**

Let's consider a `products` table:

```sql
CREATE TABLE products (
    id SERIAL PRIMARY KEY,
    name VARCHAR(255) NOT NULL,
    price DECIMAL(10, 2) NOT NULL,
    description TEXT,
    created_at TIMESTAMP WITHOUT TIME ZONE DEFAULT (NOW() at time zone 'utc'),
    updated_at TIMESTAMP WITHOUT TIME ZONE DEFAULT (NOW() at time zone 'utc')
);
```

**b. Create the Audit Table:**

This table will store the change records:

```sql
CREATE TABLE products_audit (
    id SERIAL PRIMARY KEY,
    product_id INTEGER NOT NULL,
    operation VARCHAR(10) NOT NULL, -- INSERT, UPDATE, DELETE
    name VARCHAR(255),
    price DECIMAL(10, 2),
    description TEXT,
    created_at TIMESTAMP WITHOUT TIME ZONE,
    updated_at TIMESTAMP WITHOUT TIME ZONE,
    changed_at TIMESTAMP WITHOUT TIME ZONE DEFAULT (NOW() at time zone 'utc'),
    FOREIGN KEY (product_id) REFERENCES products(id)
);
```

**c. Create the Triggers:**

We'll create triggers for `INSERT`, `UPDATE`, and `DELETE` operations:

```sql
CREATE OR REPLACE FUNCTION products_audit_insert()
RETURNS TRIGGER AS $$
BEGIN
    INSERT INTO products_audit (product_id, operation, name, price, description, created_at, updated_at)
    VALUES (NEW.id, 'INSERT', NEW.name, NEW.price, NEW.description, NEW.created_at, NEW.updated_at);
    RETURN NEW;
END;
$$ LANGUAGE plpgsql;

CREATE TRIGGER products_insert_trigger
AFTER INSERT ON products
FOR EACH ROW
EXECUTE FUNCTION products_audit_insert();

CREATE OR REPLACE FUNCTION products_audit_update()
RETURNS TRIGGER AS $$
BEGIN
    INSERT INTO products_audit (product_id, operation, name, price, description, created_at, updated_at)
    VALUES (NEW.id, 'UPDATE', NEW.name, NEW.price, NEW.description, NEW.created_at, NEW.updated_at);
    RETURN NEW;
END;
$$ LANGUAGE plpgsql;

CREATE TRIGGER products_update_trigger
AFTER UPDATE ON products
FOR EACH ROW
EXECUTE FUNCTION products_audit_update();

CREATE OR REPLACE FUNCTION products_audit_delete()
RETURNS TRIGGER AS $$
BEGIN
    INSERT INTO products_audit (product_id, operation, name, price, description, created_at, updated_at)
    VALUES (OLD.id, 'DELETE', OLD.name, OLD.price, OLD.description, OLD.created_at, OLD.updated_at);
    RETURN OLD;
END;
$$ LANGUAGE plpgsql;

CREATE TRIGGER products_delete_trigger
AFTER DELETE ON products
FOR EACH ROW
EXECUTE FUNCTION products_audit_delete();
```

**d. Flask Integration:**

Now, in your Flask application, you can query the `products_audit` table to retrieve change records.

```plaintext
from flask import Flask
import psycopg2
import os

app = Flask(__name__)

# Database configuration
DATABASE_URL = os.environ.get("DATABASE_URL", "postgresql://user:password@localhost:5432/mydatabase")

def get_db_connection():
    conn = psycopg2.connect(DATABASE_URL)
    return conn

@app.route('/changes')
def get_changes():
    conn = get_db_connection()
    cur = conn.cursor()
    cur.execute("SELECT * FROM products_audit ORDER BY changed_at DESC")
    changes = cur.fetchall()
    cur.close()
    conn.close()

    # Format the data as needed (e.g., JSON)
    change_list = []
    for change in changes:
        change_list.append({
            'id': change[0],
            'product_id': change[1],
            'operation': change[2],
            'name': change[3],
            'price': change[4],
            'description': change[5],
            'created_at': change[6],
            'updated_at': change[7],
            'changed_at': change[8]
        })

    return {'changes': change_list}

if __name__ == '__main__':
    app.run(debug=True)
```

**Explanation:**

- The Flask route `/changes` queries the `products_audit` table.
- The results are fetched and formatted into a list of dictionaries.
- The data is returned as a JSON response.

### 2. CDC with Logical Replication

PostgreSQL's logical replication provides a more robust and efficient way to capture data changes. It allows you to subscribe to a stream of changes based on a _publication_.

**Advantages:**

- Less impact on the source database performance compared to triggers.
- Supports replication to different database systems (e.g., other PostgreSQL instances, message queues).
- Can capture both pre-image and post-image of the data.

**Disadvantages:**

- Requires more configuration and setup.
- Requires a separate logical decoding plugin.

**Implementation Steps:**

**a. Configure PostgreSQL for Logical Replication:**

1.  **Enable WAL archiving:**

    In your `postgresql.conf` file:

    ```
    wal_level = logical
    max_wal_senders = 10  # Adjust as needed
    max_replication_slots = 10 # Adjust as needed
    ```

    Restart the PostgreSQL server after making these changes.

2.  **Create a Publication:**

    A publication specifies which tables and changes will be replicated.

    ```sql
    CREATE PUBLICATION my_publication FOR TABLE products;
    ```

    To replicate all tables in a schema:

    ```sql
    CREATE PUBLICATION my_publication FOR ALL TABLES;
    ```

**b. Choose a Logical Decoding Plugin:**

PostgreSQL doesn't inherently know how to format the logical replication stream. You need a _logical decoding plugin_ to convert the raw changes into a usable format (e.g., JSON, CSV). Popular options include:

- **`wal2json`:** A widely used plugin that outputs changes as JSON.
- **`decoderbufs`:** A plugin that uses Google Protocol Buffers for encoding.

For this example, we'll use `wal2json`. Install it on your PostgreSQL server (the installation process varies depending on your operating system).

**c. Create a Replication Slot:**

A replication slot is a persistent connection to the database that tracks the changes. This prevents the database from deleting WAL segments that haven't been processed by the subscriber.

**d. Python Subscriber using `psycopg2` and a separate process:**

The best way to handle logical replication is often with a dedicated process that consumes the replication stream. Here's an example illustrating the concepts, though production implementations often involve more robust error handling and configuration.

```plaintext
import psycopg2
import psycopg2.extras
import os
import json
import time
from multiprocessing import Process

DATABASE_URL = os.environ.get("DATABASE_URL", "postgresql://user:password@localhost:5432/mydatabase")
REPLICATION_SLOT = "my_replication_slot"
PUBLICATION_NAME = "my_publication"

def consume_changes():
    try:
        conn = psycopg2.connect(DATABASE_URL, replication="logical", options="-c wal_level=logical")
        conn.set_isolation_level(psycopg2.extensions.ISOLATION_LEVEL_AUTOCOMMIT)
        cur = conn.cursor()

        # Attempt to create the replication slot if it doesn't exist
        try:
            cur.execute(f"SELECT slot_name FROM pg_replication_slots WHERE slot_name = '{REPLICATION_SLOT}'")
            if not cur.fetchone():
                cur.execute(f"SELECT pg_create_logical_replication_slot('{REPLICATION_SLOT}', 'wal2json')")
                conn.commit()  # Commit immediately after creating the slot

        except psycopg2.errors.DuplicateObject:
            print(f"Replication slot '{REPLICATION_SLOT}' already exists.")
        except Exception as e:
            print(f"Error creating replication slot: {e}")
            conn.close()
            return

        cur.execute(f"START_REPLICATION SLOT {REPLICATION_SLOT} LOGICAL 0/0 (proto_version '1', publication_names '{{{PUBLICATION_NAME}}}')")

        while True:
            try:
                cur.poll()
                if cur.consume() == psycopg2.extensions.STATUS_COPY:
                    msg = cur.copy_read().decode('utf-8')
                    if msg:
                        try:
                            data = json.loads(msg)
                            # Process the data changes here.  For example, send to a queue or update a cache
                            print("Received change:", json.dumps(data, indent=2))
                        except json.JSONDecodeError:
                            print(f"Error decoding JSON message: {msg}")
                        except Exception as e:
                            print(f"Error processing message: {e}")


            except psycopg2.Error as e:
                print(f"Database error: {e}")
                conn.rollback()
                time.sleep(5) #Re-attempt connection.

            except Exception as e:
                print(f"Unexpected error: {e}")
                time.sleep(5) #Re-attempt connection.

            time.sleep(0.1)

    except psycopg2.Error as e:
        print(f"Connection error: {e}")
    finally:
        if conn:
            conn.close()


#Flask Example using Multiprocessing
from flask import Flask

app = Flask(__name__)


@app.route('/')
def hello_world():
    return 'Hello, World!'


if __name__ == '__main__':
    # Start the CDC consumer in a separate process
    cdc_process = Process(target=consume_changes)
    cdc_process.start()

    # Start the Flask app
    app.run(debug=True)
```

**Explanation:**

- The `consume_changes` function establishes a logical replication connection to the database.
- It creates a replication slot (if it doesn't exist). **Important:** You'll need `SUPERUSER` privileges for this.
- It starts the replication stream using `START_REPLICATION`.
- The `cur.poll()` and `cur.consume()` methods continuously check for incoming changes.
- When changes are received, the `copy_read()` method retrieves the data.
- The data is then decoded from JSON using `json.loads(msg)`.
- **Important:** The `print` statement is a placeholder. In a real application, you would process the changes, e.g., by sending them to a message queue (like Kafka or RabbitMQ), updating a cache, or performing other actions.
- Error handling is included to attempt to reconnect to the database if necessary.

**Important Considerations for Logical Replication:**

- **Plugin Installation:** Make sure you have `wal2json` (or your chosen plugin) correctly installed and configured in PostgreSQL. The installation process varies by operating system.
- **Replication User:** The user connecting to the replication stream needs the `REPLICATION` privilege in PostgreSQL. You may need to create a dedicated user for this purpose. For example: `CREATE USER repl_user WITH REPLICATION LOGIN PASSWORD 'your_password';`
- **Firewall Rules:** Ensure your firewall allows connections from the Flask application server to the PostgreSQL server on the replication port (usually 5432).
- **Slot Management:** Monitor your replication slots. If a subscriber disconnects and doesn't reconnect, the slot can accumulate WAL segments, potentially leading to disk space issues. You can manually remove inactive slots using: `SELECT pg_drop_replication_slot('my_replication_slot');`
- **Message Queue Integration:** For production systems, integrate with a message queue (Kafka, RabbitMQ) to decouple the change capture process from downstream consumers. This improves scalability and resilience.
- **Scalability**: The above consumer is designed to read the changes, scale accordingly when using multiple consumers.

**Running the Example:**

1.  **Install Dependencies:** `pip install Flask psycopg2`
2.  **Set the DATABASE_URL environment variable:** `export DATABASE_URL="postgresql://your_user:your_password@localhost:5432/your_database"`
3.  **Run the Flask application:** `python your_app_file.py`

Now, whenever you insert, update, or delete data in the `products` table, the `consume_changes` function will print the changes (in JSON format) to the console.

### 3. Polling (Least Recommended)

This approach involves regularly querying the database for changes based on a timestamp or sequence number. While simple to understand, it's generally **not recommended** for high-volume or real-time scenarios.

**Advantages:**

- Simple to implement for very basic use cases.
- No need for triggers or logical replication.

**Disadvantages:**

- Inefficient, as it involves periodic full or incremental scans of the table.
- Can miss changes if the polling interval is too long.
- High latency.
- Puts unnecessary load on the database.

**Example:**

```plaintext
from flask import Flask
import psycopg2
import os
import time

app = Flask(__name__)

# Database configuration
DATABASE_URL = os.environ.get("DATABASE_URL", "postgresql://user:password@localhost:5432/mydatabase")
LAST_PROCESSED_TIMESTAMP = None  # Initial timestamp

def get_db_connection():
    conn = psycopg2.connect(DATABASE_URL)
    return conn

@app.route('/poll_changes')
def poll_changes():
    global LAST_PROCESSED_TIMESTAMP
    conn = get_db_connection()
    cur = conn.cursor()

    if LAST_PROCESSED_TIMESTAMP is None:
        # First run, get all records
        cur.execute("SELECT id, name, price, description, updated_at FROM products ORDER BY updated_at")
    else:
        cur.execute("SELECT id, name, price, description, updated_at FROM products WHERE updated_at > %s ORDER BY updated_at", (LAST_PROCESSED_TIMESTAMP,))

    changes = cur.fetchall()
    cur.close()
    conn.close()

    change_list = []
    for change in changes:
        change_list.append({
            'id': change[0],
            'name': change[1],
            'price': change[2],
            'description': change[3],
            'updated_at': change[4]
        })
        LAST_PROCESSED_TIMESTAMP = change[4] # Store the latest timestamp

    return {'changes': change_list}

if __name__ == '__main__':
    app.run(debug=True)
```

**Explanation:**

- The `poll_changes` route queries the `products` table for records with `updated_at` timestamps greater than the `LAST_PROCESSED_TIMESTAMP`.
- The `LAST_PROCESSED_TIMESTAMP` is updated after each poll.
- **Important:** This approach is highly inefficient and should be avoided for production systems, especially with frequently changing data. It also misses any DELETE operations and might miss UPDATE operations if the polling frequency is too low and `updated_at` does not update.

## Best Practices for CDC Implementation

- **Choose the Right Approach:** Consider your requirements and constraints when selecting a CDC method. Logical replication is generally preferred for performance and scalability.
- **Optimize Triggers (if using):** Ensure trigger functions are well-optimized to minimize performance impact. Consider using `BEFORE` triggers instead of `AFTER` triggers when appropriate.
- **Use a Message Queue:** Decouple change capture from downstream consumers using a message queue.
- **Handle Errors Gracefully:** Implement robust error handling and retry mechanisms.
- **Monitor Performance:** Monitor database performance and adjust parameters as needed. Pay attention to WAL size, replication lag, and trigger execution times.
- **Secure Your Connections:** Use secure connections (SSL/TLS) for database and message queue communication.
- **Properly Handle Schema Evolution:** Account for schema changes in your CDC pipeline. Consider using schema registries and versioning strategies.

## Conclusion

Change Data Capture is a valuable technique for building real-time data pipelines and event-driven applications with Flask and PostgreSQL. While database triggers provide a simple starting point, logical replication offers a more robust and scalable solution. By carefully considering your requirements and following best practices, you can implement CDC effectively and unlock the power of real-time data updates. Remember to prioritize logical replication unless you have a compelling reason to use triggers. The polling approach should generally be avoided. Remember to thoroughly test and monitor your CDC implementation in a production-like environment before deploying it to production.
