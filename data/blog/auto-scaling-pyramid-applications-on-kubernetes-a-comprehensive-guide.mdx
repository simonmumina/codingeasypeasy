---
title: 'Auto-Scaling Pyramid Applications on Kubernetes: A Comprehensive Guide'
date: '2024-10-27'
lastmod: '2024-10-27'
tags: ['kubernetes', 'pyramid', 'auto-scaling', 'hpa', 'wsgi', 'gunicorn', 'docker', 'python', 'cloud-native']
draft: false
summary: 'Learn how to effectively auto-scale your Pyramid Python web applications on Kubernetes using Horizontal Pod Autoscaler (HPA). This comprehensive guide covers Dockerizing your Pyramid app, deploying to Kubernetes, configuring liveness and readiness probes, setting up resource requests and limits, and configuring the HPA for optimal performance and scalability.'
authors: ['default']
---

# Auto-Scaling Pyramid Applications on Kubernetes: A Comprehensive Guide

Scaling web applications is crucial for handling increased traffic and ensuring optimal performance. Kubernetes provides a powerful platform for orchestrating containerized applications, and its auto-scaling capabilities allow you to dynamically adjust resources based on demand. This guide demonstrates how to auto-scale Pyramid applications on Kubernetes, covering everything from Dockerizing your application to configuring a Horizontal Pod Autoscaler (HPA).

## Prerequisites

Before you begin, make sure you have the following:

*   **A working Pyramid application:** We'll use a simple example, but you can adapt these steps to your existing Pyramid project.
*   **Docker installed:**  You'll need Docker to containerize your application.  Get it from [https://www.docker.com/](https://www.docker.com/)
*   **Kubernetes cluster:** You can use a local cluster like Minikube or Kind, or a cloud-based cluster like GKE, EKS, or AKS.
*   **kubectl installed:** The Kubernetes command-line tool.  Get it from [https://kubernetes.io/docs/tasks/tools/](https://kubernetes.io/docs/tasks/tools/)
*   **Basic understanding of Docker and Kubernetes concepts.**

## 1. Dockerizing Your Pyramid Application

First, we need to package our Pyramid application into a Docker container.  Here's a step-by-step guide:

### a. Project Structure

Let's assume you have a basic Pyramid project with the following structure:

```
my_pyramid_app/
├── my_pyramid_app/
│   ├── __init__.py
│   ├── routes.py
│   ├── views.py
│   └── __main__.py
├── setup.py
├── requirements.txt
```

### b. `requirements.txt`

Your `requirements.txt` file should list the dependencies for your application.  Example:

```
pyramid
waitress  # Or Gunicorn, etc.
```

### c. `Dockerfile`

Create a `Dockerfile` in the root of your project:

```dockerfile
FROM python:3.9-slim-buster

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

EXPOSE 8080

CMD ["python", "-m", "my_pyramid_app"] # Or: ["gunicorn", "my_pyramid_app:main", "-b", "0.0.0.0:8080"]
```

**Explanation:**

*   `FROM python:3.9-slim-buster`:  Uses a lightweight Python 3.9 image. Consider using Alpine for even smaller images, but be aware of potential compatibility issues with C extensions.
*   `WORKDIR /app`: Sets the working directory inside the container.
*   `COPY requirements.txt .`: Copies the requirements file to the container.
*   `RUN pip install --no-cache-dir -r requirements.txt`: Installs the Python dependencies.  `--no-cache-dir` reduces image size.
*   `COPY . .`: Copies the entire project code to the container. **Caution:**  Consider using a `.dockerignore` file (see below) to exclude unnecessary files and folders.
*   `EXPOSE 8080`: Exposes port 8080, where your application will listen.  Adjust this based on your application's configuration.
*   `CMD ["python", "-m", "my_pyramid_app"]`: Defines the command to run when the container starts.  This example assumes you are using a `__main__.py` entrypoint.  You could also use Gunicorn (see example in the comments).

**Alternative using Gunicorn:**

For production environments, it is highly recommended to use a WSGI server like Gunicorn or uWSGI.  Here's how you can modify your `Dockerfile`:

```dockerfile
FROM python:3.9-slim-buster

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt gunicorn

COPY . .

EXPOSE 8080

CMD ["gunicorn", "my_pyramid_app:main", "-b", "0.0.0.0:8080", "--workers", "3", "--worker-class", "gevent"] #tune these values to your environment
```

**Explanation of Gunicorn options:**

* `my_pyramid_app:main`:  Specifies the WSGI application entrypoint (module name: application object).  Change `main` if your Pyramid application object is named differently.
* `-b 0.0.0.0:8080`: Binds Gunicorn to all interfaces on port 8080.
* `--workers 3`: Specifies the number of worker processes.  Adjust based on your server's CPU cores.  A good starting point is `2 * number of CPU cores + 1`.
* `--worker-class gevent`:  Specifies the worker type. `gevent` is a popular asynchronous worker, and using this will require gevent to be added to your `requirements.txt`.  Other options include `sync`, `eventlet` and `aiohttp`.  If you don't need asynchronous behaviour `sync` is fine.

### d. `.dockerignore` (Important!)

Create a `.dockerignore` file to exclude unnecessary files from being copied to the container.  This will significantly reduce the image size and build time.

```
__pycache__/
*.pyc
*.pyo
*.db
.git/
.gitignore
node_modules/
venv/
env/
```

### e. Building the Docker Image

Navigate to the root of your project in the terminal and run the following command to build the Docker image:

```bash
docker build -t my-pyramid-app:latest .
```

### f. Testing the Docker Image

Run the Docker image locally to verify that it works:

```bash
docker run -p 8080:8080 my-pyramid-app:latest
```

Open your browser and navigate to `http://localhost:8080`. You should see your Pyramid application running.

### g. Pushing the Image to a Registry

Push your Docker image to a container registry like Docker Hub, Google Container Registry (GCR), or Amazon Elastic Container Registry (ECR). This allows Kubernetes to pull the image when deploying your application.

```bash
docker tag my-pyramid-app:latest <your-dockerhub-username>/my-pyramid-app:latest
docker push <your-dockerhub-username>/my-pyramid-app:latest
```

Replace `<your-dockerhub-username>` with your Docker Hub username (or the appropriate registry prefix for other registries).

## 2. Deploying to Kubernetes

Now that you have a Docker image, you can deploy your Pyramid application to Kubernetes.

### a. Deployment YAML

Create a `deployment.yaml` file:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-pyramid-app-deployment
spec:
  replicas: 1 # start with 1 replica
  selector:
    matchLabels:
      app: my-pyramid-app
  template:
    metadata:
      labels:
        app: my-pyramid-app
    spec:
      containers:
      - name: my-pyramid-app
        image: <your-dockerhub-username>/my-pyramid-app:latest # Replace with your image
        ports:
        - containerPort: 8080
        resources:
          requests:
            cpu: 250m
            memory: 256Mi
          limits:
            cpu: 500m
            memory: 512Mi
        livenessProbe:
          httpGet:
            path: /  # Adjust if needed
            port: 8080
          initialDelaySeconds: 10
          periodSeconds: 5
        readinessProbe:
          httpGet:
            path: /  # Adjust if needed
            port: 8080
          initialDelaySeconds: 10
          periodSeconds: 5

```

**Explanation:**

*   `apiVersion: apps/v1` and `kind: Deployment`:  Defines a Deployment resource.
*   `metadata.name`:  The name of the deployment.
*   `spec.replicas`: The initial number of replicas (pods) to run.  We start with 1.
*   `spec.selector`:  Defines how the Deployment matches the pods it manages.
*   `spec.template`: Defines the pod template.
*   `containers`:  A list of containers to run within the pod.  In this case, we only have one container running our Pyramid application.
*   `image`:  The Docker image to use. **Replace `<your-dockerhub-username>/my-pyramid-app:latest` with your actual image.**
*   `containerPort`:  The port the container exposes.
*   `resources.requests`:  The minimum resources the container requires.  Kubernetes will try to schedule the pod on a node that has at least these resources available.  **Important for HPA to work correctly**.  `250m` represents 250 millicores (0.25 CPU cores).
*   `resources.limits`:  The maximum resources the container can use.  If the container tries to exceed these limits, Kubernetes may throttle it or even kill it.  **Important for stability**.
*   `livenessProbe`: Checks if the application is running. If the probe fails, Kubernetes will restart the pod.
*   `readinessProbe`: Checks if the application is ready to serve traffic. If the probe fails, Kubernetes will stop sending traffic to the pod.  This is important during startup.

**Liveness and Readiness Probes:**

The `livenessProbe` and `readinessProbe` are crucial for Kubernetes to manage your application effectively. They tell Kubernetes:

*   **Liveness Probe:**  "Is the application still alive and potentially recoverable?" If the liveness probe fails, Kubernetes will restart the pod. This is useful for handling situations where the application gets into an unrecoverable state (e.g., a deadlock).

*   **Readiness Probe:** "Is the application ready to serve traffic?" If the readiness probe fails, Kubernetes will remove the pod from the service's endpoints. This is important during application startup or when the application is temporarily unable to handle requests (e.g., during database migrations).

Adjust the `path` in the `httpGet` probes based on your application's health check endpoint.  A simple endpoint that returns a 200 OK status is sufficient.  Consider adding more sophisticated health checks that verify dependencies like database connections.

### b. Service YAML

Create a `service.yaml` file to expose your application:

```yaml
apiVersion: v1
kind: Service
metadata:
  name: my-pyramid-app-service
spec:
  selector:
    app: my-pyramid-app
  ports:
    - protocol: TCP
      port: 80
      targetPort: 8080
  type: LoadBalancer # Or NodePort, ClusterIP depending on your environment

```

**Explanation:**

*   `apiVersion: v1` and `kind: Service`: Defines a Service resource.
*   `metadata.name`: The name of the service.
*   `spec.selector`:  Matches the pods that the service will route traffic to.
*   `ports`: Defines the port mapping.  The service listens on port 80 and forwards traffic to port 8080 of the target pods.
*   `type: LoadBalancer`: Creates an external load balancer (if supported by your Kubernetes provider).  Use `NodePort` for local clusters like Minikube, or `ClusterIP` for internal-only services.

### c. Applying the YAML Files

Apply the YAML files to your Kubernetes cluster:

```bash
kubectl apply -f deployment.yaml
kubectl apply -f service.yaml
```

### d. Verifying the Deployment

Check the status of your deployment and service:

```bash
kubectl get deployments
kubectl get services
kubectl get pods
```

Make sure the deployment has the desired number of replicas and the service has been created successfully.  If using `LoadBalancer`, it might take a few minutes for the external IP to be provisioned.

## 3. Configuring Horizontal Pod Autoscaler (HPA)

Now, the core of the auto-scaling: configuring the Horizontal Pod Autoscaler (HPA).  The HPA automatically adjusts the number of pods in a deployment based on observed CPU utilization or other select metrics.

### a. HPA YAML

Create a `hpa.yaml` file:

```yaml
apiVersion: autoscaling/v2beta2 # Use autoscaling/v2beta2 or autoscaling/v2 depending on your k8s version.
kind: HorizontalPodAutoscaler
metadata:
  name: my-pyramid-app-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: my-pyramid-app-deployment
  minReplicas: 1
  maxReplicas: 5 # Or more, depending on your load
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70 # target CPU utilization %
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80 # target memory utilization %

```

**Explanation:**

*   `apiVersion: autoscaling/v2beta2` and `kind: HorizontalPodAutoscaler`: Defines the HPA resource.  Use `autoscaling/v2beta1` or `autoscaling/v1` for older Kubernetes versions.  `v2beta2` allows for more sophisticated scaling policies based on multiple metrics. `autoscaling/v1` only supports CPU utilization.
*   `metadata.name`: The name of the HPA.
*   `spec.scaleTargetRef`: Specifies the target resource to scale (in this case, our Deployment).
*   `minReplicas`: The minimum number of replicas the HPA will maintain.
*   `maxReplicas`: The maximum number of replicas the HPA will scale to.  **Crucial: Set this to a reasonable value to prevent runaway scaling.**
*   `metrics`:  Defines the metrics used to trigger scaling.
    *   `type: Resource`:  Indicates that we are using resource metrics (CPU and memory).
    *   `resource`:  Specifies the resource to monitor (CPU and memory in this case).
    *   `target`: Defines the target value for the metric.
        *   `type: Utilization`:  Indicates that we are using average utilization as the target.
        *   `averageUtilization`:  The target CPU utilization percentage (e.g., 70% means the HPA will try to maintain an average CPU utilization of 70% across all pods).  The target memory utilization percentage (e.g., 80% means the HPA will try to maintain an average memory utilization of 80% across all pods).

**Important Considerations for HPA:**

*   **Resource Requests and Limits:**  The HPA relies on the resource requests and limits defined in your Deployment's container specifications.  Make sure you have set these values appropriately.  Without resource requests, the HPA cannot accurately calculate CPU utilization.  Without resource limits, your application could consume unbounded resources, leading to instability.
*   **Metric Server:**  The HPA uses the Metrics Server to collect resource utilization metrics.  Make sure the Metrics Server is installed and running in your Kubernetes cluster.  If it's not, HPA won't function correctly.  Instructions for installation can be found here: [https://github.com/kubernetes-sigs/metrics-server](https://github.com/kubernetes-sigs/metrics-server)
*   **Custom Metrics:** For advanced scaling, you can use custom metrics collected from your application. This requires using a custom metrics adapter. See the Kubernetes documentation for details.
*   **Testing and Monitoring:**  It's essential to thoroughly test your HPA configuration and monitor its behavior in a production-like environment.  Use load testing tools to simulate realistic traffic and observe how the HPA scales your application.

### b. Applying the HPA YAML

Apply the HPA YAML file to your Kubernetes cluster:

```bash
kubectl apply -f hpa.yaml
```

### c. Verifying the HPA

Check the status of the HPA:

```bash
kubectl get hpa
```

The output should show the target and current CPU utilization, as well as the minimum and maximum number of replicas.

## 4. Testing the Auto-Scaling

To test the auto-scaling, you need to generate some load on your application.  You can use tools like `locust`, `wrk`, or `ApacheBench (ab)`.

Here's an example using `ab`:

```bash
ab -n 1000 -c 100 http://<your-service-ip>
```

Replace `<your-service-ip>` with the external IP address of your service (obtained from `kubectl get services`).

Monitor the HPA and the number of pods:

```bash
kubectl get hpa
kubectl get pods
```

You should see the CPU utilization increase, and the HPA should start scaling up the number of pods.  Once the load decreases, the HPA will eventually scale down the number of pods to the minimum replica count.

## 5. Advanced Considerations

*   **Scaling Based on Custom Metrics:**  You can scale based on custom metrics specific to your application. This requires using a custom metrics adapter and exposing the metrics in a format that Kubernetes can understand (e.g., Prometheus).

*   **Scaling Policies:** The HPA supports different scaling policies that allow you to control the rate at which the HPA scales up or down.  This can help prevent over-scaling or under-scaling.

*   **External Metrics:** You can scale based on metrics from external sources, such as cloud provider metrics or application performance monitoring (APM) tools.

*   **Graceful Shutdown:**  Ensure that your application handles shutdown signals gracefully. When a pod is terminated, it should complete any in-flight requests before exiting. This will prevent data loss and ensure a smooth transition during scaling.

## Conclusion

Auto-scaling your Pyramid applications on Kubernetes enables you to efficiently manage resources and ensure optimal performance under varying traffic conditions. By following this guide, you can effectively Dockerize your application, deploy it to Kubernetes, and configure the Horizontal Pod Autoscaler to dynamically adjust the number of pods based on CPU and memory utilization (or other custom metrics). Remember to thoroughly test your configuration and monitor its behavior to ensure it meets your specific needs.  Good luck!