---
title: 'FastAPI and PostgreSQL: A Comprehensive Guide to Building Scalable APIs'
date: '2024-10-27'
lastmod: '2024-10-27'
tags: ['fastapi', 'postgresql', 'api', 'python', 'database', 'orm', 'sqlalchemy', 'async', 'development']
draft: false
summary: 'Learn how to build robust and scalable APIs using FastAPI and PostgreSQL. This comprehensive guide covers everything from setup to database interactions and asynchronous operations.'
authors: ['default']
---

# FastAPI and PostgreSQL: A Comprehensive Guide to Building Scalable APIs

FastAPI is a modern, high-performance web framework for building APIs with Python 3.7+ based on standard Python type hints. Its key features include automatic data validation, serialization, and API documentation. PostgreSQL, on the other hand, is a powerful, open-source object-relational database system known for its reliability, feature robustness, and standards compliance.

Combining FastAPI with PostgreSQL allows you to create efficient, scalable, and maintainable APIs that leverage the strengths of both technologies. This guide will walk you through the process of setting up a FastAPI application and connecting it to a PostgreSQL database, covering everything from installation and configuration to database interaction and asynchronous operations.

## Prerequisites

Before we begin, make sure you have the following installed:

*   **Python 3.7+:**  FastAPI requires Python 3.7 or later.
*   **pip:** The Python package installer.
*   **PostgreSQL:**  Make sure you have a PostgreSQL server installed and running.  You can download it from the official PostgreSQL website.
*   **pgAdmin (Optional):** A graphical administration tool for PostgreSQL.

## Setting Up the Project

1.  **Create a Project Directory:**

    Create a new directory for your project:

    ```bash
    mkdir fastapi-postgresql
    cd fastapi-postgresql
    ```

2.  **Create a Virtual Environment:**

    It's recommended to use a virtual environment to isolate your project's dependencies:

    ```bash
    python3 -m venv venv
    source venv/bin/activate  # On Linux/macOS
    # venv\Scripts\activate  # On Windows
    ```

3.  **Install Dependencies:**

    Install FastAPI, Uvicorn (an ASGI server), SQLAlchemy (an ORM), Psycopg2 (a PostgreSQL adapter), and Alembic (for database migrations):

    ```bash
    pip install fastapi uvicorn sqlalchemy psycopg2-binary alembic python-dotenv
    ```

    *   **FastAPI:** The web framework.
    *   **Uvicorn:** An ASGI server to run the FastAPI application.
    *   **SQLAlchemy:**  A powerful and flexible Python SQL toolkit and Object-Relational Mapper (ORM).
    *   **Psycopg2-binary:**  A PostgreSQL adapter for Python.  Using the binary version is generally easier to install.
    *   **Alembic:**  A database migration tool for SQLAlchemy.
    *   **python-dotenv:**  For managing environment variables.

## Configuring the Database Connection

1.  **Create a `.env` file:**

    Create a `.env` file in your project root to store database credentials.  This prevents you from hardcoding sensitive information in your code.

    ```
    DATABASE_URL=postgresql://user:password@host:port/database
    ```

    Replace `user`, `password`, `host`, `port`, and `database` with your PostgreSQL credentials. For example:

    ```
    DATABASE_URL=postgresql://postgres:mysecretpassword@localhost:5432/mydatabase
    ```

2.  **Create a `database.py` file:**

    Create a file named `database.py` to handle the database connection and session management.

    ```python
    # database.py
    from sqlalchemy import create_engine
    from sqlalchemy.ext.declarative import declarative_base
    from sqlalchemy.orm import sessionmaker
    from dotenv import load_dotenv
    import os

    load_dotenv()

    DATABASE_URL = os.getenv("DATABASE_URL")

    engine = create_engine(DATABASE_URL)

    SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

    Base = declarative_base()

    def get_db():
        db = SessionLocal()
        try:
            yield db
        finally:
            db.close()
    ```

    *   `load_dotenv()`: Loads environment variables from the `.env` file.
    *   `create_engine()`: Creates a database engine using the connection string.
    *   `SessionLocal`:  A sessionmaker class used to create database sessions.
    *   `Base`: A declarative base class for defining SQLAlchemy models.
    *   `get_db()`: A dependency injection function that provides a database session to API endpoints. It uses a `try...finally` block to ensure the session is closed after each request.

## Defining Models

1.  **Create a `models.py` file:**

    Create a file named `models.py` to define your database models using SQLAlchemy.

    ```python
    # models.py
    from sqlalchemy import Column, Integer, String, DateTime
    from sqlalchemy.sql import func
    from sqlalchemy.orm import relationship
    from .database import Base

    class Item(Base):
        __tablename__ = "items"

        id = Column(Integer, primary_key=True, index=True)
        name = Column(String, index=True)
        description = Column(String, nullable=True)
        price = Column(Integer)
        created_at = Column(DateTime(timezone=True), server_default=func.now())
    ```

    *   `Item`:  A SQLAlchemy model representing a table named "items" in the database.
    *   `Column`: Defines a column in the table, specifying the data type (e.g., `Integer`, `String`, `DateTime`), constraints (e.g., `primary_key`, `nullable`), and indexing options (e.g., `index`).
    *   `func.now()`: A SQLAlchemy function that returns the current timestamp.
    *   `__tablename__`:  Specifies the name of the table in the database.

## Setting Up Database Migrations with Alembic

1.  **Initialize Alembic:**

    ```bash
    alembic init alembic
    ```

    This creates an `alembic` directory with the necessary configuration files.

2.  **Configure `alembic.ini`:**

    Edit the `alembic.ini` file and update the `sqlalchemy.url` setting to match your database URL:

    ```ini
    sqlalchemy.url = postgresql://user:password@host:port/database
    ```

    Remember to replace with the actual credentials.

3.  **Edit `alembic/env.py`:**

    Modify the `alembic/env.py` file to connect to your database and import your SQLAlchemy models:

    ```python
    # alembic/env.py
    import os
    from logging.config import fileConfig

    from sqlalchemy import create_engine
    from sqlalchemy import pool

    from alembic import context

    from dotenv import load_dotenv

    load_dotenv()

    # this is the Alembic Config object, which provides
    # access to the values within the .ini file in use.
    config = context.config

    # Interpret the config file for Python logging.
    # This line sets up loggers basically.
    if config.config_file_name is not None:
        fileConfig(config.config_file_name)

    # add your model's MetaData object here
    # for 'autogenerate' support
    # from myapp import mymodel
    # target_metadata = mymodel.Base.metadata

    from database import Base
    target_metadata = Base.metadata

    # other values from the config, defined by the needs of env.py,
    # can be acquired:
    # my_important_option = config.get_main_option("my_important_option")
    # ... etc.

    def run_migrations_offline() -> None:
        """Run migrations in 'offline' mode.

        This configures the context with just a URL
        and not an Engine, though an Engine is acceptable
        here as well.  By skipping the Engine creation
        we don't even need a DBAPI to be available.

        Calls to context.execute() here emit the script
        directly to the output stream.

        """
        url = os.environ.get("DATABASE_URL")
        context.configure(
            url=url,
            target_metadata=target_metadata,
            literal_binds=True,
            dialect_opts={"paramstyle": "named"},
        )

        with context.begin_transaction():
            context.run_migrations()


    def run_migrations_online() -> None:
        """Run migrations in 'online' mode.

        In this scenario we need to create an Engine
        and associate a connection with the context.

        """
        url = os.environ.get("DATABASE_URL")
        engine = create_engine(url)

        with engine.connect() as connection:
            context.configure(
                connection=connection, target_metadata=target_metadata
            )

            with context.begin_transaction():
                context.run_migrations()


    if context.is_offline_mode():
        run_migrations_offline()
    else:
        run_migrations_online()
    ```

    Key changes:

    *   Import `os` and `load_dotenv()` to access environment variables.
    *   Load the database URL from the environment.
    *   Import the `Base` metadata from `database.py`.
    *   Assign `Base.metadata` to `target_metadata`.

4.  **Create the initial migration:**

    ```bash
    alembic revision --autogenerate -m "Create tables"
    ```

    This will generate a new migration script in the `alembic/versions` directory based on the models you defined.

5.  **Apply the migration:**

    ```bash
    alembic upgrade head
    ```

    This applies the latest migration to your database, creating the tables defined in your models.

## Creating the FastAPI Application

1.  **Create a `main.py` file:**

    Create a file named `main.py` to define your FastAPI application.

    ```python
    # main.py
    from typing import List

    from fastapi import FastAPI, Depends, HTTPException
    from sqlalchemy.orm import Session

    from . import models, schemas
    from .database import get_db, engine

    models.Base.metadata.create_all(bind=engine)

    app = FastAPI()


    @app.post("/items/", response_model=schemas.Item)
    def create_item(item: schemas.ItemCreate, db: Session = Depends(get_db)):
        db_item = models.Item(**item.dict())
        db.add(db_item)
        db.commit()
        db.refresh(db_item)
        return db_item


    @app.get("/items/", response_model=List[schemas.Item])
    def read_items(skip: int = 0, limit: int = 100, db: Session = Depends(get_db)):
        items = db.query(models.Item).offset(skip).limit(limit).all()
        return items


    @app.get("/items/{item_id}", response_model=schemas.Item)
    def read_item(item_id: int, db: Session = Depends(get_db)):
        item = db.query(models.Item).filter(models.Item.id == item_id).first()
        if item is None:
            raise HTTPException(status_code=404, detail="Item not found")
        return item

    @app.delete("/items/{item_id}", response_model=schemas.Item)
    def delete_item(item_id: int, db: Session = Depends(get_db)):
        item = db.query(models.Item).filter(models.Item.id == item_id).first()
        if item is None:
            raise HTTPException(status_code=404, detail="Item not found")
        db.delete(item)
        db.commit()
        return item
    ```

    *   `models.Base.metadata.create_all(bind=engine)`: Creates the database tables if they don't already exist.  This is a simpler approach than migrations for very small projects.  For larger projects migrations are highly recommended.
    *   `@app.post("/items/")`: Defines a POST endpoint for creating new items.
    *   `@app.get("/items/")`: Defines a GET endpoint for retrieving all items.
    *   `@app.get("/items/{item_id}")`: Defines a GET endpoint for retrieving a specific item by its ID.
    *   `Depends(get_db)`:  Injects the database session into the API endpoint.
    *   `HTTPException`:  Raises an HTTP exception with a specific status code and detail message.
    *   `db.query(models.Item)`:  Creates a query object for the `Item` model.
    *   `db.add(db_item)`: Adds a new item to the database session.
    *   `db.commit()`: Commits the changes to the database.
    *   `db.refresh(db_item)`: Refreshes the item object with the latest data from the database, including the generated ID.
    *   `db.delete(item)`: Deletes item from the database

2.  **Create a `schemas.py` file:**

    Create a file named `schemas.py` to define your data validation schemas using Pydantic.

    ```python
    # schemas.py
    from typing import Optional

    from pydantic import BaseModel
    from datetime import datetime


    class ItemBase(BaseModel):
        name: str
        description: Optional[str] = None
        price: int


    class ItemCreate(ItemBase):
        pass


    class Item(ItemBase):
        id: int
        created_at: datetime

        class Config:
            orm_mode = True
    ```

    *   `ItemBase`: A base schema containing the common attributes for all Item-related schemas.
    *   `ItemCreate`: A schema for creating new items. It inherits from `ItemBase`.
    *   `Item`: A schema for representing an existing item, including the ID and created_at.
    *   `orm_mode = True`: Enables ORM mode, which allows Pydantic to map SQLAlchemy model attributes to schema fields.

## Running the Application

1.  **Start the Uvicorn server:**

    ```bash
    uvicorn main:app --reload
    ```

    *   `main:app`: Specifies the module (`main`) and the FastAPI application instance (`app`).
    *   `--reload`: Enables automatic reloading of the server when code changes are detected.

2.  **Access the API documentation:**

    Open your browser and navigate to `http://localhost:8000/docs` to access the automatically generated API documentation using Swagger UI.

## Asynchronous Operations

For improved performance, especially with I/O-bound operations like database queries, you can use asynchronous operations.  However, be aware that you must use an asynchronous PostgreSQL driver and configure your SQLAlchemy connection accordingly.  Psycopg2 is NOT async; you need to use `asyncpg`

Here's a basic example using `asyncpg` and `databases` library (instead of sqlalchemy):

First install the required libraries.
```bash
pip install databases asyncpg
```

Update database.py (and remove SQLAlchemy and alembic dependencies.)

```python
# database.py
import os
import databases
from dotenv import load_dotenv

load_dotenv()

DATABASE_URL = os.getenv("DATABASE_URL")

database = databases.Database(DATABASE_URL)

# Create a table metadata object (optional, used for migrations)
import sqlalchemy

metadata = sqlalchemy.MetaData()

items = sqlalchemy.Table(
    "items",
    metadata,
    sqlalchemy.Column("id", sqlalchemy.Integer, primary_key=True),
    sqlalchemy.Column("name", sqlalchemy.String(100)),
    sqlalchemy.Column("description", sqlalchemy.String(255)),
    sqlalchemy.Column("price", sqlalchemy.Integer),
    sqlalchemy.Column("created_at", sqlalchemy.DateTime),
)

engine = sqlalchemy.create_engine(DATABASE_URL) # Necessary for table creation only. Consider removing if migrations are used.

def create_db():
    metadata.create_all(engine)

```

Update `models.py`  (This file is largely unnecessary when using raw queries but can hold data schemas if needed for type hinting):

```python
from typing import Optional

from pydantic import BaseModel
from datetime import datetime


class ItemBase(BaseModel):
    name: str
    description: Optional[str] = None
    price: int


class ItemCreate(ItemBase):
    pass


class Item(ItemBase):
    id: int
    created_at: datetime

    class Config:
        orm_mode = True
```

Update `main.py`:

```python
# main.py
from typing import List
from datetime import datetime

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from . import models
from .database import database, items, create_db

app = FastAPI()

# CORS (Cross-Origin Resource Sharing) allows the frontend served from
# a different domain or port to access the API.
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Allows all origins
    allow_credentials=True,
    allow_methods=["*"],  # Allows all methods
    allow_headers=["*"],  # Allows all headers
)


@app.on_event("startup")
async def startup():
    await database.connect()
    create_db()  # Consider using proper migrations instead.

@app.on_event("shutdown")
async def shutdown():
    await database.disconnect()


@app.post("/items/", response_model=models.Item)
async def create_item(item: models.ItemCreate):
    query = items.insert().values(**item.dict(), created_at=datetime.now())
    last_record_id = await database.execute(query)
    return {**item.dict(), "id": last_record_id, "created_at": datetime.now()}


@app.get("/items/", response_model=List[models.Item])
async def read_items(skip: int = 0, limit: int = 100):
    query = items.select().offset(skip).limit(limit)
    return await database.fetch_all(query)


@app.get("/items/{item_id}", response_model=models.Item)
async def read_item(item_id: int):
    query = items.select().where(items.c.id == item_id)
    item = await database.fetch_one(query)
    if item is None:
        raise HTTPException(status_code=404, detail="Item not found")
    return item


@app.delete("/items/{item_id}", response_model=models.Item)
async def delete_item(item_id: int):
    query = items.delete().where(items.c.id == item_id)
    rows_deleted = await database.execute(query)
    if rows_deleted == 0:
        raise HTTPException(status_code=404, detail="Item not found")  # Ensure you are actually deleting something.
    return models.Item(id=item_id, name='DELETED', price=0, description="Item deleted", created_at=datetime.now()) # You might want a different way to handle this.  Returning an example object is common.
```

Key changes:

*   **Asynchronous database connection:** Using `database = databases.Database(DATABASE_URL)` and asynchronous connect/disconnect events.
*   **`async` keyword:**  All database interaction functions are now defined with the `async` keyword.
*   **`await` keyword:**  Use `await` when calling asynchronous database operations.
*   **Direct SQL queries:**  Instead of using SQLAlchemy ORM features directly, constructing and executing raw SQL queries using `databases`. This gives you more control and can be more efficient for certain operations.  However, it requires writing and managing the SQL yourself.

**Choosing Between SQLAlchemy and `databases` (with `asyncpg`)**

*   **SQLAlchemy ORM:** Offers a high-level abstraction, making code more readable and maintainable, especially for complex applications.  Handles database interactions more abstractly.
*   **`databases` with `asyncpg`:** Provides more control over SQL queries and can be more efficient for specific use cases.  It's a lighter-weight approach and avoids the ORM layer.
*   For smaller projects or projects where you want very fine-grained control over the SQL, `databases` may be appropriate. For larger projects, SQLAlchemy offers significant advantages in terms of maintainability and code clarity.

## Security Considerations

*   **Input Validation:**  Always validate user input to prevent SQL injection and other vulnerabilities.  FastAPI's Pydantic integration makes this easy.
*   **Authentication and Authorization:** Implement proper authentication and authorization mechanisms to protect your API endpoints.  FastAPI offers several options for handling authentication, including JWT (JSON Web Tokens).
*   **HTTPS:**  Use HTTPS to encrypt communication between the client and the server.
*   **Database Credentials:** Store database credentials securely, preferably using environment variables and a secrets management system. Never hardcode credentials directly in your code.
*   **CORS:** Configure CORS properly to prevent cross-origin requests from unauthorized domains.

## Conclusion

This guide has provided a comprehensive overview of how to use FastAPI with PostgreSQL to build robust and scalable APIs.  By following these steps, you can create efficient and maintainable applications that leverage the strengths of both technologies.  Remember to consider security best practices and choose the right approach for your specific needs, whether it's using SQLAlchemy's ORM for abstraction or `databases` with `asyncpg` for fine-grained control.  Good luck!