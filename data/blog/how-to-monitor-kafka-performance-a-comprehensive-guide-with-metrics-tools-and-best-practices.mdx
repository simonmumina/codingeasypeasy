---
title: 'How to Monitor Kafka Performance: A Comprehensive Guide with Metrics, Tools, and Best Practices'
date: '2024-10-26'
lastmod: '2024-10-27'
tags:
  [
    'kafka',
    'performance monitoring',
    'apache kafka',
    'kafka metrics',
    'kafka monitoring tools',
    'message queue',
    'distributed systems',
    'observability',
  ]
draft: false
summary: 'Learn how to effectively monitor Apache Kafka performance to ensure optimal throughput, low latency, and system stability. This comprehensive guide covers essential Kafka metrics, popular monitoring tools, and best practices for proactive performance management.'
authors: ['default']
---

# How to Monitor Kafka Performance: A Comprehensive Guide

Apache Kafka is a powerful, distributed streaming platform widely used for building real-time data pipelines and streaming applications. Ensuring its optimal performance is crucial for the reliability and scalability of your data infrastructure. This guide provides a comprehensive overview of Kafka performance monitoring, covering key metrics, popular tools, and best practices.

## Why Monitor Kafka Performance?

Kafka's performance directly impacts the applications it supports. Poor performance can lead to:

- **Data Loss:** If producers can't keep up, data might be lost due to buffer overflows or timeouts.
- **Increased Latency:** Higher latency in message delivery can degrade user experience and impact real-time applications.
- **System Instability:** Overloaded brokers can become unstable and lead to cluster-wide failures.
- **Resource Exhaustion:** Inefficient configurations or bottlenecks can lead to resource exhaustion (CPU, memory, disk), hindering overall performance.
- **Application Downtime:** Ultimately, poorly performing Kafka clusters can cause downstream application downtime.

By proactively monitoring Kafka performance, you can identify and address potential issues before they escalate into serious problems.

## Key Kafka Metrics to Monitor

Understanding the right metrics is essential for effective monitoring. Here's a breakdown of the most important Kafka metrics, categorized for clarity:

### Broker Metrics

These metrics provide insights into the health and performance of individual Kafka brokers.

- **CPU Utilization (%idle, %user, %system):** High CPU utilization indicates the broker is struggling to process requests. Investigate CPU-intensive operations like compression/decompression and message serialization/deserialization.

- **Memory Usage (Heap, Non-Heap):** Monitor heap usage to prevent OutOfMemoryErrors. Configure appropriate heap size based on workload. Track non-heap usage for potential memory leaks.

- **Disk I/O (Disk Reads/Writes, Disk Latency):** Kafka relies heavily on disk I/O. High disk latency can significantly impact performance. Consider using faster storage (SSD) or increasing the number of disks.

- **Network I/O (Bytes In/Out, Network Latency):** Network bandwidth can be a bottleneck. Monitor network traffic and identify any network-related issues.

- **Active Controller Count:** There should always be exactly one active controller. Multiple active controllers indicate a split-brain scenario. Zero active controllers mean the cluster is without leadership and unstable.

- **Under Replicated Partitions:** This metric indicates the number of partitions that do not have the number of replicas specified in the topic configuration. A high number of under replicated partitions can lead to data loss.

- **Offline Partition Count:** The number of partitions that are offline, meaning they are unavailable for reads and writes. This is a critical metric that directly impacts data availability.

- **Request Handler Idle Ratio:** Indicates the percentage of time request handlers are idle. A low idle ratio suggests the handlers are overloaded, potentially leading to increased latency.

- **Bytes In/Out per Second (for each partition):** Monitor this metric at the partition level to identify hot partitions. Partitions receiving significantly more traffic than others may require rebalancing or topic redesign.

- **Log Flush Latency:** Measures the time it takes to flush data to disk. High latency can indicate disk I/O bottlenecks. Tune `log.flush.interval.ms` and `log.flush.scheduler.interval.ms` settings.

- **Purgatory Size:** Kafka's purgatory holds delayed requests. High purgatory sizes for produce and fetch requests usually indicate problems in processing requests or fetching data, respectively.

**Code Example (using JMX to access Broker Metrics):**

```plaintext
import javax.management.*;
import javax.management.remote.*;
import java.io.IOException;
import java.util.HashMap;
import java.util.Map;
import java.util.Set;

public class KafkaJMXMonitor {

    public static void main(String[] args) throws IOException, MalformedObjectNameException, NullPointerException, AttributeNotFoundException, InstanceNotFoundException, MBeanException, ReflectionException {

        // Replace with your Kafka JMX URL
        String jmxURL = "service:jmx:rmi:///jndi/rmi://localhost:9999/jmxrmi";

        // Connect to JMX
        JMXServiceURL serviceURL = new JMXServiceURL(jmxURL);
        Map<String, String[]> env = new HashMap<>();
        String[] credentials = {"username", "password"}; // Replace with JMX credentials if needed
        env.put(JMXConnectorFactory.PROTOCOL_PROVIDER_PACKAGES, new String[]{"com.sun.jmx.rmi.registry"}); //Needed if you have issues establishing a connection
        env.put(JMXConnector.CREDENTIALS, credentials);
        JMXConnector jmxConnector = JMXConnectorFactory.connect(serviceURL, env);
        MBeanServerConnection mbeanConn = jmxConnector.getMBeanServerConnection();

        // Example: Get the request handler idle ratio
        ObjectName objectName = new ObjectName("kafka.server:type=BrokerTopicMetrics,name=RequestHandlerAvgIdlePercent,topic=*,partition=*");
        Set<ObjectName> objectNames = mbeanConn.queryNames(objectName, null);

        for (ObjectName name : objectNames) {
            Double idlePercent = (Double) mbeanConn.getAttribute(name, "Value");
            System.out.println("Topic: " + name.getKeyProperty("topic") + ", Partition: " + name.getKeyProperty("partition") + ", RequestHandlerAvgIdlePercent: " + idlePercent);
        }


        // Close the connection
        jmxConnector.close();
    }
}
```

This Java code snippet demonstrates how to connect to Kafka's JMX endpoint and retrieve the `RequestHandlerAvgIdlePercent` metric. Adapt it to retrieve other metrics of interest. Remember to enable JMX on your Kafka brokers and configure appropriate security settings. The kafka brokers will need these environment variables to enable JMX.

```plaintext
KAFKA_JMX_OPTS="-Dcom.sun.management.jmxremote=true -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Djava.rmi.server.hostname=127.0.0.1 -Dcom.sun.management.jmxremote.port=9999"
```

### Producer Metrics

These metrics reflect the performance of Kafka producers.

- **Request Latency:** Measures the time it takes for a producer to send a message and receive an acknowledgment. High latency can indicate network issues, broker overload, or inefficient producer configuration.

- **Bytes Sent/Second:** Tracks the amount of data sent by the producer per second. Helps identify producers sending excessive amounts of data.

- **Records Sent/Second:** Tracks the number of messages sent by the producer per second.

- **Compression Ratio:** Indicates the effectiveness of compression. Higher compression ratios reduce network traffic and storage requirements.

- **Connection Count:** The number of active connections the producer has to the brokers.

- **Errors (e.g., `record-send-error-total`, `connection-close`)**: Track the different types of errors occurring on the producer side.

**Code Example (Producer Metrics using the Kafka Clients API):**

```plaintext
import org.apache.kafka.clients.producer.*;
import org.apache.kafka.common.Metric;
import org.apache.kafka.common.MetricName;

import java.util.Map;
import java.util.Properties;
import java.util.concurrent.ExecutionException;

public class KafkaProducerExample {

    public static void main(String[] args) throws ExecutionException, InterruptedException {
        Properties props = new Properties();
        props.put("bootstrap.servers", "localhost:9092");
        props.put("acks", "all");
        props.put("retries", 0);
        props.put("batch.size", 16384);
        props.put("linger.ms", 1);
        props.put("buffer.memory", 33554432);
        props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
        props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");

        try (KafkaProducer<String, String> producer = new KafkaProducer<>(props)) {
            for (int i = 0; i < 100; i++) {
                producer.send(new ProducerRecord<>("my-topic", Integer.toString(i), Integer.toString(i))).get();
            }

            // Access producer metrics
            Map<MetricName, ? extends Metric> metrics = producer.metrics();
            for (Map.Entry<MetricName, ? extends Metric> entry : metrics.entrySet()) {
                System.out.println("Metric Name: " + entry.getKey().name());
                System.out.println("Metric Value: " + entry.getValue().metricValue());
            }


        }
    }
}
```

This example demonstrates how to access producer metrics using the Kafka Clients API. The `producer.metrics()` method returns a map of available metrics, which can be analyzed to monitor producer performance. You'll find valuable metrics like `record-send-rate`, `record-send-total`, `request-latency-avg`, etc.

### Consumer Metrics

These metrics provide insights into the performance of Kafka consumers.

- **Fetch Latency:** Measures the time it takes for a consumer to fetch messages from the broker. High latency can indicate broker overload, network issues, or slow consumer processing.

- **Bytes Received/Second:** Tracks the amount of data received by the consumer per second.

- **Records Consumed/Second:** Tracks the number of messages consumed per second.

- **Commit Latency:** Measures the time it takes for a consumer to commit offsets. High latency can indicate issues with the Kafka transaction log.

- **Consumer Lag:** The difference between the latest offset in a partition and the consumer's current offset. High consumer lag indicates that the consumer is falling behind and not processing messages quickly enough. This is one of the most critical metrics to monitor.

- **Records Lag Max:** The largest lag across all partitions that the consumer is subscribed to.

- **Heartbeat Latency:** The time taken for the consumer to send a heartbeat to the coordinator. Higher than normal latency may indicate issues with the consumer or the coordinator.

**Code Example (Consumer Metrics using the Kafka Clients API):**

```plaintext
import org.apache.kafka.clients.consumer.*;
import org.apache.kafka.common.Metric;
import org.apache.kafka.common.MetricName;
import org.apache.kafka.common.TopicPartition;

import java.time.Duration;
import java.util.*;

public class KafkaConsumerExample {

    public static void main(String[] args) {
        Properties props = new Properties();
        props.setProperty("bootstrap.servers", "localhost:9092");
        props.setProperty("group.id", "test-group");
        props.setProperty("enable.auto.commit", "false"); //disable auto commit for consumer lag monitoring
        props.setProperty("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
        props.setProperty("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");

        try (KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props)) {
            consumer.subscribe(Collections.singletonList("my-topic"));

            while (true) {
                ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(100));
                for (ConsumerRecord<String, String> record : records) {
                    System.out.printf("offset = %d, key = %s, value = %s%n", record.offset(), record.key(), record.value());
                }

                // Print consumer metrics
                Map<MetricName, ? extends Metric> metrics = consumer.metrics();
                for (Map.Entry<MetricName, ? extends Metric> entry : metrics.entrySet()) {
                    System.out.println("Metric Name: " + entry.getKey().name());
                    System.out.println("Metric Value: " + entry.getValue().metricValue());
                }


            }
        }
    }
}
```

Similar to the producer example, this demonstrates how to access consumer metrics using the `consumer.metrics()` method. Pay close attention to metrics like `fetch-latency-avg`, `records-consumed-rate`, `records-lag-max`, and `commit-latency-avg`.

### Zookeeper Metrics (If applicable, older Kafka versions)

Older Kafka versions rely on Zookeeper for metadata management. If you're running an older version, monitor Zookeeper metrics like:

- **Latency:** Average request processing time. High latency can indicate Zookeeper overload.
- **Outstanding Requests:** Number of unprocessed requests. A growing queue indicates Zookeeper is struggling to keep up.
- **Node Count:** Number of nodes in the Zookeeper ensemble.

(Kafka versions 3.0 and later generally do not rely on Zookeeper)

## Kafka Monitoring Tools

Several tools are available for monitoring Kafka performance. Here's a rundown of some popular options:

- **Kafka Manager (Yahoo! Kafka Manager):** A web-based tool for managing and monitoring Kafka clusters. Provides a user-friendly interface for visualizing cluster topology, topic configuration, and consumer lag. (Community support may vary).

- **Confluent Control Center:** A comprehensive monitoring and management tool offered by Confluent. Provides real-time dashboards, alerting, and schema management. This is a commercial offering with a community edition.

- **Prometheus and Grafana:** A powerful combination for collecting and visualizing metrics. Prometheus is a popular open-source monitoring and alerting toolkit, while Grafana is a versatile data visualization platform. Use the JMX Exporter to expose Kafka metrics to Prometheus.

- **Datadog:** A cloud-based monitoring platform that provides pre-built integrations for Kafka. Offers comprehensive dashboards, alerting, and anomaly detection.

- **New Relic:** Another cloud-based monitoring platform with Kafka integration. Provides similar features to Datadog.

- **Dynatrace:** Provides AI-powered monitoring and observability, including deep visibility into Kafka performance.

- **Kafka Command-Line Tools:** Kafka ships with several command-line tools for monitoring and managing the cluster. For example, `kafka-consumer-groups.sh` can be used to check consumer lag. `kafka-topics.sh` can be used to check topic configurations and partitions.

**Example: Setting up Prometheus and Grafana for Kafka Monitoring:**

1.  **Install and Configure Prometheus:** Install Prometheus and configure it to scrape metrics from the JMX Exporter running on your Kafka brokers. Add a scrape configuration to your `prometheus.yml` file:

    ```plaintext
    scrape_configs:
      - job_name: 'kafka'
        metrics_path: /metrics
        static_configs:
          - targets: ['kafka-broker-1:9999', 'kafka-broker-2:9999', 'kafka-broker-3:9999'] # Replace with your broker addresses and JMX port
    ```

2.  **Install and Configure JMX Exporter:** Download the JMX Exporter JAR file and configure it to expose Kafka metrics. Create a configuration file (`jmx_exporter.yml`) specifying the MBeans to export. Example:

    ```plaintext
    lowercaseOutputName: true
    lowercaseOutputLabelNames: true
    rules:
      - pattern: 'kafka.server<type=BrokerTopicMetrics, name=(.*)>(Value|Count)'
        name: 'kafka_broker_topic_metrics_$1'
        type: ATTRIBUTE
    ```

    Start the JMX Exporter along with the Kafka broker using the `-javaagent` flag:

    ```plaintext
    KAFKA_OPTS="-javaagent:/path/to/jmx_prometheus_javaagent-0.16.1.jar=9999:/path/to/jmx_exporter.yml" ./kafka-server-start.sh config/server.properties
    ```

3.  **Install and Configure Grafana:** Install Grafana and add Prometheus as a data source. Import pre-built Kafka dashboards or create your own to visualize the metrics. Many community-contributed Grafana dashboards are available for Kafka monitoring.

## Best Practices for Kafka Performance Monitoring

- **Establish Baseline Metrics:** Before making any changes to your Kafka configuration, establish a baseline of performance metrics. This will help you track the impact of any subsequent changes.

- **Set Up Alerts:** Configure alerts for critical metrics to be notified of potential issues proactively. For example, alert when consumer lag exceeds a certain threshold, or when CPU utilization on a broker is consistently high.

- **Monitor at Multiple Levels:** Monitor Kafka performance at the broker, producer, consumer, and topic levels. This will provide a comprehensive view of the system's health.

- **Use Appropriate Metrics:** Select the metrics that are most relevant to your specific use case. Not all metrics are equally important.

- **Analyze Trends:** Don't just look at current values; analyze trends over time to identify patterns and potential problems.

- **Optimize Kafka Configuration:** Based on your monitoring data, tune Kafka configuration parameters like `num.partitions`, `replication.factor`, `log.retention.bytes`, `batch.size`, `linger.ms`, `compression.type`, and consumer fetch sizes (`fetch.min.bytes`, `fetch.max.wait.ms`).

- **Regularly Review and Update Monitoring:** As your application evolves, review and update your monitoring setup to ensure it's still relevant and effective.

- **Automate Monitoring:** Use automation tools to collect metrics, generate reports, and trigger alerts. This will save time and effort, and help you respond to issues more quickly.

- **Capacity Planning:** Use historical performance data to plan for future capacity needs. Scale your Kafka cluster proactively to avoid performance bottlenecks.

- **Consumer Group Monitoring:** Actively monitor consumer group status and rebalancing. Excessive rebalancing can indicate issues with consumer liveness, session timeouts, or partition assignment strategies. The `DescribeGroups` API can be very useful here.

## Conclusion

Monitoring Kafka performance is essential for maintaining a reliable and scalable data streaming platform. By understanding the key metrics, utilizing appropriate monitoring tools, and following best practices, you can proactively identify and address potential issues, ensuring optimal performance and stability for your applications. Remember to tailor your monitoring strategy to your specific use case and continuously refine it as your application evolves.
