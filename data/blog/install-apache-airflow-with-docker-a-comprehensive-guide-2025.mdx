---
title: 'Install Apache Airflow with Docker: A Comprehensive Guide (2025)'
date: '2025-05-27'
lastmod: '2025-05-15'
tags:
  [
    'airflow',
    'docker',
    'docker-compose',
    'data-engineering',
    'etl',
    'workflow-automation',
    'apache-airflow',
    'installation-guide',
  ]
draft: false
summary: 'Learn how to install and run Apache Airflow using Docker and Docker Compose. This comprehensive guide covers everything from setting up your environment to running your first DAG.  Includes best practices and troubleshooting tips for a smooth Airflow experience.'
authors: ['default']
---

# Install Apache Airflow with Docker: A Comprehensive Guide (2025)

Apache Airflow is a powerful open-source platform for programmatically authoring, scheduling, and monitoring workflows. It's a vital tool for data engineers and anyone dealing with complex ETL pipelines, data transformations, or automated tasks. While Airflow offers several installation methods, using Docker provides a clean, consistent, and reproducible environment, simplifying the setup process significantly.

This guide will walk you through the complete process of installing and running Apache Airflow using Docker and Docker Compose. We'll cover everything from setting up your environment to verifying your installation and running your first DAG (Directed Acyclic Graph).

## Why Use Docker for Airflow?

Before we dive into the installation, let's understand why using Docker is a preferred method for Airflow deployments:

- **Isolation:** Docker containers provide isolated environments, preventing conflicts between Airflow dependencies and your system's existing software.
- **Reproducibility:** Docker images ensure consistent environments across different machines, guaranteeing that your Airflow setup works identically regardless of the underlying infrastructure.
- **Simplified Setup:** Docker abstracts away the complexities of installing and configuring Airflow's dependencies, streamlining the entire setup process.
- **Scalability:** Docker Compose allows you to easily scale your Airflow components (e.g., Webserver, Scheduler, Worker) as your workload grows.
- **Version Control:** You can easily manage different Airflow versions using Docker images.

## Prerequisites

Before you begin, ensure you have the following prerequisites installed:

- **Docker:** Download and install Docker Desktop from the official Docker website ([https://www.docker.com/products/docker-desktop](https://www.docker.com/products/docker-desktop)). Make sure Docker is running after installation.
- **Docker Compose:** Docker Compose is usually bundled with Docker Desktop. If it's not, you might need to install it separately. You can verify its installation by running `docker-compose --version` in your terminal. If it's missing, refer to the Docker documentation for your operating system.

## Step-by-Step Installation Guide

Now, let's proceed with the installation steps:

**1. Create a Project Directory:**

Create a directory to house your Airflow Docker Compose configuration:

```plaintext
mkdir airflow-docker
cd airflow-docker
```

**2. Create a `docker-compose.yaml` file:**

This file defines the services that make up your Airflow environment (Webserver, Scheduler, Postgres Database, Redis, and Flower).

```plaintext
touch docker-compose.yaml
```

**3. Populate the `docker-compose.yaml` file:**

Open the `docker-compose.yaml` file in your favorite text editor and paste the following configuration. This example uses the official Apache Airflow Docker image, version 2.8.1. **Always check the official Airflow documentation ([https://airflow.apache.org/docs/](https://airflow.apache.org/docs/)) for the latest version and best practices before using this.**

```plaintext
version: '3.7'
services:
  postgres:
    image: postgres:13
    container_name: airflow-postgres
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    ports:
      - '5432:5432'
    volumes:
      - postgres_data:/var/lib/postgresql/data

  redis:
    image: redis:6.2
    container_name: airflow-redis
    ports:
      - '6379:6379'

  airflow-webserver:
    image: apache/airflow:2.8.1
    container_name: airflow-webserver
    depends_on:
      - postgres
      - redis
    environment:
      - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
      - AIRFLOW__CELERY__RESULT_BACKEND=redis://redis:6379/0
      - AIRFLOW__CELERY__BROKER_URL=redis://redis:6379/0
      - AIRFLOW__CORE__DEFAULT_TIMEZONE=UTC # Adjust to your timezone
      - AIRFLOW__API__AUTH_BACKEND=airflow.api.auth.backend.basic_auth
      - AIRFLOW__CORE__FERNET_KEY=YOUR_FERNET_KEY # Generate a Fernet key (see below)
      - AIRFLOW__CORE__ENABLE_XCOM_PICKLING=True
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
    ports:
      - '8080:8080'
    healthcheck:
      test: ['CMD-SHELL', 'airflow db check']
      interval: 30s
      timeout: 30s
      retries: 3

  airflow-scheduler:
    image: apache/airflow:2.8.1
    container_name: airflow-scheduler
    depends_on:
      - postgres
      - redis
    environment:
      - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
      - AIRFLOW__CELERY__RESULT_BACKEND=redis://redis:6379/0
      - AIRFLOW__CELERY__BROKER_URL=redis://redis:6379/0
      - AIRFLOW__CORE__DEFAULT_TIMEZONE=UTC # Adjust to your timezone
      - AIRFLOW__API__AUTH_BACKEND=airflow.api.auth.backend.basic_auth
      - AIRFLOW__CORE__FERNET_KEY=YOUR_FERNET_KEY # Generate a Fernet key (see below)
      - AIRFLOW__CORE__ENABLE_XCOM_PICKLING=True
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins

  airflow-worker:
    image: apache/airflow:2.8.1
    container_name: airflow-worker
    depends_on:
      - postgres
      - redis
      - airflow-scheduler
    environment:
      - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
      - AIRFLOW__CELERY__RESULT_BACKEND=redis://redis:6379/0
      - AIRFLOW__CELERY__BROKER_URL=redis://redis:6379/0
      - AIRFLOW__CORE__DEFAULT_TIMEZONE=UTC # Adjust to your timezone
      - AIRFLOW__API__AUTH_BACKEND=airflow.api.auth.backend.basic_auth
      - AIRFLOW__CORE__FERNET_KEY=YOUR_FERNET_KEY # Generate a Fernet key (see below)
      - AIRFLOW__CORE__ENABLE_XCOM_PICKLING=True
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins

  flower:
    image: mher/flower:1.2
    container_name: airflow-flower
    depends_on:
      - redis
    ports:
      - '5555:5555'
    environment:
      - CELERY_BROKER_URL=redis://redis:6379/0
      - FLOWER_PORT=5555

volumes:
  postgres_data:
```

**Important Notes:**

- **Airflow Version:** Double-check the latest Airflow version on the official Apache Airflow website and update the `image` tag accordingly (e.g., `apache/airflow:2.8.1`). Using the latest stable version is generally recommended.
- **Timezone:** Adjust the `AIRFLOW__CORE__DEFAULT_TIMEZONE` environment variable to your local timezone. A list of valid timezones can be found at [https://en.wikipedia.org/wiki/List_of_tz_database_time_zones](https://en.wikipedia.org/wiki/List_of_tz_database_time_zones).
- **Fernet Key:** The `AIRFLOW__CORE__FERNET_KEY` environment variable is crucial for encrypting sensitive information, such as passwords and connections. You _must_ generate a unique Fernet key. You can generate one using the following Python code:

  ```plaintext
  from cryptography.fernet import Fernet
  print(Fernet.generate_key().decode())
  ```

  Copy the output of this command and replace `YOUR_FERNET_KEY` in the `docker-compose.yaml` file with the generated key. **Do not share your Fernet key!**

- **Volumes:** The `volumes` section maps local directories on your host machine to directories within the containers. This allows you to persist data (like DAGs, logs, and plugins) even after the containers are stopped. We are mounting:
  - `./dags` to `/opt/airflow/dags`: Place your DAG files in the `./dags` directory.
  - `./logs` to `/opt/airflow/logs`: Airflow logs will be stored in the `./logs` directory.
  - `./plugins` to `/opt/airflow/plugins`: Place your custom plugins in the `./plugins` directory.

**4. Create the Required Directories:**

Create the directories defined in the `volumes` section of your `docker-compose.yaml` file:

```plaintext
mkdir dags logs plugins
```

**5. Initialize the Airflow Database:**

Before starting the containers, initialize the Airflow database. This sets up the necessary tables and configurations:

```plaintext
docker-compose up airflow-init
```

**Note:** If you are using Docker Desktop on macOS or Windows, you might encounter issues with file permissions. If you do, try running the following command before initializing the database:

```plaintext
docker run --rm -v $(pwd)/dags:/opt/airflow/dags -v $(pwd)/logs:/opt/airflow/logs -v $(pwd)/plugins:/opt/airflow/plugins apache/airflow:2.8.1 airflow db init
```

This command runs the `airflow db init` command directly within a Docker container, ensuring the correct permissions. Remember to replace `2.8.1` with your chosen Airflow version. However, the `docker-compose up airflow-init` should be sufficient in most cases.

**6. Start the Airflow Services:**

Now that the database is initialized, you can start all the Airflow services using Docker Compose:

```plaintext
docker-compose up -d
```

This command will start the containers in detached mode (i.e., in the background).

**7. Verify the Installation:**

Wait a few minutes for the services to start up. You can check the status of the containers using:

```plaintext
docker-compose ps
```

You should see all the services (postgres, redis, airflow-webserver, airflow-scheduler, airflow-worker, flower) listed as "Up" or "Healthy". You can also check the logs of individual containers using `docker logs <container_name>`. For example, to check the logs of the webserver:

```plaintext
docker logs airflow-webserver
```

**8. Access the Airflow Web UI:**

Once the webserver is up and running, you can access the Airflow Web UI in your browser at `http://localhost:8080`.

The default credentials are:

- **Username:** `airflow`
- **Password:** `airflow`

**Important Security Note:** **Immediately change the default password after logging in!** You can do this in the Airflow UI under "Admin" -> "Users" -> Edit User. For production deployments, consider implementing more robust authentication methods (e.g., LDAP, OAuth).

**9. Access Flower (Celery Monitoring UI):**

You can access the Flower UI, which provides real-time monitoring of the Celery workers, at `http://localhost:5555`. This is helpful for troubleshooting task execution issues.

## Running Your First DAG

Airflow comes with several example DAGs. You should see them in the Airflow UI after logging in. Let's enable the `example_dag_basic` to see Airflow in action:

1.  In the Airflow UI, find the `example_dag_basic` DAG.
2.  Click the "DAG" button (looks like a play button).
3.  Confirm that you want to enable the DAG.

Airflow will now schedule and execute the DAG. You can monitor its progress in the UI by clicking on the DAG name and then viewing the "Graph" or "Tree" view.

## Troubleshooting

Here are some common issues you might encounter and how to resolve them:

- **"Database is not ready" error:** This usually occurs if the Airflow services start before the Postgres database is fully initialized. Ensure that the `postgres` service is running and accessible before starting the other services. You can check the Postgres logs for any errors. The `depends_on` in the `docker-compose.yaml` file helps mitigate this, but sometimes delays can still occur, especially on resource-constrained machines.
- **Permission issues:** If you encounter permission errors when accessing files within the containers (e.g., DAG files), ensure that the user running the containers has the correct permissions to access the mounted directories. On Linux, you might need to adjust the ownership or permissions of the `./dags`, `./logs`, and `./plugins` directories. Using `docker run --rm -v $(pwd)/dags:/opt/airflow/dags -v $(pwd)/logs:/opt/airflow/logs -v $(pwd)/plugins:/opt/airflow/plugins apache/airflow:2.8.1 airflow db init` command often fixes permission issues.
- **Webserver not accessible:** If you can't access the Airflow Web UI, ensure that the `airflow-webserver` container is running and that the port mapping (8080:8080) is correctly configured. Check the webserver logs for any errors. Firewall issues on your host machine might also prevent access.
- **Task failures:** If your tasks are failing, examine the task logs in the Airflow UI for detailed error messages. This will help you identify the root cause of the problem. Check the worker logs as well. Common causes include incorrect dependencies, network connectivity issues, or errors in your DAG code.
- **Celery issues:** If you are using the CeleryExecutor and encounter problems with task execution, ensure that Redis is running correctly and that the Celery worker is properly configured. Check the logs of the `airflow-worker` and `flower` containers for any errors. The Celery broker URL and result backend must be correctly configured in the `docker-compose.yaml` file.

## Customizing Your Airflow Environment

Once you have a basic Airflow installation running, you can customize it to meet your specific needs. Here are a few common customizations:

- **Installing Python Dependencies:** You can install additional Python dependencies required by your DAGs by creating a `requirements.txt` file in your project directory and adding it to the Docker image. You can either build a custom Docker image based on the official Airflow image or use a Dockerfile to install the dependencies. Here's an example of how to create a custom Dockerfile:

  ```dockerfile
  FROM apache/airflow:2.8.1

  USER root

  COPY requirements.txt /tmp/requirements.txt
  RUN pip install --no-cache-dir -r /tmp/requirements.txt

  USER airflow
  ```

  Then, rebuild your images with `docker-compose build`. Ensure your `requirements.txt` file is correctly placed relative to your Dockerfile. The `requirements.txt` file might contain:

  ```
  requests
  pandas
  boto3
  ```

- **Adding Custom Plugins:** You can add custom operators, hooks, sensors, and other plugins by placing them in the `./plugins` directory. Airflow will automatically detect and load them.
- **Configuring Connections:** Define connections to external systems (e.g., databases, APIs) in the Airflow UI under "Admin" -> "Connections". This allows your DAGs to interact with these systems. Use environment variables for sensitive connection details (like passwords) and inject them into the containers.
- **Adjusting Configuration:** You can modify Airflow's configuration by setting environment variables or by creating a custom `airflow.cfg` file. Refer to the Airflow documentation for a complete list of configuration options.

## Cleaning Up

When you're finished experimenting with Airflow, you can stop and remove the containers using:

```plaintext
docker-compose down -v
```

The `-v` flag removes the volumes, including the Postgres database. If you want to preserve the database for future use, omit the `-v` flag.

## Conclusion

This guide has provided a comprehensive overview of installing Apache Airflow using Docker and Docker Compose. By following these steps, you can quickly set up a reliable and reproducible Airflow environment for automating your data workflows. Remember to consult the official Airflow documentation for the latest information and best practices. With a solid Docker-based installation, you'll be well-equipped to leverage the power of Airflow to build and manage complex data pipelines. Good luck!
