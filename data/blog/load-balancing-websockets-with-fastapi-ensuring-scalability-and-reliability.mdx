---
title: 'Load Balancing WebSockets with FastAPI: Ensuring Scalability and Reliability'
date: '2024-02-29'
lastmod: '2024-02-29'
tags:
  [
    'fastapi',
    'websockets',
    'load balancing',
    'redis',
    'asgi',
    'scalability',
    'cloud',
    'kubernetes',
    'python',
  ]
draft: false
summary: 'Learn how to effectively load balance WebSocket connections in a FastAPI application for enhanced scalability and reliability. Explore different strategies and implement a solution using Redis as a message broker for distributed WebSocket management.'
authors: ['default']
---

# Load Balancing WebSockets with FastAPI: Ensuring Scalability and Reliability

WebSockets provide a persistent, full-duplex communication channel between a client and a server, making them ideal for real-time applications such as chat applications, live dashboards, and collaborative tools. When dealing with a high volume of WebSocket connections, a single server can quickly become overwhelmed. This is where load balancing comes in. Load balancing distributes incoming WebSocket connections across multiple servers, ensuring that no single server is overloaded and that the application remains responsive and available.

This blog post will explore strategies for load balancing WebSocket connections in a FastAPI application, focusing on a robust solution using Redis as a message broker.

## Why Load Balancing WebSockets is Crucial

Without load balancing, your WebSocket application faces several challenges:

- **Single Point of Failure:** If the server handling WebSocket connections crashes, all connected clients are disconnected.
- **Performance Bottleneck:** A single server can only handle a limited number of concurrent WebSocket connections. As the number of users increases, performance degrades.
- **Limited Scalability:** You are limited by the resources of a single machine, making it difficult to scale your application to handle increasing demand.

Load balancing solves these problems by distributing the load across multiple servers, ensuring high availability, performance, and scalability.

## Load Balancing Strategies for WebSockets

Several strategies can be used to load balance WebSocket connections:

1.  **Round Robin:** Distributes connections evenly across servers. Simple but doesn't consider server load or connection stickiness. Not ideal for WebSockets where maintaining a persistent connection to the same server is often crucial.

2.  **Least Connections:** Directs new connections to the server with the fewest active connections. A better approach than round robin, but still lacks awareness of individual WebSocket message flows and state.

3.  **IP Hash:** Uses the client's IP address to determine which server to connect to. Ensures that a client is always connected to the same server, providing stickiness. However, clients behind the same NAT (Network Address Translation) may be routed to the same server, creating an uneven distribution of load.

4.  **Message Brokering (Recommended):** This approach decouples WebSocket connections from the application logic. Each server can handle WebSocket connections and communicate with other servers via a message broker (e.g., Redis, RabbitMQ). This strategy offers the most flexibility and scalability. Clients connect to any available server, and the server handles the message routing internally using the message broker. This is the strategy we will focus on in this post.

## Implementing WebSocket Load Balancing with Redis and FastAPI

We'll use Redis as a message broker to facilitate communication between different FastAPI WebSocket endpoints running on different servers. The basic idea is:

1.  Clients connect to any available FastAPI server.
2.  Each server subscribes to a Redis channel for broadcasting messages.
3.  When a server receives a message for a specific client, it publishes the message to the Redis channel.
4.  All servers receive the message from Redis.
5.  The server that has an active WebSocket connection with the target client delivers the message.

Here's a step-by-step guide:

**1. Install Dependencies:**

```plaintext
pip install fastapi uvicorn websockets redis
```

**2. Create a FastAPI Application (main.py):**

```plaintext
from typing import Dict

import redis.asyncio as redis
from fastapi import FastAPI, WebSocket, WebSocketDisconnect
from fastapi.responses import HTMLResponse
import asyncio

app = FastAPI()

# Redis Connection Details (replace with your Redis configuration)
REDIS_HOST = "localhost"
REDIS_PORT = 6379
REDIS_PASSWORD = None  # Or your password if required

redis_client = redis.Redis(host=REDIS_HOST, port=REDIS_PORT, password=REDIS_PASSWORD, decode_responses=True)


html = """
<!DOCTYPE html>
<html>
    <head>
        <title>Chat</title>
    </head>
    <body>
        <h1>WebSocket Chat</h1>
        <input type="text" id="messageText" autocomplete="off"/>
        <button onclick="sendMessage(event)">Send</button>
        <ul id='messages'>
        </ul>
        <script>
            var ws = new WebSocket("ws://localhost:8000/ws/testclient"); // Change localhost:8000 if needed
            ws.onmessage = function(event) {
                var messages = document.getElementById('messages')
                var message = document.createElement('li')
                var content = document.createTextNode(event.data)
                message.appendChild(content)
                messages.appendChild(message)
            };
            function sendMessage(event) {
                var input = document.getElementById("messageText")
                ws.send(input.value)
                input.value = ''
                event.preventDefault()
            }
        </script>
    </body>
</html>
"""


@app.get("/")
async def get():
    return HTMLResponse(html)


class ConnectionManager:
    def __init__(self):
        self.active_connections: Dict[str, WebSocket] = {}

    async def connect(self, client_id: str, websocket: WebSocket):
        await websocket.accept()
        self.active_connections[client_id] = websocket
        print(f"Client {client_id} connected")

    def disconnect(self, client_id: str):
        del self.active_connections[client_id]
        print(f"Client {client_id} disconnected")

    async def send_personal_message(self, message: str, websocket: WebSocket):
        await websocket.send_text(message)


    async def receive_and_broadcast(self, client_id: str, websocket: WebSocket):
        try:
            while True:
                data = await websocket.receive_text()
                # Publish the message to Redis, including the sender
                message_to_publish = f"{client_id}: {data}" # Append client_id to message
                await redis_client.publish("chat", message_to_publish) # Publish message to redis channel 'chat'

        except WebSocketDisconnect:
            self.disconnect(client_id)
            # Unsubscribe from Redis channel
            print("Client disconnected") #Removed redis unsubscribe - not necessary with this example, connections are managed centrally


    async def redis_listener(self):
        pubsub = redis_client.pubsub()
        await pubsub.subscribe("chat")  # Subscribe to the 'chat' channel

        try:
            while True:
                message = await pubsub.get_message(ignore_subscribe_messages=True)
                if message:
                    message_data = message['data'] # message data is the actual message text
                    print(f"Received message from Redis: {message_data}")

                    # Extract sender (client_id) from message data
                    sender, message_text = message_data.split(':', 1) #Split based on the first colon.

                    #Broadcast logic.
                    for client_id, websocket in manager.active_connections.items():
                        #Avoid sending message back to sender.
                        if client_id != sender:
                            await manager.send_personal_message(f"From {sender}: {message_text}", websocket)

                await asyncio.sleep(0.01)  # Prevent busy-waiting

        except Exception as e:
            print(f"Error in Redis listener: {e}")
        finally:
            await pubsub.unsubscribe("chat")
            print("Redis listener stopped")


manager = ConnectionManager()

@app.websocket("/ws/{client_id}")
async def websocket_endpoint(websocket: WebSocket, client_id: str):
    await manager.connect(client_id, websocket)

    # Start the Redis listener as a background task when the first client connects.
    if len(manager.active_connections) == 1:
        asyncio.create_task(manager.redis_listener()) # Start listening from redis.

    await manager.receive_and_broadcast(client_id, websocket)
```

**3. Running the Application**

Run the application using Uvicorn:

```plaintext
uvicorn main:app --reload
```

This will start the FastAPI server on `http://localhost:8000`.

**4. Testing with Multiple Clients**

Open multiple browser windows or tabs and navigate to `http://localhost:8000`. Enter a unique `client_id` in each tab or window's WebSocket connection string when connecting via javascript/code. Type a message in one window and see it appear in the others. This simulates multiple clients connected to different servers.

**Explanation:**

- **`ConnectionManager`:** This class manages the active WebSocket connections. It keeps track of which client is connected to which WebSocket.
- **`redis_client`:** This is an asynchronous Redis client used for publishing and subscribing to messages.
- **`/ws/{client_id}` endpoint:** This WebSocket endpoint handles incoming connections. It accepts the `client_id` as a path parameter.
- **`receive_and_broadcast`:** This function receives messages from the WebSocket connection, prepends the client's ID to the message, and publishes it to the "chat" Redis channel.
- **`redis_listener`:** This function subscribes to the "chat" Redis channel and listens for incoming messages. When a message is received, it checks if the message is for the client connected to that particular server. If so, the message is sent to the client.
- **`asyncio.create_task(manager.redis_listener())`**: This creates a background task for the Redis listener. This ensures that the listener is running even when no WebSocket connections are active. We only start the Redis listener when the first client connects to avoid unnecessary resource consumption when no clients are connected.

**5. Running Multiple Instances**

To simulate a load-balanced environment, run the application on multiple ports:

```plaintext
uvicorn main:app --reload --port 8000
uvicorn main:app --reload --port 8001
uvicorn main:app --reload --port 8002
```

Now you have three instances of the application running on ports 8000, 8001, and 8002. To fully utilize a load balancer, you'd configure a reverse proxy (e.g., Nginx, HAProxy) to distribute incoming traffic across these instances. You can then test the scenario by connecting clients via your Load Balancer's address, which will distribute connections amongst the available FastAPI servers running the provided code.

**6. Configuring a Load Balancer (Nginx Example)**

Here's an example Nginx configuration to load balance WebSocket connections:

```nginx
upstream websocket_servers {
    server localhost:8000;
    server localhost:8001;
    server localhost:8002;
}

server {
    listen 80;

    location / {
        proxy_pass http://websocket_servers;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
        proxy_set_header Host $host;
    }
}
```

Save this configuration to a file (e.g., `nginx.conf`) and start Nginx. Now, clients can connect to `ws://localhost/ws/client123` , and Nginx will distribute the connections among the backend servers.

**Important Considerations:**

- **Redis Configuration:** Ensure your Redis instance is configured for high availability and persistence, especially in production environments. Consider using Redis Sentinel or Redis Cluster.
- **Error Handling:** Implement robust error handling in the `redis_listener` and other parts of the code to handle connection errors and other unexpected issues.
- **Client ID Generation:** Use a robust method for generating unique `client_id` values, such as UUIDs.
- **Scalability:** As your application grows, consider using a more sophisticated message broker like RabbitMQ, which offers more advanced features like message routing and acknowledgements.
- **Deployment:** This setup is well-suited for deployment in containerized environments (e.g., Docker, Kubernetes).
- **Security:** Secure your WebSockets with WSS (WebSocket Secure) by using SSL/TLS certificates. Also, consider implementing authentication and authorization mechanisms to protect your WebSocket endpoints.
- **Health Checks:** Configure health checks in your load balancer to ensure that only healthy servers receive traffic.

## Advantages of Using Redis as a Message Broker

- **Decoupling:** Decouples WebSocket connections from the application logic, making it easier to scale and maintain the application.
- **Scalability:** Allows you to easily add more servers to handle increasing load.
- **Flexibility:** Provides flexibility in how you route messages between clients.
- **Reliability:** Redis is a robust and reliable message broker that can handle a high volume of messages.

## Conclusion

Load balancing WebSocket connections is crucial for building scalable and reliable real-time applications. Using Redis as a message broker in conjunction with FastAPI offers a robust and flexible solution for managing WebSocket connections in a load-balanced environment. By following the steps outlined in this blog post, you can ensure that your WebSocket application remains responsive and available, even under heavy load. Remember to consider the important considerations outlined above to create a secure, production-ready solution.
