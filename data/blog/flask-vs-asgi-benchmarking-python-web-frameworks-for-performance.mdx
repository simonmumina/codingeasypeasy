---
title: 'Flask vs ASGI: Benchmarking Python Web Frameworks for Performance'
date: '2024-10-26'
lastmod: '2024-10-27'
tags:
  [
    'python',
    'flask',
    'asgi',
    'benchmark',
    'web framework',
    'performance',
    'uvicorn',
    'gunicorn',
    'asyncio',
    'web development',
  ]
draft: false
summary: 'A comprehensive guide to benchmarking Flask, a traditional WSGI Python web framework, against modern ASGI frameworks like FastAPI and Quart. Learn how to measure request handling speed, concurrency, and resource usage to choose the best framework for your Python web application.'
authors: ['default']
---

# Flask vs ASGI: Benchmarking Python Web Frameworks for Performance

Choosing the right web framework is crucial for building performant Python web applications. While Flask has been a long-standing and popular choice, the rise of Asynchronous Server Gateway Interface (ASGI) frameworks like FastAPI and Quart promises significant performance improvements, particularly in handling concurrent requests. This blog post provides a detailed guide to benchmarking Flask against these ASGI alternatives, enabling you to make an informed decision based on your application's specific needs.

## Understanding WSGI and ASGI

Before diving into benchmarking, let's briefly understand the core difference between WSGI (Web Server Gateway Interface) and ASGI (Asynchronous Server Gateway Interface).

- **WSGI (Web Server Gateway Interface):** The traditional standard for Python web applications. It's synchronous, meaning each request occupies a thread until completion. This can lead to bottlenecks with I/O-bound operations. Flask, Django, and other classic frameworks rely on WSGI.

- **ASGI (Asynchronous Server Gateway Interface):** An evolution of WSGI that supports asynchronous operations using `asyncio`. ASGI allows a single thread to handle multiple concurrent requests, significantly improving performance for I/O-bound tasks. Frameworks like FastAPI, Quart, and Starlette are built on ASGI.

The key advantage of ASGI is its ability to handle more concurrent requests with fewer resources, leading to lower latency and higher throughput.

## Benchmarking Methodology

To accurately compare Flask and ASGI frameworks, we'll use a standardized approach:

1.  **Simple Application:** Create minimal "Hello, World!" applications for each framework. This eliminates application logic as a bottleneck and focuses on framework performance.
2.  **Load Testing Tool:** Utilize a load testing tool like `wrk` or `hey` to simulate a large number of concurrent requests.
3.  **Metrics:** Measure key performance indicators (KPIs) such as:
    - **Requests per second (RPS):** The number of requests the server can handle per second.
    - **Latency (Average, Max, Percentiles):** The time it takes to process a request.
    - **CPU Utilization:** The percentage of CPU resources used.
    - **Memory Usage:** The amount of memory the application consumes.
4.  **Deployment Environment:** Use a consistent deployment environment (e.g., same server hardware, operating system) for all frameworks.
5.  **Multiple Runs:** Conduct multiple benchmark runs to account for variability and calculate average results.
6.  **Vary Concurrency:** Test with different levels of concurrency (number of concurrent connections) to observe how performance scales.

## Code Examples: Minimal Applications

Here are the minimal "Hello, World!" applications we'll use for benchmarking:

**1. Flask (WSGI):**

```plaintext
# app.py
from flask import Flask

app = Flask(__name__)

@app.route('/')
def hello_world():
    return 'Hello, World!'

if __name__ == '__main__':
    app.run(debug=False, host='0.0.0.0', port=8000)
```

**2. FastAPI (ASGI):**

```plaintext
# main.py
from fastapi import FastAPI

app = FastAPI()

@app.get("/")
async def read_root():
    return {"message": "Hello, World!"}
```

**3. Quart (ASGI):**

```plaintext
# app.py
from quart import Quart

app = Quart(__name__)

@app.route('/')
async def hello_world():
    return 'Hello, World!'

if __name__ == '__main__':
    app.run(debug=False, host='0.0.0.0', port=8000)
```

## Setting Up the Environment

1.  **Install Frameworks:** Use `pip` to install the required packages.

    ```plaintext
    pip install flask uvicorn gunicorn fastapi quart
    ```

2.  **Choose a Web Server:**

    - **Flask:** We'll use Gunicorn with multiple worker processes. Gunicorn is a WSGI HTTP server.
    - **FastAPI:** We'll use Uvicorn. Uvicorn is an ASGI server.
    - **Quart:** We'll use Uvicorn. Quart is designed to work natively with ASGI servers like Uvicorn.

3.  **Deployment Scripts:** Create simple scripts to run each application using Gunicorn and Uvicorn.

    **Flask (Gunicorn):**

    ```plaintext
    gunicorn --workers 3 --bind 0.0.0.0:8000 app:app
    ```

    **FastAPI (Uvicorn):**

    ```plaintext
    uvicorn main:app --host 0.0.0.0 --port 8000 --workers 3
    ```

    **Quart (Uvicorn):**

    ```plaintext
    uvicorn app:app --host 0.0.0.0 --port 8000 --workers 3
    ```

    _Note: Adjust the number of workers based on your server's CPU cores. A common rule of thumb is 2-4 workers per CPU core._

## Using `wrk` for Load Testing

`wrk` is a command-line HTTP benchmarking tool. If you don't have it, you can usually install it using your system's package manager (e.g., `apt-get install wrk` on Debian/Ubuntu, `brew install wrk` on macOS). Alternatively `hey` is also another very good option and can be installed using `go install github.com/rakyll/hey@latest`

Example `wrk` command:

```plaintext
wrk -t4 -c200 -d10s http://localhost:8000
```

- `-t4`: Use 4 threads.
- `-c200`: Maintain 200 concurrent connections.
- `-d10s`: Run the test for 10 seconds.
- `http://localhost:8000`: The target URL.

Example `hey` command:

```plaintext
hey -c 200 -n 20000 -q 10 http://localhost:8000
```

- `-c 200`: Maintain 200 concurrent connections.
- `-n 20000`: Make 20000 requests.
- `-q 10`: Send 10 queries per second

## Interpreting the Results

After running the benchmark, analyze the output from `wrk` or `hey`. Pay close attention to:

- **Requests per second (RPS):** A higher RPS indicates better performance.
- **Latency:** Lower latency is better. Analyze average latency and percentiles (e.g., 99th percentile) to understand worst-case performance.
- **Errors/Failures:** A high number of errors indicates the server is overloaded.

## Example Benchmark Results (Illustrative)

The actual results will vary depending on your hardware and configuration. Here's an example of what you might see:

| Framework | Requests per Second (RPS) | Average Latency (ms) |
| --------- | ------------------------- | -------------------- |
| Flask     | 1200                      | 5.5                  |
| FastAPI   | 4500                      | 1.2                  |
| Quart     | 4000                      | 1.3                  |

In this hypothetical scenario, FastAPI and Quart significantly outperform Flask in terms of RPS and latency.

## Optimizations

Several factors can influence benchmark results. Consider these optimizations:

- **Number of Workers:** Experiment with the number of Gunicorn/Uvicorn workers to find the optimal configuration for your hardware.
- **Asyncio Event Loop:** For ASGI frameworks, try different event loops (e.g., uvloop) for potential performance gains. Uvloop is a high-performance event loop written in Cython. To use it with FastAPI:

  ```plaintext
  # main.py
  import asyncio
  import uvloop
  from fastapi import FastAPI

  uvloop.install()
  app = FastAPI()

  @app.get("/")
  async def read_root():
      return {"message": "Hello, World!"}
  ```

  And when running Uvicorn:

  ```plaintext
  uvicorn main:app --host 0.0.0.0 --port 8000 --workers 3 --loop uvloop
  ```

- **Keep-Alive Connections:** Ensure HTTP keep-alive is enabled for persistent connections, reducing overhead. `wrk` uses keep-alive by default. For `hey`, use the `-disable-keepalive=false` flag.
- **Gunicorn/Uvicorn Configuration:** Tune Gunicorn/Uvicorn settings like `timeout`, `keepalive`, and `accesslog` for optimal performance.
- **Hardware:** More powerful hardware (CPU, RAM, SSD) will generally improve performance.
- **Code Profiling:** Use a Python profiler to identify performance bottlenecks in your application code.

## Beyond "Hello, World!"

While benchmarking a simple "Hello, World!" application provides a baseline, it's essential to benchmark with realistic workloads. Consider testing with:

- **Database Interactions:** Simulate database queries (e.g., using SQLAlchemy) to measure performance with I/O-bound operations.
- **Complex Logic:** Implement more complex application logic to see how frameworks handle CPU-intensive tasks.
- **Middleware:** Include middleware components (e.g., authentication, logging) to assess their impact on performance.
- **Real-world data**: Use real-world data inputs to simulate actual requests as closely as possible.

## When to Choose Flask vs. ASGI

- **Choose Flask if:**

  - You have an existing WSGI application and are not yet ready to migrate to ASGI.
  - Your application is not heavily I/O-bound and can handle the concurrency limitations of WSGI.
  - You prefer the simplicity and extensive ecosystem of Flask.
  - You don't need the raw performance of an ASGI application.

- **Choose ASGI (FastAPI, Quart) if:**
  - You need high concurrency and low latency.
  - Your application is heavily I/O-bound (e.g., API endpoints that frequently interact with databases or external services).
  - You want to leverage the benefits of `asyncio` for improved performance.
  - You are building a new application from scratch and want to take advantage of modern features.

## Conclusion

Benchmarking is crucial for understanding the performance characteristics of different Python web frameworks. By following the methodology outlined in this guide and testing with realistic workloads, you can make an informed decision about whether Flask or an ASGI framework is the best choice for your specific application. Remember to continuously monitor and optimize your application's performance as it evolves. The move to ASGI can yield significant benefits, but it's important to weigh the complexity of the change against the gains you'll see in performance.
