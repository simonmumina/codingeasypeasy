---
title: 'Real-Time ML Features with Feast and Flask: A Comprehensive Guide'
date: '2024-03-08'
lastmod: '2024-03-08'
tags:
  [
    'machine learning',
    'feast',
    'feature store',
    'flask',
    'real-time',
    'python',
    'deployment',
    'inference',
  ]
draft: false
summary: 'Learn how to integrate Feast, a feature store, with Flask to serve real-time machine learning features for your model inference API. This guide covers installation, feature definition, data ingestion, and model deployment.'
authors: ['default']
---

# Real-Time ML Features with Feast and Flask: A Comprehensive Guide

In the ever-evolving world of machine learning, deploying models that make accurate predictions in real-time is a crucial objective. This often necessitates access to up-to-date and relevant features. Feature stores, like Feast, play a critical role in managing and serving these features efficiently. This blog post provides a detailed guide on how to integrate Feast with Flask to create a robust real-time feature serving layer for your machine learning models.

## Why Use Feast with Flask?

- **Real-Time Feature Access:** Feast allows you to access features at low latency, enabling real-time predictions.
- **Feature Consistency:** Ensures that features used for training and inference are consistent, preventing training-serving skew.
- **Scalability:** Feast is designed to scale to handle large volumes of data and requests.
- **Centralized Feature Management:** Provides a central repository for defining, storing, and managing your features.
- **Simplified Deployment:** Flask offers a lightweight and flexible framework for deploying your model and feature serving logic.

## Prerequisites

Before we dive in, make sure you have the following installed:

- **Python 3.7+:** Python is the primary language we'll be using.
- **Feast:** The feature store we'll be integrating. Install with `pip install feast[snowflake]` or your preferred offline store. Adjust the `[snowflake]` part depending on your preferred offline store (e.g., `[bigquery]`, `[redshift]`). Install `feast` without any brackets for a default setup.
- **Flask:** The web framework for serving our API. Install with `pip install flask`.
- **Pandas:** For data manipulation. `pip install pandas`.
- **Scikit-learn (Optional):** If you plan to deploy a model as part of your API. `pip install scikit-learn`.
- **A Feast-Supported Offline Store:** Examples in this guide may use a Snowflake instance for offline storage. Alternatives include BigQuery, Redshift, etc. Feast also supports an in-memory store for quick prototyping. Consult the Feast documentation for a comprehensive list.

## Step 1: Setting Up Your Feast Environment

First, initialize a new Feast project:

```bash
feast init my_feature_repo
cd my_feature_repo
```

This creates a directory structure with the necessary files for defining your feature repository. Let's examine the key components.

### `feature_store.yaml`: Configuration File

This file configures your Feast deployment. Open `feature_store.yaml` and configure your offline and online stores. A basic example using the in-memory store might look like this:

```yaml
project: my_feature_repo
registry: data/registry.db
provider: local
online_store:
  type: sqlite
  path: data/online_store.db
offline_store:
  type: file
  path: data/offline_store.parquet
```

**Important:** For production environments, _never_ use the in-memory or file-based offline stores. Choose a robust database solution like Snowflake, BigQuery, or Redshift. Adjust the `feature_store.yaml` file accordingly based on your chosen offline/online store.

### Defining Feature Views: `example.py`

This file contains your feature definitions. A `FeatureView` specifies the data source, schema, and freshness. Let's create a simple example using a driver ranking system:

```plaintext
# my_feature_repo/driver_ranking/feature_defs.py

from feast import FeatureView, Field
from feast.types import Float32, Int64, String
from datetime import timedelta

driver_hourly_stats = FeatureView(
    name="driver_hourly_stats",
    entities=["driver_id"],
    ttl=timedelta(days=1),  # Time to live for feature values
    features=[
        Field(name="average_daily_rides", dtype=Float32),
        Field(name="ride_quality", dtype=Float32),
        Field(name="driver_age", dtype=Int64),
        Field(name="driver_name", dtype=String),
    ],
    online=True,
    source="driver_hourly_stats_source",  # This is a name; define it separately.
    tags={},  # Optional tags for organizing FeatureViews
)

# Define the data source (replace with your actual data source configuration)
from feast.data_source import FileSource

driver_hourly_stats_source = FileSource(
    path="data/driver_stats.parquet",  # Path to your Parquet file
    event_timestamp_column="event_timestamp",  # Name of the timestamp column
    created_timestamp_column="created",      # Name of the creation timestamp column
)
```

**Explanation:**

- **`FeatureView`:** Defines the group of features.
- **`name`:** A unique identifier for the feature view.
- **`entities`:** The entity (e.g., `driver_id`) that the features are associated with. Entities are the keys used to retrieve features.
- **`features`:** A list of `Field` objects, each representing a feature with its name and data type.
- **`ttl`:** The maximum age of the feature data.
- **`online`:** Whether to store features in the online store for low-latency access. `True` for real-time serving.
- **`source`:** The data source where the feature data is stored. We define it using `FileSource` pointing to a parquet file in this simple example. In a real-world scenario, this would likely be a database table or a streaming source.
- **`event_timestamp_column`:** The name of the column representing when the event happened. Important for point-in-time correctness.
- **`created_timestamp_column`:** The name of the column indicating when the record was created. Helps with data freshness.

**Note:** Replace `"data/driver_stats.parquet"` with the actual path to your data. You'll need to create this data in a subsequent step.

### Defining Entities: `example.py` (Continued)

You'll also want to define the Entities used in your `FeatureView`. Add this to your `feature_defs.py` file:

```plaintext
from feast import Entity

driver_id = Entity(name="driver_id", value_type=Int64, description="The ID of the driver")
```

This defines the `driver_id` entity, specifying its data type (`Int64`) and a description.

### Complete `example.py`

Your `example.py` file should now contain the following:

```plaintext
# my_feature_repo/driver_ranking/feature_defs.py

from feast import FeatureView, Field, Entity
from feast.types import Float32, Int64, String
from datetime import timedelta
from feast.data_source import FileSource


driver_hourly_stats_source = FileSource(
    path="data/driver_stats.parquet",
    event_timestamp_column="event_timestamp",
    created_timestamp_column="created",
)


driver_hourly_stats = FeatureView(
    name="driver_hourly_stats",
    entities=["driver_id"],
    ttl=timedelta(days=1),
    features=[
        Field(name="average_daily_rides", dtype=Float32),
        Field(name="ride_quality", dtype=Float32),
        Field(name="driver_age", dtype=Int64),
        Field(name="driver_name", dtype=String),
    ],
    online=True,
    source=driver_hourly_stats_source,
    tags={},
)

driver_id = Entity(name="driver_id", value_type=Int64, description="The ID of the driver")
```

## Step 2: Ingesting Data into Feast

Before you can retrieve features, you need to ingest data into your Feast offline store. Create a sample Parquet file (`data/driver_stats.parquet`):

```plaintext
import pandas as pd
from datetime import datetime, timezone

# Sample data
data = {
    'driver_id': [1001, 1002, 1001, 1003],
    'average_daily_rides': [15.5, 20.0, 18.2, 12.7],
    'ride_quality': [4.5, 4.8, 4.6, 4.2],
    'driver_age': [35, 42, 28, 51],
    'driver_name': ["Alice", "Bob", "Charlie", "David"],
    'event_timestamp': [datetime(2024, 3, 8, 10, 0, 0, tzinfo=timezone.utc),
                        datetime(2024, 3, 8, 10, 15, 0, tzinfo=timezone.utc),
                        datetime(2024, 3, 8, 10, 30, 0, tzinfo=timezone.utc),
                        datetime(2024, 3, 8, 10, 45, 0, tzinfo=timezone.utc)],
    'created': [datetime(2024, 3, 8, 10, 0, 0, tzinfo=timezone.utc),
                datetime(2024, 3, 8, 10, 15, 0, tzinfo=timezone.utc),
                datetime(2024, 3, 8, 10, 30, 0, tzinfo=timezone.utc),
                datetime(2024, 3, 8, 10, 45, 0, tzinfo=timezone.utc)]
}

df = pd.DataFrame(data)

# Save to Parquet
df.to_parquet('data/driver_stats.parquet')

print("Parquet file created successfully at data/driver_stats.parquet")
```

Next, apply your feature definitions:

```bash
feast apply
```

This command registers your feature views and entities with Feast.

Finally, ingest the data from your offline store into the online store:

```bash
feast materialize-incremental $(date -v-1d '+%Y-%m-%dT%H:%M:%S')
```

This command materializes data from yesterday until now. You can also use `feast materialize` to materialize a specific time range. Make sure you have the path to the `data/driver_stats.parquet` file present in your `feature_store.yaml`.

## Step 3: Creating a Flask API

Now, let's create a Flask API to serve real-time features.

```plaintext
# app.py

from flask import Flask, request, jsonify
from feast import FeatureStore
import os

app = Flask(__name__)

# Initialize Feast feature store
# Get the absolute path to the feature repository
repo_path = os.path.abspath("my_feature_repo")
fs = FeatureStore(repo_path=repo_path)


@app.route('/predict', methods=['POST'])
def predict():
    """
    Endpoint to retrieve features and make a prediction (placeholder).
    """
    try:
        data = request.get_json()
        driver_id = data.get('driver_id')

        if driver_id is None:
            return jsonify({"error": "driver_id is required"}), 400

        # Retrieve features from Feast
        feature_vector = fs.get_online_features(
            features=[
                "driver_hourly_stats:average_daily_rides",
                "driver_hourly_stats:ride_quality",
                "driver_hourly_stats:driver_age",
                "driver_hourly_stats:driver_name",
            ],
            entity_rows=[{"driver_id": driver_id}],
        ).to_dict()

        # Check if features were retrieved
        if not feature_vector:
            return jsonify({"error": f"No features found for driver_id {driver_id}"}), 404

        # Extract the feature values.  The result from `to_dict()` is a dictionary where
        # keys are the feature names and the values are lists. We only requested
        # one entity, so we can take the first element of each list.
        features = {
            "average_daily_rides": feature_vector["driver_hourly_stats:average_daily_rides"][0],
            "ride_quality": feature_vector["driver_hourly_stats:ride_quality"][0],
            "driver_age": feature_vector["driver_hourly_stats:driver_age"][0],
            "driver_name": feature_vector["driver_hourly_stats:driver_name"][0],
        }

        # Placeholder for model prediction
        prediction = f"Placeholder prediction for driver {driver_id} with features: {features}"

        return jsonify({"driver_id": driver_id, "prediction": prediction, "features": features}), 200

    except Exception as e:
        print(f"Error: {e}")
        return jsonify({"error": str(e)}, {"message": "An unexpected error occurred"}), 500


if __name__ == '__main__':
    app.run(debug=True)  # Use debug=False for production
```

**Explanation:**

- **Flask Setup:** Initializes a Flask application.
- **Feast Initialization:** Initializes the Feast `FeatureStore` with the path to your feature repository. Critically, we now use `os.path.abspath` to ensure that the path is correct no matter where you run the script from.
- **`predict` Endpoint:**
  - Extracts the `driver_id` from the request body.
  - Uses `fs.get_online_features()` to retrieve the specified features from Feast's online store. The feature names are specified as strings in the format `"feature_view_name:feature_name"`.
  - Converts the results to a dictionary using `to_dict()`.
  - Extracts feature values from dictionary which holds list of values for features for all requested entities. In this case it only holds a single entity.
  - Includes a placeholder for your model prediction logic. **Replace this with your actual model inference code.**
  - Returns the prediction and the features as a JSON response.
- **Error Handling:** Includes basic error handling for missing `driver_id` and other exceptions.

## Step 4: Running the API and Making Requests

1. **Start the Flask application:**

   ```bash
   python app.py
   ```

2. **Send a POST request to the `/predict` endpoint:**

   You can use `curl`, `Postman`, or any other HTTP client. Here's an example using `curl`:

   ```bash
   curl -X POST -H "Content-Type: application/json" -d '{"driver_id": 1001}' http://127.0.0.1:5000/predict
   ```

   This should return a JSON response containing the prediction and the retrieved features. For example:

   ```json
   {
     "driver_id": 1001,
     "prediction": "Placeholder prediction for driver 1001 with features: {'average_daily_rides': 15.5, 'ride_quality': 4.5, 'driver_age': 35, 'driver_name': 'Alice'}",
     "features": {
       "average_daily_rides": 15.5,
       "ride_quality": 4.5,
       "driver_age": 35,
       "driver_name": "Alice"
     }
   }
   ```

## Step 5: Integrating with Your Model

The most important step is to replace the placeholder prediction logic in the `predict` function with your actual machine learning model inference code. Here's an example using Scikit-learn:

```plaintext
# app.py (modified)

from flask import Flask, request, jsonify
from feast import FeatureStore
import os
import joblib  # For loading the model
import pandas as pd #For transforming dictionary to dataframe

app = Flask(__name__)

# Initialize Feast feature store
repo_path = os.path.abspath("my_feature_repo")
fs = FeatureStore(repo_path=repo_path)

# Load your pre-trained model (replace with your actual model path)
model = joblib.load("model.pkl") # Assuming your model is saved as model.pkl

@app.route('/predict', methods=['POST'])
def predict():
    """
    Endpoint to retrieve features and make a prediction.
    """
    try:
        data = request.get_json()
        driver_id = data.get('driver_id')

        if driver_id is None:
            return jsonify({"error": "driver_id is required"}), 400

        # Retrieve features from Feast
        feature_vector = fs.get_online_features(
            features=[
                "driver_hourly_stats:average_daily_rides",
                "driver_hourly_stats:ride_quality",
                "driver_hourly_stats:driver_age",
            ],
            entity_rows=[{"driver_id": driver_id}],
        ).to_dict()

        # Check if features were retrieved
        if not feature_vector:
            return jsonify({"error": f"No features found for driver_id {driver_id}"}), 404

        # Extract the feature values. Only including numerical features for model prediction
        features = {
            "average_daily_rides": feature_vector["driver_hourly_stats:average_daily_rides"][0],
            "ride_quality": feature_vector["driver_hourly_stats:ride_quality"][0],
            "driver_age": feature_vector["driver_hourly_stats:driver_age"][0],
        }


        # Convert features to a DataFrame for prediction
        feature_df = pd.DataFrame([features])

        # Make a prediction using your model
        prediction = model.predict(feature_df)

        return jsonify({"driver_id": driver_id, "prediction": prediction[0], "features": features}), 200

    except Exception as e:
        print(f"Error: {e}")
        return jsonify({"error": str(e)}), 500


if __name__ == '__main__':
    #Create dummy model for demonstration purpose
    from sklearn.linear_model import LinearRegression
    import numpy as np
    # Create a dummy model
    model = LinearRegression()
    X = np.array([[1, 1, 1], [2, 2, 2], [3, 3, 3]])
    y = np.array([2, 4, 6])
    model.fit(X, y)

    # Save the dummy model to a file
    joblib.dump(model, 'model.pkl')

    app.run(debug=True)  # Use debug=False for production
```

**Changes:**

- **Model Loading:** Loads a pre-trained Scikit-learn model using `joblib`. **Replace `"model.pkl"` with the actual path to your model file.** You need to train and save your model beforehand. In this example, a dummy model is trained and saved during app startup if "model.pkl" does not exist.
- **Feature Transformation:** Transform feature dictionary to Pandas DataFrame to enable passing to machine learning model for prediction.
- **Prediction:** Calls the model's `predict()` method to make a prediction based on the retrieved features.
- **Numerical Features:** Only uses numerical features (average_daily_rides, ride_quality, driver_age) in the model prediction. You'll need to adapt this based on the features your model expects. In this case driver name has been removed.
- **Error Handling:** Maintains the error handling from the previous example.

**Important Considerations:**

- **Model Training:** You need to train your model offline using historical feature data. Feast can help you retrieve this historical data consistently with your online feature serving.
- **Feature Engineering:** You might need to perform feature engineering steps (e.g., scaling, one-hot encoding) before feeding the features to your model. Ensure that you apply the same feature engineering steps during both training and inference.
- **Model Versioning:** Implement a mechanism for managing different versions of your model.
- **Monitoring:** Monitor the performance of your model and feature serving layer to detect issues and ensure accuracy.

## Step 6: Production Deployment

Deploying your Flask API with Feast integration to production requires careful planning and consideration. Here are some key aspects:

- **Containerization:** Use Docker to containerize your application. This ensures consistent execution across different environments.

  - Create a `Dockerfile`:

    ```dockerfile
    FROM python:3.9-slim-buster

    WORKDIR /app

    COPY requirements.txt .
    RUN pip install --no-cache-dir -r requirements.txt

    COPY . .

    CMD ["python", "app.py"]
    ```

  - Create a `requirements.txt` file:

    ```txt
    Flask
    feast[snowflake]  # Replace [snowflake] with your offline store
    joblib
    pandas
    scikit-learn  # Add if you're using scikit-learn
    ```

  - Build the Docker image:

    ```bash
    docker build -t feast-flask-app .
    ```

  - Run the Docker container:

    ```bash
    docker run -p 5000:5000 feast-flask-app
    ```

- **Scalability:** Use a load balancer and multiple instances of your Flask application to handle increased traffic. Consider using a container orchestration platform like Kubernetes.
- **Monitoring and Logging:** Implement comprehensive monitoring and logging to track the performance of your API and Feast infrastructure. Use tools like Prometheus, Grafana, and ELK stack.
- **Security:** Secure your API using appropriate authentication and authorization mechanisms. Consider using HTTPS.
- **Infrastructure as Code (IaC):** Use IaC tools like Terraform or CloudFormation to automate the provisioning and management of your infrastructure.
- **Online Store Configuration:** Your online store should be highly available and scalable. Consider using a managed database service like Redis or Cassandra.
- **Data Freshness:** Ensure that your features are updated regularly to reflect the latest data. Use Feast's built-in scheduling capabilities or an external scheduler.

## Conclusion

Integrating Feast with Flask provides a powerful solution for serving real-time machine learning features. By following the steps outlined in this guide, you can build a robust and scalable feature serving layer for your models. Remember to tailor the configuration and code examples to your specific use case and environment. Always prioritize data consistency, performance monitoring, and security in your production deployment. Good luck!
