---
title: 'Kafka on Kubernetes: Deployment Patterns, Best Practices, and Examples'
date: '2024-02-29'
lastmod: '2024-10-27'
tags:
  [
    'kafka',
    'kubernetes',
    'k8s',
    'deployment',
    'apache kafka',
    'cloud native',
    'statefulsets',
    'operators',
    'helm',
    'zookeeper',
    'messaging',
    'event streaming',
  ]
draft: false
summary: 'Explore various Kafka deployment patterns on Kubernetes, including StatefulSets, Operators, and Helm charts. Learn best practices for resilience, scalability, and monitoring for your Kafka clusters running on K8s.'
authors: ['default']
---

# Kafka on Kubernetes: Deployment Patterns, Best Practices, and Examples

Kafka has become the de-facto standard for event streaming, powering real-time data pipelines and applications. Kubernetes, the leading container orchestration platform, offers a powerful environment to deploy and manage Kafka clusters. Combining these technologies unlocks significant benefits: improved scalability, resilience, and resource utilization. However, deploying Kafka on Kubernetes requires careful planning and understanding of various deployment patterns. This comprehensive guide explores different approaches, best practices, and practical examples to help you successfully run Kafka on Kubernetes.

## Why Run Kafka on Kubernetes?

Before diving into deployment patterns, let's understand the motivations for running Kafka on Kubernetes:

- **Scalability and Elasticity:** Kubernetes allows you to easily scale Kafka brokers up or down based on demand, ensuring optimal resource utilization and performance.
- **Resilience and Fault Tolerance:** Kubernetes provides mechanisms for automatic restarts and failover, enhancing the reliability of your Kafka cluster. The `StatefulSet` controller is particularly well-suited for Kafka due to its stable network identities and ordered deployment/termination.
- **Simplified Management:** Kubernetes provides a unified platform for managing all aspects of your Kafka deployment, from configuration to monitoring.
- **Infrastructure as Code:** Define your Kafka infrastructure using declarative YAML files, enabling version control and automated deployments.
- **Cost Optimization:** Kubernetes helps you optimize resource allocation, reducing infrastructure costs.
- **Cloud-Native Architecture:** Integrate Kafka seamlessly into your cloud-native ecosystem, leveraging Kubernetes' features for networking, storage, and security.

## Deployment Patterns for Kafka on Kubernetes

There are several ways to deploy Kafka on Kubernetes, each with its own trade-offs. Let's explore the most common patterns:

### 1. StatefulSets

StatefulSets are the recommended approach for managing stateful applications like Kafka on Kubernetes. They provide:

- **Stable Network Identities:** Each Kafka broker gets a unique and persistent hostname (e.g., `kafka-0.my-kafka-cluster.default.svc.cluster.local`). This is crucial for Kafka's internal communication.
- **Ordered Deployment and Scaling:** Brokers are deployed and scaled in a specific order, ensuring data consistency. This prevents the "split-brain" scenario where multiple brokers think they are the leader.
- **Persistent Storage:** StatefulSets are designed to work with Persistent Volumes (PVs), allowing each broker to have its own dedicated storage.

**Example: Kafka StatefulSet YAML**

```plaintext
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: kafka
spec:
  serviceName: 'kafka'
  replicas: 3
  selector:
    matchLabels:
      app: kafka
  template:
    metadata:
      labels:
        app: kafka
    spec:
      containers:
        - name: kafka
          image: confluentinc/cp-kafka:7.3.0 # Consider using a specific, stable version
          ports:
            - containerPort: 9092
              name: client
            - containerPort: 9093
              name: interbroker
          env:
            - name: KAFKA_BROKER_ID
              value: $(POD_NAME)
            - name: KAFKA_ZOOKEEPER_CONNECT
              value: zookeeper:2181
            - name: KAFKA_LISTENERS
              value: PLAINTEXT://:9092,BROKER://:9093
            - name: KAFKA_ADVERTISED_LISTENERS
              value: PLAINTEXT://kafka-$(POD_NAME).kafka.default.svc.cluster.local:9092,BROKER://kafka-$(POD_NAME).kafka.default.svc.cluster.local:9093
            - name: KAFKA_LISTENER_SECURITY_PROTOCOL_MAP
              value: PLAINTEXT:PLAINTEXT,BROKER:PLAINTEXT
            - name: KAFKA_INTER_BROKER_LISTENER_NAME
              value: BROKER
            - name: KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR
              value: '1' # Adjust based on your replication requirements.  Consider '3' for production.
            - name: KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR
              value: '1' # Adjust based on your replication requirements. Consider '3' for production.
            - name: KAFKA_TRANSACTION_STATE_LOG_MIN_ISR
              value: '1' # Adjust based on your replication requirements. Consider '2' for replication factor of '3'
          volumeMounts:
            - name: data
              mountPath: /var/lib/kafka/data
  volumeClaimTemplates:
    - metadata:
        name: data
      spec:
        accessModes: ['ReadWriteOnce']
        resources:
          requests:
            storage: 10Gi # Adjust storage request as needed

---
apiVersion: v1
kind: Service
metadata:
  name: kafka
spec:
  clusterIP: None # Important for StatefulSets
  selector:
    app: kafka
  ports:
    - protocol: TCP
      port: 9092
      name: client
    - protocol: TCP
      port: 9093
      name: interbroker
```

**Explanation:**

- **`StatefulSet` Kind:** Defines the resource type as a StatefulSet.
- **`serviceName: "kafka"`:** Specifies the headless service that manages the network identities of the pods.
- **`replicas: 3`:** Deploys three Kafka brokers.
- **`KAFKA_BROKER_ID`:** Sets the broker ID based on the pod name. This ensures each broker has a unique ID.
- **`KAFKA_ZOOKEEPER_CONNECT`:** Specifies the connection string for Zookeeper (required by Kafka). You'll need to deploy Zookeeper separately. Consider using a Zookeeper Operator for simplified management.
- **`KAFKA_LISTENERS` and `KAFKA_ADVERTISED_LISTENERS`:** Defines the listeners that Kafka brokers expose for client and inter-broker communication. `ADVERTISED_LISTENERS` is _crucial_ for clients to connect correctly. It's important to configure the listeners based on your network configuration and Kubernetes ingress/service setup. The example uses the fully qualified domain name (FQDN) of the pod. For external access you'll need further configuration.
- **`volumeClaimTemplates`:** Defines a Persistent Volume Claim (PVC) for each broker, ensuring persistent storage.
- **`Service` (Headless Service):** The headless service enables stable network identities for the Kafka brokers. Its `clusterIP: None` setting is critical for StatefulSets.

**Important Considerations for StatefulSets:**

- **Zookeeper:** Kafka relies on Zookeeper for metadata management. You'll need to deploy Zookeeper as well, ideally using a StatefulSet or a Zookeeper Operator. Consider alternatives like KRaft mode in newer Kafka versions (described below).
- **Storage:** Carefully choose the storage class and size based on your data volume and performance requirements. Consider using local storage with proper data replication for optimal performance.
- **Networking:** Ensure that Kafka brokers can communicate with each other and with clients. Use appropriate Kubernetes networking policies and service configurations.
- **Monitoring:** Implement comprehensive monitoring of your Kafka cluster using tools like Prometheus and Grafana.

### 2. Kafka Operators

Kafka Operators automate the deployment, management, and scaling of Kafka clusters on Kubernetes. They provide a higher level of abstraction and simplify the operational aspects of running Kafka.

**Benefits of Using a Kafka Operator:**

- **Automated Deployments:** Easily deploy and configure Kafka clusters with a few simple commands.
- **Simplified Scaling:** Scale your Kafka cluster up or down without manual intervention.
- **Automated Upgrades:** Perform rolling upgrades of Kafka brokers without downtime.
- **Self-Healing:** Automatically recover from failures and ensure cluster health.
- **Lifecycle Management:** Automate tasks such as backup, restore, and configuration management.

**Popular Kafka Operators:**

- **Strimzi:** A popular open-source Kafka Operator developed by Red Hat. It's highly configurable and supports various deployment scenarios.
- **CNCF Pravega:** An open-source streaming storage system that integrates with Kubernetes and provides a Kafka operator.
- **Red Hat OpenShift Streams for Apache Kafka:** Based on Strimzi and available as a managed service on OpenShift.

**Example: Using Strimzi to Deploy Kafka**

First, install the Strimzi Operator using the provided YAML files:

```plaintext
kubectl create namespace kafka
kubectl apply -f 'https://strimzi.io/install/latest?namespace=kafka' -n kafka
```

Then, create a Kafka cluster using a custom resource definition (CRD):

```plaintext
apiVersion: kafka.strimzi.io/v1beta2
kind: Kafka
metadata:
  name: my-cluster
  namespace: kafka
spec:
  kafka:
    version: 3.3.1 # Adjust to your desired Kafka version
    replicas: 3
    listeners:
      - name: plain
        port: 9092
        type: internal
        tls: false
      - name: tls
        port: 9093
        type: internal
        tls: true
    config:
      offsets.topic.replication.factor: 3
      transaction.state.log.replication.factor: 3
      transaction.state.log.min.isr: 2
    storage:
      type: jbod
      volumes:
        - id: 0
          type: persistent-claim
          size: 10Gi
          deleteClaim: false # Consider setting to true in development environments

  zookeeper:
    replicas: 3
    storage:
      type: persistent-claim
      size: 10Gi
      deleteClaim: false # Consider setting to true in development environments

  entityOperator:
    topicOperator: {}
    userOperator: {}
```

**Explanation:**

- **`apiVersion: kafka.strimzi.io/v1beta2` and `kind: Kafka`:** Define a Kafka custom resource.
- **`kafka` Section:** Configures the Kafka cluster, including the version, number of replicas, listeners, and storage.
- **`zookeeper` Section:** Configures the Zookeeper cluster.
- **`entityOperator` Section:** Enables the Topic Operator and User Operator, which automate the management of Kafka topics and users.

Apply this YAML file to your Kubernetes cluster:

```plaintext
kubectl apply -f kafka-cluster.yaml -n kafka
```

The Strimzi Operator will then deploy and manage the Kafka cluster according to the configuration you provided. It will handle tasks such as creating the necessary StatefulSets, Services, and Persistent Volume Claims.

**Advantages of using Strimzi**

- Provides an interface for managing your Kafka connect, Kafka bridge, Kafka mirror maker instances.
- Provides options to configure security (TLS/SSL)
- Manages the upgrades and versioning of your Kafka clusters.

### 3. Helm Charts

Helm is a package manager for Kubernetes that allows you to define, install, and upgrade even the most complex Kubernetes applications. Helm charts provide a standardized way to package and deploy applications, including Kafka.

**Benefits of Using Helm:**

- **Simplified Deployments:** Deploy Kafka with a single command.
- **Configuration Management:** Easily configure Kafka using Helm values.
- **Version Control:** Track and manage different versions of your Kafka deployments.
- **Reusability:** Reuse Helm charts across different environments.

**Example: Using the Confluent Platform Helm Chart**

Confluent provides a Helm chart for deploying the entire Confluent Platform, including Kafka, Zookeeper, Schema Registry, and more.

1.  **Add the Confluent Helm Repository:**

    ```plaintext
    helm repo add confluentinc https://packages.confluent.io/helm
    helm repo update
    ```

2.  **Create a `values.yaml` file:**

    This file allows you to customize the Kafka deployment. Here's a basic example:

    ```plaintext
    kafka:
      enabled: true
      replicas: 3
      resources:
        requests:
          memory: 4Gi
          cpu: 2
      storage:
        class: 'standard' #  Replace with your storage class name
        size: 10Gi
    zookeeper:
      enabled: true
      replicas: 3
      resources:
        requests:
          memory: 2Gi
          cpu: 1
      storage:
        class: 'standard' # Replace with your storage class name
        size: 10Gi
    ```

3.  **Install the Confluent Platform:**

    ```plaintext
    helm install my-confluent confluentinc/cp-helm-charts -f values.yaml
    ```

Helm will then deploy the Confluent Platform, including Kafka and Zookeeper, according to the configuration in your `values.yaml` file.

**Advantages of using Helm**

- Provides a single interface for deploying the complete Confluent platform.
- Can easily override configurations using the `values.yaml` file.
- Manages the dependencies required to spin up Kafka.

### 4. Deploying Kafka without Zookeeper (KRaft mode)

Recent versions of Kafka introduced KRaft mode, which removes the dependency on Zookeeper. This simplifies the architecture and reduces operational overhead.

**Benefits of KRaft Mode:**

- **Simplified Architecture:** Eliminates the need for Zookeeper.
- **Improved Performance:** Reduces latency and improves throughput.
- **Easier Management:** Simplifies deployment and management.

**Deployment Steps:**

1. **Configure the Kafka Brokers for KRaft Mode**

Modify the Kafka broker configuration to enable KRaft mode. This typically involves setting the `process.roles` and `node.id` properties. For example:

```plaintext
# Configuration for the Kafka broker
process.roles=broker,controller
node.id=1 # must be unique across all Kafka brokers in the cluster
controller.quorum.voters=1@<controller-address>:9093 # address of the controller, or multiple controllers comma separated
```

2. **Deploy Controller Nodes**

Deploy dedicated controller nodes to manage the Kafka metadata. These nodes are responsible for leader election and cluster management. You can often co-locate the controller and broker roles on the same instances (as shown above) for smaller deployments.

3. **Deploy Kafka Brokers**

Deploy the Kafka brokers, ensuring that they are configured to connect to the controller nodes.

**Example: StatefulSet for Kafka with KRaft mode:**

```plaintext
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: kafka
spec:
  serviceName: 'kafka'
  replicas: 3
  selector:
    matchLabels:
      app: kafka
  template:
    metadata:
      labels:
        app: kafka
    spec:
      containers:
        - name: kafka
          image: confluentinc/cp-kafka:7.3.0 # Check if this version supports KRaft fully and officially. Consider using a newer version.
          ports:
            - containerPort: 9092
              name: client
            - containerPort: 9093
              name: interbroker
          env:
            - name: KAFKA_NODE_ID
              value: $(POD_NAME) # Unique ID for each broker
            - name: KAFKA_PROCESS_ROLES
              value: 'broker,controller' # enable broker and controller roles in one pod
            - name: KAFKA_CONTROLLER_QUORUM_VOTERS
              value: '$(POD_NAME)@kafka-$(POD_NAME).kafka.default.svc.cluster.local:9093' # Address of the controller
            - name: KAFKA_LISTENERS
              value: PLAINTEXT://:9092,BROKER://:9093,CONTROLLER://:9094
            - name: KAFKA_ADVERTISED_LISTENERS
              value: PLAINTEXT://kafka-$(POD_NAME).kafka.default.svc.cluster.local:9092,BROKER://kafka-$(POD_NAME).kafka.default.svc.cluster.local:9093
            - name: KAFKA_LISTENER_SECURITY_PROTOCOL_MAP
              value: PLAINTEXT:PLAINTEXT,BROKER:PLAINTEXT,CONTROLLER:PLAINTEXT
            - name: KAFKA_INTER_BROKER_LISTENER_NAME
              value: BROKER
            - name: KAFKA_CONTROLLER_LISTENER_NAMES
              value: CONTROLLER
            - name: KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR
              value: '1' # For testing.  Consider '3' for production
            - name: KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR
              value: '1' # For testing.  Consider '3' for production
            - name: KAFKA_TRANSACTION_STATE_LOG_MIN_ISR
              value: '1' # For testing. Consider '2' if replication factor is '3'
          volumeMounts:
            - name: data
              mountPath: /var/lib/kafka/data
  volumeClaimTemplates:
    - metadata:
        name: data
      spec:
        accessModes: ['ReadWriteOnce']
        resources:
          requests:
            storage: 10Gi

---
apiVersion: v1
kind: Service
metadata:
  name: kafka
spec:
  clusterIP: None
  selector:
    app: kafka
  ports:
    - protocol: TCP
      port: 9092
      name: client
    - protocol: TCP
      port: 9093
      name: interbroker
    - protocol: TCP
      port: 9094
      name: controller
```

**Important considerations for KRaft mode**

- Make sure you are using a version of Kafka that fully supports KRaft in production environments.
- Upgrade paths from Zookeeper-based Kafka clusters to KRaft mode can be complex. Plan carefully.
- Monitor the controller nodes closely. Their health is crucial to the overall cluster health.

### Choosing the Right Deployment Pattern

The best deployment pattern for your Kafka on Kubernetes depends on your specific requirements and expertise.

- **StatefulSets:** A good starting point for simple deployments where you want more control over the configuration.
- **Kafka Operators:** Ideal for complex deployments that require automation and simplified management.
- **Helm Charts:** Suitable for deploying the entire Confluent Platform or for quickly deploying pre-configured Kafka clusters.
- **KRaft Mode:** A compelling option for new deployments, simplifying the architecture and potentially improving performance by removing Zookeeper dependency. Thoroughly test the chosen Kafka version before adopting KRaft in production.

## Best Practices for Running Kafka on Kubernetes

Regardless of the deployment pattern you choose, follow these best practices to ensure a successful Kafka deployment on Kubernetes:

- **Use Persistent Volumes (PVs):** Always use PVs for Kafka data to ensure data persistence.
- **Configure Resource Limits and Requests:** Set appropriate resource limits and requests for Kafka brokers to prevent resource contention.
- **Monitor Your Cluster:** Implement comprehensive monitoring of your Kafka cluster using Prometheus and Grafana. Monitor metrics such as CPU usage, memory usage, disk I/O, and network traffic.
- **Implement Data Replication:** Configure Kafka with a replication factor of at least 3 to ensure data durability and fault tolerance. Adjust the `min.insync.replicas` accordingly to prevent data loss.
- **Secure Your Cluster:** Implement security measures such as TLS/SSL encryption, authentication, and authorization.
- **Tune Kafka Configuration:** Tune Kafka configuration parameters, such as `num.partitions`, `default.replication.factor`, `message.max.bytes`, and `replica.fetch.max.bytes`, to optimize performance.
- **Use a Dedicated Namespace:** Deploy Kafka in a dedicated Kubernetes namespace to isolate it from other applications.
- **Automate Deployments and Upgrades:** Use tools like Helm or operators to automate deployments and upgrades, minimizing downtime and reducing errors.
- **Regularly Back Up Your Data:** Implement a backup strategy to protect your Kafka data from accidental deletion or corruption.
- **Test Your Disaster Recovery Plan:** Regularly test your disaster recovery plan to ensure that you can recover your Kafka cluster in the event of a failure.
- **Network Policies:** Implement Kubernetes network policies to restrict network traffic to and from your Kafka cluster, enhancing security. Only allow necessary traffic to the Kafka brokers and Zookeeper (if applicable).
- **Consider Local Storage:** For maximum performance, especially in high-throughput scenarios, consider using local storage with a proper data replication strategy. Local storage can provide lower latency and higher throughput than network-attached storage. However, it requires careful planning and management to ensure data durability.

## Monitoring Kafka on Kubernetes

Effective monitoring is crucial for maintaining a healthy and performant Kafka cluster on Kubernetes. Here are some key metrics to monitor:

- **Broker Metrics:**
  - **CPU Usage:** Monitor CPU utilization to identify bottlenecks.
  - **Memory Usage:** Track memory usage to prevent out-of-memory errors.
  - **Disk I/O:** Monitor disk I/O to identify storage performance issues.
  - **Network Traffic:** Track network traffic to identify network bottlenecks.
  - **Under-Replicated Partitions:** Monitor the number of under-replicated partitions to ensure data durability.
  - **Offline Partitions:** Monitor the number of offline partitions to identify potential data loss.
  - **Active Controller Count:** Ensure only one controller is active. Monitor controller election times.
  - **Request Latency:** Monitor the latency of producer and consumer requests.
- **Zookeeper Metrics (if applicable):**
  - **Latency:** Monitor Zookeeper latency.
  - **Connections:** Track the number of Zookeeper connections.
  - **Outstanding Requests:** Monitor the number of outstanding Zookeeper requests.
- **Operating System Metrics:** Monitor CPU, memory, disk I/O, and network traffic at the operating system level.

**Tools for Monitoring:**

- **Prometheus:** A popular open-source monitoring and alerting toolkit.
- **Grafana:** A data visualization and dashboarding tool that integrates with Prometheus.
- **Kafka Exporter:** Exports Kafka metrics to Prometheus.
- **JMX Exporter:** Exports metrics from Java Management Extensions (JMX) to Prometheus.

**Example: Configuring Prometheus to Monitor Kafka**

1.  **Install Prometheus:**

    You can install Prometheus using Helm or other methods.

2.  **Install Kafka Exporter:**

    The Kafka Exporter exposes Kafka metrics in a format that Prometheus can understand. You can deploy it as a Kubernetes deployment.

    ```plaintext
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: kafka-exporter
      labels:
        app: kafka-exporter
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: kafka-exporter
      template:
        metadata:
          labels:
            app: kafka-exporter
        spec:
          containers:
            - name: kafka-exporter
              image: danielqsj/kafka-exporter:latest # Use a specific, stable version
              args:
                - --kafka.server=kafka-0.kafka.default.svc.cluster.local:9092,kafka-1.kafka.default.svc.cluster.local:9092,kafka-2.kafka.default.svc.cluster.local:9092 # Adjust based on your Kafka brokers
              ports:
                - containerPort: 9308
                  name: metrics

    ---
    apiVersion: v1
    kind: Service
    metadata:
      name: kafka-exporter
    spec:
      selector:
        app: kafka-exporter
      ports:
        - protocol: TCP
          port: 9308
          targetPort: metrics
          name: metrics
    ```

3.  **Configure Prometheus to Scrape Metrics:**

    Add a scrape configuration to your Prometheus configuration file to scrape metrics from the Kafka Exporter.

    ```plaintext
    scrape_configs:
      - job_name: 'kafka'
        static_configs:
          - targets: ['kafka-exporter:9308']
    ```

4.  **Create Grafana Dashboards:**

    Import pre-built Grafana dashboards for Kafka or create your own custom dashboards to visualize the metrics.

## Conclusion

Deploying Kafka on Kubernetes offers significant advantages in terms of scalability, resilience, and manageability. By understanding the different deployment patterns, best practices, and monitoring techniques, you can successfully run Kafka clusters on Kubernetes and leverage its power for your event streaming needs. Remember to carefully evaluate your requirements and choose the deployment pattern that best fits your specific scenario. The move towards KRaft mode represents a significant simplification in Kafka architecture and deserves serious consideration for new deployments.
