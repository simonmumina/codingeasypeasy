---
title: 'Streaming LLM Responses with Server-Sent Events (SSE) and FastAPI: A Comprehensive Guide'
date: '2024-10-26'
lastmod: '2024-10-26'
tags:
  [
    'fastapi',
    'llm',
    'openai',
    'server-sent events',
    'streaming',
    'python',
    'real-time',
    'sse',
    'api',
  ]
draft: false
summary: 'Learn how to build a real-time streaming API with FastAPI using Server-Sent Events (SSE) to efficiently stream LLM responses, like those from OpenAI. This guide provides detailed code examples and explanations.'
authors: ['default']
---

# Streaming LLM Responses with Server-Sent Events (SSE) and FastAPI: A Comprehensive Guide

Large Language Models (LLMs) like those provided by OpenAI are revolutionizing the way we interact with machines. However, waiting for an entire LLM response before displaying it to the user can lead to a poor user experience. Streaming responses allows your users to see the output in real-time, improving perceived responsiveness and overall satisfaction. This post will guide you through implementing LLM response streaming using Server-Sent Events (SSE) with FastAPI, a modern, fast (high-performance), web framework for building APIs with Python 3.7+ based on standard Python type hints.

## Why Stream LLM Responses?

- **Improved User Experience:** Real-time feedback keeps users engaged and provides a sense of immediacy.
- **Reduced Perceived Latency:** Even though the total response time might be the same, displaying content incrementally makes the process feel faster.
- **Resource Efficiency:** In some cases, you can process parts of the response as they arrive, rather than waiting for the whole thing.
- **Handles Long Responses Better:** Streaming is crucial when dealing with potentially long LLM outputs.

## What are Server-Sent Events (SSE)?

Server-Sent Events (SSE) are a web standard that allows a server to push updates to a client over a single HTTP connection. Unlike WebSockets, SSE is unidirectional (server to client), making it simpler to implement when you only need the server to send data. This makes it a perfect fit for streaming LLM responses. Key characteristics:

- **Unidirectional:** Server pushes data to the client.
- **HTTP-based:** Uses standard HTTP protocol, making it firewall-friendly.
- **Text-based:** Data is transmitted as UTF-8 encoded text.
- **Retry Mechanism:** Client automatically reconnects if the connection is lost.

## Prerequisites

Before you start, make sure you have the following installed:

- **Python 3.7+:** FastAPI requires Python 3.7 or higher.
- **FastAPI:** `pip install fastapi`
- **Uvicorn:** An ASGI server to run FastAPI: `pip install uvicorn`
- **OpenAI Python Library:** `pip install openai`
- **dotenv (Optional):** For managing environment variables. `pip install python-dotenv`

## Setting up Your FastAPI Application

First, let's create a basic FastAPI application:

```plaintext
# main.py
from fastapi import FastAPI, HTTPException, StreamingResponse
from pydantic import BaseModel
import openai
import os
from dotenv import load_dotenv

load_dotenv()  # Load environment variables from .env file

app = FastAPI()

# Configure OpenAI API Key
openai.api_key = os.getenv("OPENAI_API_KEY")

if not openai.api_key:
    raise ValueError("OpenAI API key not found.  Please set the OPENAI_API_KEY environment variable.")


class PromptRequest(BaseModel):
    prompt: str
    model: str = "gpt-3.5-turbo" # Default to gpt-3.5-turbo
    temperature: float = 0.7


```

**Explanation:**

1.  **Imports:** We import necessary modules from FastAPI, Pydantic (for data validation), OpenAI, and the `os` and `dotenv` libraries.
2.  **Environment Variables:** We load the OpenAI API key from the environment variables using `dotenv`. This is crucial for security â€“ avoid hardcoding your API keys! Create a `.env` file in your project directory with the following content:

    ```
    OPENAI_API_KEY=YOUR_OPENAI_API_KEY
    ```

    Replace `YOUR_OPENAI_API_KEY` with your actual OpenAI API key.

3.  **FastAPI Instance:** We create an instance of the `FastAPI` class.
4.  **OpenAI API Key Configuration:** We set the `openai.api_key` with the value from the environment variable. An error will be raised if the key is missing.
5.  **`PromptRequest` Pydantic Model:** Defines the structure of the request body for our endpoint. It includes a `prompt` (string) and the `model` to use (defaults to `"gpt-3.5-turbo"`).

## Implementing SSE Streaming with FastAPI

Now, let's implement the endpoint that streams the LLM response using SSE:

```plaintext
# main.py (continued)

async def generate_stream(prompt: str, model: str, temperature: float):
    try:
        response = openai.ChatCompletion.create(
            model=model,
            messages=[{"role": "user", "content": prompt}],
            stream=True,
            temperature=temperature
        )

        async for chunk in response:
            if chunk and 'choices' in chunk and len(chunk['choices']) > 0 and 'delta' in chunk['choices'][0] and 'content' in chunk['choices'][0]['delta']:
                chunk_content = chunk['choices'][0]['delta'].get('content', '')
                yield f"data: {chunk_content}\n\n"  # SSE format

    except Exception as e:
        yield f"data: Error: {str(e)}\n\n"


@app.post("/stream")
async def stream_endpoint(request: PromptRequest):
    return StreamingResponse(generate_stream(request.prompt, request.model, request.temperature), media_type="text/event-stream")
```

**Explanation:**

1.  **`generate_stream` Function:** This asynchronous function is the heart of our streaming implementation.
    - It takes the `prompt`, `model`, and `temperature` as input.
    - It uses `openai.ChatCompletion.create` with `stream=True` to get a streaming response from OpenAI.
    - It iterates through the `response` (which is an async generator).
    - For each `chunk` received, it extracts the content using `chunk['choices'][0]['delta'].get('content', '')`.
    - It formats the content as a Server-Sent Event using `f"data: {chunk_content}\n\n"` and yields it. The double newline `\n\n` is crucial for SSE to recognize the end of an event.
    - A try-except block handles any errors that might occur during the OpenAI API call, yielding an error message to the client.
2.  **`/stream` Endpoint:** This endpoint handles POST requests to `/stream`.
    - It uses `StreamingResponse` from FastAPI to stream the output of the `generate_stream` function.
    - `media_type="text/event-stream"` is essential to tell the client that the response is in SSE format.

## Running the Application

Save the code as `main.py` and run the application using Uvicorn:

```plaintext
uvicorn main:app --reload
```

The `--reload` flag enables automatic reloading of the server when you make changes to the code.

## Testing the Streaming Endpoint

You can test the streaming endpoint using `curl`, JavaScript, or a dedicated SSE client.

**1. Using `curl`:**

```plaintext
curl -X POST -H "Content-Type: application/json" -d '{"prompt": "Write a short poem about the ocean."}' http://localhost:8000/stream
```

You will see the poem printed to your terminal in real-time, chunk by chunk.

**2. Using JavaScript (Browser):**

```plaintext
<!DOCTYPE html>
<html>
  <head>
    <title>SSE Example</title>
  </head>
  <body>
    <h1>Streaming LLM Response</h1>
    <div id="output"></div>

    <script>
      const eventSource = new EventSource('http://localhost:8000/stream', {
        withCredentials: false,
      }) // Set withCredentials to false

      eventSource.onmessage = function (event) {
        const outputDiv = document.getElementById('output')
        outputDiv.innerHTML += event.data
      }

      eventSource.onerror = function (error) {
        console.error('SSE error:', error)
        eventSource.close()
      }

      fetch('http://localhost:8000/stream', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({ prompt: 'Write a short story about a robot learning to love.' }),
      }).catch((error) => {
        console.error('Fetch error:', error)
      })
    </script>
  </body>
</html>
```

**Explanation of the JavaScript code:**

- **`new EventSource()`:** Creates a new EventSource object, connecting to the `/stream` endpoint.
- **`eventSource.onmessage`:** This function is called whenever the server sends a new event. It appends the event data to the `output` div.
- **`eventSource.onerror`:** Handles any errors that occur with the SSE connection.
- **`fetch()`:** Sends a POST request to the `/stream` endpoint with the prompt in the body.
- **`withCredentials: false`:** Important. Some browsers, especially when using localhost, might have CORS issues. Setting `withCredentials` to `false` can often resolve these. In a production environment, you'll need proper CORS configuration on your FastAPI server.

**Important: CORS Configuration**

In a real-world application, you'll need to configure CORS (Cross-Origin Resource Sharing) in your FastAPI application to allow requests from your client-side domain. You can do this using the `fastapi.middleware.cors` module:

```plaintext
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware

app = FastAPI()

origins = [
    "http://localhost",  # Allow requests from localhost
    "http://localhost:8080", #  Allow requests from port 8080
    "http://your-frontend-domain.com", # Add your front end domain
    "https://your-frontend-domain.com",
]

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ... rest of your code
```

Replace `"http://your-frontend-domain.com"` with the actual domain of your frontend application. Remember to adjust the `origins` list based on your specific needs.

## Advanced Considerations

- **Error Handling:** Improve error handling to provide more informative messages to the client. You could define custom error types and corresponding SSE messages.
- **Client-Side Buffering:** On the client-side, consider buffering the incoming chunks before displaying them to the user. This can help smooth out the output and prevent flickering.
- **Rate Limiting:** Implement rate limiting to protect your API from abuse. FastAPI provides middleware for this purpose.
- **Token Usage:** Consider tracking token usage and providing this information to the client, especially if you're charging users based on token consumption. You can modify the `generate_stream` function to parse token information from the OpenAI response metadata and include it in the SSE events.
- **Choice of LLM:** The above example defaults to `gpt-3.5-turbo`. Consider making the choice of model configurable.
- **Context Management:** For more complex conversations, you'll need to manage conversation history (context) and send it to the LLM with each request. This requires storing and retrieving the conversation history, and incorporating it into the `messages` array in the `openai.ChatCompletion.create` call.
- **Security:** Always validate user input (the `prompt`) to prevent prompt injection attacks. Consider using techniques like prompt engineering and input sanitization to mitigate these risks.
- **Infrastructure:** For production deployments, consider using a more robust ASGI server like Gunicorn or Hypercorn, and deploying your application behind a load balancer.

## Complete Code (main.py)

```plaintext
from fastapi import FastAPI, HTTPException, StreamingResponse
from pydantic import BaseModel
import openai
import os
from dotenv import load_dotenv
from fastapi.middleware.cors import CORSMiddleware

load_dotenv()

app = FastAPI()

origins = [
    "http://localhost",
    "http://localhost:8080",
    "http://localhost:3000",
]

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

openai.api_key = os.getenv("OPENAI_API_KEY")

if not openai.api_key:
    raise ValueError("OpenAI API key not found.  Please set the OPENAI_API_KEY environment variable.")


class PromptRequest(BaseModel):
    prompt: str
    model: str = "gpt-3.5-turbo"
    temperature: float = 0.7


async def generate_stream(prompt: str, model: str, temperature: float):
    try:
        response = openai.ChatCompletion.create(
            model=model,
            messages=[{"role": "user", "content": prompt}],
            stream=True,
            temperature=temperature
        )

        async for chunk in response:
            if chunk and 'choices' in chunk and len(chunk['choices']) > 0 and 'delta' in chunk['choices'][0] and 'content' in chunk['choices'][0]['delta']:
                chunk_content = chunk['choices'][0]['delta'].get('content', '')
                yield f"data: {chunk_content}\n\n"

    except Exception as e:
        yield f"data: Error: {str(e)}\n\n"


@app.post("/stream")
async def stream_endpoint(request: PromptRequest):
    return StreamingResponse(generate_stream(request.prompt, request.model, request.temperature), media_type="text/event-stream")
```

## Conclusion

This guide demonstrated how to stream LLM responses using Server-Sent Events and FastAPI. Streaming significantly enhances the user experience by providing real-time feedback and reducing perceived latency. Remember to consider security, error handling, and CORS configuration for production deployments. With this knowledge, you can build more responsive and engaging applications powered by LLMs.
