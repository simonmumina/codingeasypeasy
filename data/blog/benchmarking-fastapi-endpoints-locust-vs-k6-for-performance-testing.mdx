---
title: 'Benchmarking FastAPI Endpoints: Locust vs. k6 for Performance Testing'
date: '2024-01-01'
lastmod: '2024-01-01'
tags:
  [
    'fastapi',
    'benchmarking',
    'performance testing',
    'locust',
    'k6',
    'python',
    'api',
    'load testing',
  ]
draft: false
summary: 'Learn how to benchmark FastAPI endpoints effectively using Locust and k6. Compare the tools, understand their features, and see practical code examples for load testing your APIs.'
authors: ['default']
---

# Benchmarking FastAPI Endpoints: Locust vs. k6 for Performance Testing

Ensuring your FastAPI application can handle the load is crucial for delivering a smooth and reliable user experience. Performance testing, also known as load testing or benchmarking, helps you identify bottlenecks and optimize your code before it's deployed to production. This post delves into how to benchmark FastAPI endpoints using two popular tools: Locust and k6. We'll explore their strengths, weaknesses, and provide practical code examples to get you started.

## Why Benchmark FastAPI?

Before we dive into the tools, let's solidify why benchmarking is so important:

- **Identify Performance Bottlenecks:** Load testing reveals the weak points in your application, such as slow database queries, inefficient algorithms, or resource limitations.
- **Ensure Scalability:** Determine how many concurrent users your application can handle before performance degrades. This helps you plan for future growth and allocate resources accordingly.
- **Optimize Code:** Benchmarking data allows you to pinpoint areas that need optimization, leading to faster response times and reduced server load.
- **Validate Infrastructure:** Verify that your servers, databases, and network infrastructure can handle the expected load.
- **Improve User Experience:** A responsive application leads to happier users and increased engagement.

## Introducing Locust and k6

Locust and k6 are powerful open-source tools designed for load testing and performance monitoring. While both achieve the same goal, they differ in their approach and feature set.

### Locust: User-Centric Load Testing with Python

Locust allows you to define user behavior using Python code. It simulates the actions of multiple concurrent users, allowing you to measure the performance of your application under realistic conditions.

**Key Features of Locust:**

- **Python-Based:** Easy to learn and use for Python developers.
- **User-Centric:** Models user behavior with tasks.
- **Web-Based UI:** Provides a real-time dashboard for monitoring performance.
- **Distributed Testing:** Can be run on multiple machines to simulate very high loads.
- **Extensible:** Supports custom metrics and reporting.

### k6: Go-Based Load Testing with Scriptable Workloads

k6 is a modern load testing tool built in Go, known for its performance and efficiency. It uses JavaScript (specifically ECMAScript 2015/ES6) to define test scenarios and provides powerful features for scripting and analysis.

**Key Features of k6:**

- **Go-Based:** Highly performant and resource-efficient.
- **JavaScript Scripting:** Uses JavaScript for defining tests, making it accessible to web developers.
- **CLI-Based:** Primarily controlled through the command line.
- **Cloud Support:** Integrates with various cloud platforms for distributed testing and reporting.
- **Metrics and Monitoring:** Offers detailed metrics and integrates with monitoring tools like Prometheus and Grafana.

## Setting up FastAPI for Benchmarking

First, let's create a simple FastAPI application that we can benchmark. Save the following code as `main.py`:

```plaintext
from fastapi import FastAPI
import time
import random

app = FastAPI()

@app.get("/")
async def read_root():
    return {"Hello": "World"}

@app.get("/items/{item_id}")
async def read_item(item_id: int):
    # Simulate some processing time
    time.sleep(random.uniform(0.01, 0.1))
    return {"item_id": item_id, "message": "This is an item"}

@app.get("/slow_endpoint")
async def slow_endpoint():
    time.sleep(0.5) # Simulate a slow operation
    return {"message": "This endpoint is deliberately slow"}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

This application defines three endpoints:

- `/`: A simple "Hello World" endpoint.
- `/items/{item_id}`: An endpoint that simulates some processing time using `time.sleep()`.
- `/slow_endpoint`: An endpoint that deliberately takes 0.5 seconds to respond.

Now, run the FastAPI application:

```plaintext
python main.py
```

## Benchmarking with Locust

### Installing Locust

Install Locust using pip:

```plaintext
pip install locust
```

### Creating a Locustfile

Create a file named `locustfile.py` with the following content:

```plaintext
from locust import HttpUser, task, between
import random

class QuickstartUser(HttpUser):
    wait_time = between(1, 2)

    @task(2)
    def index_page(self):
        self.client.get("/")

    @task(1)
    def view_item(self):
        item_id = random.randint(1, 100)
        self.client.get(f"/items/{item_id}")

    @task(1)
    def view_slow(self):
        self.client.get("/slow_endpoint")
```

This `locustfile.py` defines a `QuickstartUser` class that simulates a user interacting with the FastAPI application. Here's a breakdown:

- `HttpUser`: Base class for defining users that interact with HTTP endpoints.
- `wait_time = between(1, 2)`: Users will wait between 1 and 2 seconds between tasks.
- `@task`: Decorator that defines a task that a user can perform. The number in parentheses determines the weight of the task (higher number = more frequent).
- `self.client.get()`: Makes a GET request to the specified endpoint.

### Running Locust

Run Locust from your terminal:

```plaintext
locust -f locustfile.py --host=http://localhost:8000
```

This command starts Locust and points it to your FastAPI application running at `http://localhost:8000`. Open your web browser and navigate to `http://localhost:8089` to access the Locust web UI.

In the Locust UI, you can configure:

- **Number of users to simulate:** The total number of concurrent users.
- **Hatch rate:** The rate at which new users are spawned.

Click "Start swarming" to begin the load test. The Locust UI will display real-time statistics, including:

- **Requests per second (RPS):** The number of requests your application is handling per second.
- **Response times:** The average, minimum, and maximum response times for each endpoint.
- **Failures:** The number of requests that failed.

### Analyzing Locust Results

Locust provides a wealth of information to help you identify performance bottlenecks. Pay attention to the following:

- **Response times:** High response times indicate that your application is struggling to handle the load.
- **Failure rate:** A high failure rate indicates that your application is crashing or timing out.
- **RPS:** A decrease in RPS as the number of users increases suggests that your application is reaching its limit.

## Benchmarking with k6

### Installing k6

Install k6 using your system's package manager. Refer to the official k6 documentation for installation instructions specific to your operating system: [https://k6.io/docs/getting-started/installation/](https://k6.io/docs/getting-started/installation/)

For example, on macOS with Homebrew:

```plaintext
brew install k6
```

### Creating a k6 Script

Create a file named `script.js` with the following content:

```plaintext
import http from 'k6/http'
import { sleep } from 'k6'
import { randomIntBetween } from 'https://jslib.k6.io/k6-utils/1.4.0/index.js'

export const options = {
  vus: 10, // Virtual Users
  duration: '10s', // Test duration
}

export default function () {
  let itemId = randomIntBetween(1, 100)
  http.get('http://localhost:8000/')
  http.get(`http://localhost:8000/items/${itemId}`)
  http.get('http://localhost:8000/slow_endpoint')
  sleep(1)
}
```

This script defines a load test that simulates 10 virtual users (VUs) making requests to your FastAPI application for 10 seconds. Here's a breakdown:

- `import http from 'k6/http';`: Imports the HTTP module for making HTTP requests.
- `import { sleep } from 'k6';`: Imports the `sleep` function for pausing execution.
- `export const options = { ... };`: Defines the test configuration options.
  - `vus`: Number of virtual users to simulate.
  - `duration`: Duration of the test.
- `export default function () { ... };`: Defines the main test function.
  - `http.get()`: Makes a GET request to the specified endpoint.
  - `sleep()`: Pauses execution for the specified duration.

### Running k6

Run the k6 script from your terminal:

```plaintext
k6 run script.js
```

This command executes the `script.js` load test. k6 will output detailed performance metrics in the console, including:

- **Requests per second (RPS):** The number of requests your application is handling per second.
- **Response times:** The average, minimum, and maximum response times for each endpoint.
- **Data received/sent:** The amount of data transferred.
- **Error rate:** The percentage of requests that failed.

### Analyzing k6 Results

k6 provides comprehensive metrics to help you analyze your application's performance. Pay attention to the following:

- **`http_req_duration`:** This metric represents the total time taken for each HTTP request. Pay close attention to the average, minimum, and maximum values. High values indicate slow response times.
- **`http_req_failed`:** This metric indicates the percentage of requests that failed. A high failure rate indicates that your application is unstable or overloaded.
- **`vus`:** This metric shows the number of virtual users that were active during the test. If the number of active VUs drops significantly, it may indicate that your application is unable to handle the load.
- **`iteration_duration`:** This metric represents the total time taken for each iteration of the test script. A high iteration duration indicates that the test script is taking too long to execute, which may affect the accuracy of the results.

## Locust vs. k6: A Comparison

| Feature        | Locust                               | k6                                                   |
| -------------- | ------------------------------------ | ---------------------------------------------------- |
| Language       | Python                               | JavaScript (ES6)                                     |
| Architecture   | Event-driven, coroutine-based        | Go-based                                             |
| UI             | Web-based UI                         | CLI-based                                            |
| Scalability    | Good (distributed testing supported) | Excellent (highly performant)                        |
| Learning Curve | Relatively easy                      | Relatively easy                                      |
| Reporting      | Basic reporting in Web UI            | Detailed metrics, integrations with monitoring tools |
| Community      | Large and active                     | Growing                                              |

**When to choose Locust:**

- You prefer Python and have existing Python code.
- You want a user-friendly web interface for monitoring.
- You need to simulate complex user behavior.

**When to choose k6:**

- You need a highly performant and resource-efficient tool.
- You prefer JavaScript for scripting.
- You need detailed metrics and integration with monitoring tools.
- You plan to use cloud-based load testing services.

## Best Practices for Benchmarking FastAPI

- **Define Clear Goals:** Before you start benchmarking, define what you want to achieve. What performance metrics are important to you? What are your target response times?
- **Simulate Realistic Workloads:** Model your test scenarios after real-world user behavior. Consider different types of users and their access patterns.
- **Monitor System Resources:** Monitor CPU usage, memory usage, and disk I/O during the tests to identify resource bottlenecks. Use tools like `top`, `htop`, or monitoring services like Prometheus and Grafana.
- **Isolate the Environment:** Run your load tests in a dedicated environment that is isolated from other workloads. This will ensure that the results are accurate and reliable.
- **Start Small and Scale Up:** Start with a small number of users and gradually increase the load until you identify performance issues.
- **Automate Your Tests:** Automate your load tests and integrate them into your CI/CD pipeline. This will allow you to catch performance regressions early in the development process.
- **Analyze and Iterate:** Analyze the results of your load tests and identify areas that need optimization. Make changes to your code and repeat the tests until you achieve your performance goals.

## Conclusion

Benchmarking your FastAPI endpoints is an essential step in ensuring the performance and scalability of your application. Locust and k6 are both excellent tools for this purpose, offering different strengths and features. Choose the tool that best suits your needs and technical expertise. By following the best practices outlined in this post, you can effectively identify and address performance bottlenecks, ensuring a smooth and responsive user experience for your FastAPI applications.
