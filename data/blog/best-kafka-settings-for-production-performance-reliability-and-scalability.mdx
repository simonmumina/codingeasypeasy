---
title: 'Best Kafka Settings for Production: Performance, Reliability & Scalability'
date: '2024-10-27'
lastmod: '2024-10-27'
tags:
  [
    'kafka',
    'apache kafka',
    'production settings',
    'performance tuning',
    'scalability',
    'reliability',
    'configuration',
    'broker',
    'producer',
    'consumer',
    'kafka connect',
    'kafka streams',
  ]
draft: false
summary: 'Optimize your Kafka deployment for production environments. Learn the best Kafka settings for performance, reliability, and scalability, covering broker configuration, producer settings, consumer tuning, and important considerations for Kafka Connect and Kafka Streams.'
authors: ['default']
---

# Best Kafka Settings for Production: Performance, Reliability & Scalability

Apache Kafka is a powerful distributed streaming platform widely used for building real-time data pipelines and streaming applications. However, simply deploying Kafka with default settings in production is a recipe for disaster. Proper configuration is crucial for ensuring performance, reliability, and scalability. This article provides a comprehensive guide to the best Kafka settings for production environments, covering broker configuration, producer settings, consumer tuning, and important considerations for Kafka Connect and Kafka Streams.

## Understanding the Importance of Optimal Kafka Settings

Kafka's default settings are often geared towards development or testing environments. In production, you need to fine-tune various parameters to:

- **Maximize Throughput:** Handle high volumes of data with minimal latency.
- **Ensure Reliability:** Prevent data loss and maintain availability during failures.
- **Scale Efficiently:** Accommodate growing data volumes and user demands.
- **Optimize Resource Utilization:** Use hardware resources effectively.
- **Maintain Stability:** Prevent performance bottlenecks and ensure long-term operational stability.

## Kafka Broker Configuration: The Foundation of Your Cluster

The Kafka broker configuration file (`server.properties`) is the core of your Kafka cluster's setup. Here are the key settings you need to understand and configure:

### 1. `broker.id`

- **Description:** A unique numerical ID for each broker in the cluster.
- **Importance:** Crucial for identifying brokers and preventing conflicts.
- **Recommendation:** Assign unique, persistent IDs to each broker. Avoid reusing IDs, even after a broker is decommissioned.

### 2. `listeners` and `advertised.listeners`

- **Description:** `listeners` defines the network interfaces the broker listens on for client connections. `advertised.listeners` specifies the addresses clients should use to connect to the broker.
- **Importance:** Critical for network connectivity and security.
- **Recommendation:**

  - **`listeners`:** Bind to specific interfaces (e.g., `PLAINTEXT://0.0.0.0:9092`) rather than all interfaces.
  - **`advertised.listeners`:** Should be the externally accessible addresses of your brokers. Use DNS names or stable IPs. For example: `PLAINTEXT://kafka1.example.com:9092,PLAINTEXT://kafka2.example.com:9092`.
  - **Security:** Consider using `SSL` or `SASL_SSL` listeners for encryption and authentication. Configure the appropriate security protocols and certificates. Example with SSL: `SSL://kafka1.example.com:9093`.

### 3. `num.network.threads` and `num.io.threads`

- **Description:** The number of threads used for handling network requests and disk I/O, respectively.
- **Importance:** Impacts throughput and latency.
- **Recommendation:** Start with the number of cores on your server for `num.io.threads`. Experiment with different values to find the optimal setting for your workload. `num.network.threads` can generally be smaller (e.g., 3-4) unless you have a very high connection rate.

### 4. `log.dirs` and `log.segment.bytes`

- **Description:** `log.dirs` specifies the directories where Kafka stores its data logs. `log.segment.bytes` defines the maximum size of each log segment.
- **Importance:** Impacts disk space utilization and I/O performance.
- **Recommendation:**

  - **`log.dirs`:** Use multiple disks or RAID arrays for better I/O performance. Separate Kafka logs from the OS disk. Example: `log.dirs=/data/kafka/log1,/data/kafka/log2`.
  - **`log.segment.bytes`:** Increase this value (e.g., 1GB) for higher throughput with fewer segments. However, very large segments can increase the time it takes to roll over segments. A typical value is `1073741824` (1 GB).

### 5. `log.retention.bytes` and `log.retention.ms`

- **Description:** `log.retention.bytes` limits the total size of the log for each topic partition. `log.retention.ms` specifies the maximum time a log message is retained.
- **Importance:** Controls disk space usage and data availability.
- **Recommendation:** Configure retention policies based on your business requirements. Prioritize `log.retention.ms` to avoid running out of disk space if your data rate is higher than expected. Examples: `log.retention.ms=604800000` (7 days), `log.retention.bytes=107374182400` (100 GB).

### 6. `default.replication.factor` and `min.insync.replicas`

- **Description:** `default.replication.factor` specifies the number of replicas for each topic. `min.insync.replicas` dictates the minimum number of replicas that must be in sync before a producer can consider a write successful.
- **Importance:** Critical for fault tolerance and data durability.
- **Recommendation:**

  - **`default.replication.factor`:** Use a replication factor of 3 for high availability.
  - **`min.insync.replicas`:** Set this to 2 (one less than the replication factor) to prevent data loss in case of a broker failure.
  - **Topic Level Overrides:** You can override these settings at the topic level to customize replication based on the importance of the data.

### 7. `zookeeper.connect`

- **Description:** The connection string for the ZooKeeper ensemble used by Kafka.
- **Importance:** Essential for cluster coordination and metadata management.
- **Recommendation:** Use a dedicated ZooKeeper ensemble for Kafka. Provide a comma-separated list of ZooKeeper servers. For example: `zookeeper.connect=zk1.example.com:2181,zk2.example.com:2181,zk3.example.com:2181`.

### 8. `unclean.leader.election.enable`

- **Description:** Determines whether a non-in-sync replica can become the leader of a partition.
- **Importance:** Impacts data consistency.
- **Recommendation:** Set this to `false` in production environments to prevent potential data loss. Enabling it can lead to inconsistencies if the previous leader that went down comes back online.

### 9. `auto.create.topics.enable`

- **Description:** Enables automatic topic creation when a producer or consumer attempts to access a non-existent topic.
- **Importance:** Affects cluster management and security.
- **Recommendation:** Set this to `false` in production to prevent accidental or malicious topic creation. Manage topics explicitly through the Kafka command-line tools or Kafka Admin API.

### Code Example (Broker Configuration Snippet):

```properties
broker.id=1
listeners=PLAINTEXT://0.0.0.0:9092,SSL://0.0.0.0:9093
advertised.listeners=PLAINTEXT://kafka1.example.com:9092,SSL://kafka1.example.com:9093
num.network.threads=3
num.io.threads=8
log.dirs=/data/kafka/log1,/data/kafka/log2
log.segment.bytes=1073741824
log.retention.ms=604800000
default.replication.factor=3
min.insync.replicas=2
zookeeper.connect=zk1.example.com:2181,zk2.example.com:2181,zk3.example.com:2181
unclean.leader.election.enable=false
auto.create.topics.enable=false
```

## Producer Configuration: Optimizing Data Delivery

The Kafka producer is responsible for sending data to the Kafka cluster. Proper producer configuration can significantly impact throughput and latency.

### 1. `bootstrap.servers`

- **Description:** A comma-separated list of Kafka brokers to initially connect to.
- **Importance:** Essential for producer to discover the Kafka cluster.
- **Recommendation:** Provide a list of at least two or three brokers to ensure that the producer can connect even if one broker is unavailable. Example: `bootstrap.servers=kafka1.example.com:9092,kafka2.example.com:9092,kafka3.example.com:9092`

### 2. `acks`

- **Description:** Specifies the number of acknowledgments the producer requires from the Kafka brokers before considering a write successful.
- **Importance:** Controls data durability.
- **Recommendation:**

  - **`acks=0`:** The producer does not wait for any acknowledgments. Fastest, but least reliable. Data loss is possible.
  - **`acks=1`:** The producer waits for acknowledgment from the leader replica. Good balance of performance and reliability. Data loss is possible if the leader fails before the data is replicated.
  - **`acks=all` (or `-1`):** The producer waits for acknowledgment from all in-sync replicas (defined by `min.insync.replicas`). Most reliable, but slowest. Guarantees no data loss as long as `min.insync.replicas` is properly configured.

  Choose `acks=all` for critical data where data loss is unacceptable.

### 3. `retries` and `retry.backoff.ms`

- **Description:** `retries` specifies the number of times the producer will retry sending a message if it fails. `retry.backoff.ms` controls the delay between retries.
- **Importance:** Enhances reliability by automatically recovering from transient errors.
- **Recommendation:** Set `retries` to a reasonable value (e.g., 3-5) to handle temporary network issues or broker failures. Use a small `retry.backoff.ms` value (e.g., 100ms) to avoid excessive delays.

### 4. `batch.size` and `linger.ms`

- **Description:** `batch.size` defines the maximum size of a batch of messages the producer will attempt to send in a single request. `linger.ms` specifies the amount of time the producer will wait to accumulate more messages before sending a batch.
- **Importance:** Optimizes throughput by reducing the number of network requests.
- **Recommendation:** Increase `batch.size` (e.g., 16KB or 32KB) and `linger.ms` (e.g., 5-10ms) to improve throughput. However, be mindful of the impact on latency.

### 5. `compression.type`

- **Description:** Specifies the compression algorithm used to compress messages before sending them to the Kafka brokers.
- **Importance:** Reduces network bandwidth and storage space.
- **Recommendation:** Enable compression using `gzip`, `snappy`, `lz4`, or `zstd`. `zstd` generally offers the best compression ratio and speed.

### 6. `max.request.size`

- **Description:** Specifies the maximum size of a request the producer can send to the broker.
- **Importance:** Avoids exceeding broker limits and potential errors.
- **Recommendation:** Ensure this value is less than or equal to the `message.max.bytes` setting on the brokers. A good starting point is 1MB (1048576).

### Code Example (Producer Configuration):

```plaintext
import org.apache.kafka.clients.producer.*;
import java.util.Properties;

public class KafkaProducerExample {
    public static void main(String[] args) {
        Properties props = new Properties();
        props.put("bootstrap.servers", "kafka1.example.com:9092,kafka2.example.com:9092,kafka3.example.com:9092");
        props.put("acks", "all");
        props.put("retries", 3);
        props.put("retry.backoff.ms", 100);
        props.put("batch.size", 32768); // 32KB
        props.put("linger.ms", 5);
        props.put("compression.type", "zstd");
        props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
        props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");
        props.put("max.request.size", 1048576); // 1MB

        Producer<String, String> producer = new KafkaProducer<>(props);

        try {
            for (int i = 0; i < 100; i++) {
                producer.send(new ProducerRecord<>("my-topic", Integer.toString(i), "Message " + i),
                    (metadata, exception) -> {
                        if (exception != null) {
                            System.err.println("Failed to send message: " + exception.getMessage());
                        } else {
                            System.out.println("Sent message to partition " + metadata.partition() + " with offset " + metadata.offset());
                        }
                    });
            }
        } finally {
            producer.close();
        }
    }
}
```

## Consumer Configuration: Ensuring Efficient Data Consumption

The Kafka consumer reads data from the Kafka cluster. Optimizing consumer settings is crucial for achieving desired throughput and latency while ensuring data is processed correctly.

### 1. `bootstrap.servers`

- **Description:** Identical to the producer's `bootstrap.servers`.
- **Importance:** Essential for consumer to discover the Kafka cluster.
- **Recommendation:** Provide a list of at least two or three brokers. Example: `bootstrap.servers=kafka1.example.com:9092,kafka2.example.com:9092,kafka3.example.com:9092`

### 2. `group.id`

- **Description:** A unique identifier for the consumer group.
- **Importance:** Enables consumer group functionality for parallel data consumption.
- **Recommendation:** Use a descriptive and consistent `group.id` for each consumer group. Consumers within the same group will share the load of processing partitions from the subscribed topics.

### 3. `auto.offset.reset`

- **Description:** Specifies what the consumer should do when it encounters an offset that is no longer valid (e.g., due to data retention).
- **Importance:** Controls consumer behavior when starting or restarting.
- **Recommendation:**

  - **`earliest`:** Start reading from the earliest available offset.
  - **`latest`:** Start reading from the latest offset (i.e., only process new messages).
  - **`none`:** Throw an exception if no initial offset is found for the consumer group. This is useful when you require explicit offset management.

  Choose `earliest` if you need to process all data from the beginning. Choose `latest` if you only need to process new data. Choose `none` for strict control.

### 4. `enable.auto.commit` and `auto.commit.interval.ms`

- **Description:** `enable.auto.commit` enables automatic offset commits. `auto.commit.interval.ms` specifies the frequency of automatic commits.
- **Importance:** Controls how consumer offsets are managed.
- **Recommendation:**

  - **Disable auto-commit (`enable.auto.commit=false`) for more reliable processing.** Manually commit offsets after processing each batch of messages. This ensures that you only commit offsets for messages that have been successfully processed.
  - **If you enable auto-commit, adjust `auto.commit.interval.ms` to a reasonable value (e.g., 5000ms or 10000ms).** Avoid committing offsets too frequently, as this can impact performance.

### 5. `max.poll.records`

- **Description:** Specifies the maximum number of records the consumer will attempt to retrieve in a single poll request.
- **Importance:** Impacts throughput and latency.
- **Recommendation:** Increase this value (e.g., 500 or 1000) to improve throughput. However, be mindful of the impact on processing time and potential timeouts.

### 6. `session.timeout.ms` and `heartbeat.interval.ms`

- **Description:** `session.timeout.ms` specifies the maximum time a consumer can be inactive before the broker considers it dead. `heartbeat.interval.ms` controls the frequency at which the consumer sends heartbeats to the broker.
- **Importance:** Ensures proper consumer group management and failure detection.
- **Recommendation:** Adjust these values based on your application's processing time and network conditions. The `session.timeout.ms` should be greater than the time it takes to process a batch of messages. The `heartbeat.interval.ms` should be smaller than `session.timeout.ms` (typically 1/3 of the session timeout).

### 7. `fetch.min.bytes` and `fetch.max.wait.ms`

- **Description:** `fetch.min.bytes` specifies the minimum amount of data the server should return for a fetch request. `fetch.max.wait.ms` specifies the maximum amount of time the server will block waiting for enough data to satisfy `fetch.min.bytes`.
- **Importance:** Influences the amount of data fetched per request and the latency.
- **Recommendation:** Increase `fetch.min.bytes` to increase the amount of data fetched per request and reduce the overhead. A common starting point is 16KB. Adjust `fetch.max.wait.ms` accordingly to avoid excessive delays.

### Code Example (Consumer Configuration and Manual Offset Commit):

```plaintext
import org.apache.kafka.clients.consumer.*;
import org.apache.kafka.common.TopicPartition;
import java.time.Duration;
import java.util.Collections;
import java.util.Properties;

public class KafkaConsumerExample {
    public static void main(String[] args) {
        Properties props = new Properties();
        props.put("bootstrap.servers", "kafka1.example.com:9092,kafka2.example.com:9092,kafka3.example.com:9092");
        props.put("group.id", "my-consumer-group");
        props.put("enable.auto.commit", "false"); // Disable auto commit
        props.put("auto.offset.reset", "earliest");
        props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
        props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
        props.put("max.poll.records", 500);
        props.put("session.timeout.ms", 30000);
        props.put("heartbeat.interval.ms", 10000);
        props.put("fetch.min.bytes", 16384); // 16KB
        props.put("fetch.max.wait.ms", 500);



        Consumer<String, String> consumer = new KafkaConsumer<>(props);
        consumer.subscribe(Collections.singletonList("my-topic"));

        try {
            while (true) {
                ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(100));
                for (ConsumerRecord<String, String> record : records) {
                    System.out.printf("offset = %d, key = %s, value = %s%n", record.offset(), record.key(), record.value());
                    // Process the record here
                }

                //Manually commit the offset after processing the records.
                if (!records.isEmpty()) {
                    consumer.commitSync(); // Commit the latest consumed offset.  Use commitAsync for better performance but lower reliability.
                }

            }
        } finally {
            consumer.close();
        }
    }
}
```

## Kafka Connect Configuration: Moving Data In and Out

Kafka Connect provides a framework for streaming data between Kafka and other systems. Optimizing Kafka Connect configurations often involves tuning connector-specific settings, but also involves general worker properties.

### Worker Configuration (`connect-distributed.properties` or `connect-standalone.properties`)

- **`bootstrap.servers`**: Same as producer and consumer.
- **`group.id`**: The Connect worker's group ID. All workers in a distributed cluster should have the same group ID.
- **`config.storage.topic`**: The topic where connector configurations are stored. Create this topic before starting Connect.
- **`offset.storage.topic`**: The topic where connector offsets are stored. Create this topic before starting Connect.
- **`status.storage.topic`**: The topic where connector statuses are stored. Create this topic before starting Connect.
- **Replication Factor:** Ensure the `config.storage.topic`, `offset.storage.topic` and `status.storage.topic` have a replication factor of at least 3 for high availability.
- **`key.converter` and `value.converter`:** Specify the converters for keys and values. Common options include `org.apache.kafka.connect.json.JsonConverter`, `org.apache.kafka.connect.storage.StringConverter`, and `io.confluent.connect.avro.AvroConverter`.
- **`plugin.path`**: Specifies the location of connector plugins.

### Connector-Specific Settings

Connector-specific settings vary greatly depending on the connector being used (e.g., JDBC source connector, S3 sink connector). Refer to the documentation for each connector for details. Some common optimization techniques include:

- **`tasks.max`**: The maximum number of tasks that can be created for the connector. Increase this value for parallel processing.
- **Connection Pooling:** Configure connection pooling settings for database connectors to optimize connection reuse.
- **Batching:** Tune batching parameters for source and sink connectors to optimize throughput.
- **Error Handling:** Configure error handling strategies for dealing with failed messages.

### Example: Creating Topics for Kafka Connect

Before running Kafka Connect in distributed mode, create the necessary topics with the correct replication factor:

```plaintext
kafka-topics --create --topic connect-config --bootstrap-server kafka1.example.com:9092,kafka2.example.com:9092 --replication-factor 3 --partitions 1
kafka-topics --create --topic connect-offsets --bootstrap-server kafka1.example.com:9092,kafka2.example.com:9092 --replication-factor 3 --partitions 50
kafka-topics --create --topic connect-status --bootstrap-server kafka1.example.com:9092,kafka2.example.com:9092 --replication-factor 3 --partitions 10
```

## Kafka Streams Configuration: Building Real-Time Applications

Kafka Streams is a client library for building stream processing applications. Optimize Kafka Streams applications by tuning various parameters:

- **`application.id`**: A unique ID for the Kafka Streams application. All instances of the same application must use the same ID.
- **`bootstrap.servers`**: Same as producer and consumer.
- **`processing.guarantee`**: Controls the processing guarantees.
  - **`at_least_once`**: Guarantees that each message will be processed at least once. This is the default. Messages may be processed multiple times in case of failures.
  - **`exactly_once`**: Guarantees that each message will be processed exactly once. This requires transactional processing and may have a performance impact. Requires `transactions.enabled=true` in the broker configuration.
- **`num.stream.threads`**: The number of threads to use for processing. Increase this value for parallel processing.
- **`cache.max.bytes.buffering`**: The amount of memory to use for buffering data. Increase this value for improved performance.
- **`commit.interval.ms`**: The interval at which the Kafka Streams application commits its offsets. Reduce this value for lower latency, but increase it for better throughput.
- **`state.dir`**: The directory where Kafka Streams stores its state. Use a fast disk for this directory.
- **`replication.factor`**: Define the replication factor of the internal topics created by Kafka Streams. Overrides the broker's `default.replication.factor`.

### Example: Kafka Streams Configuration

```plaintext
import org.apache.kafka.streams.KafkaStreams;
import org.apache.kafka.streams.StreamsBuilder;
import org.apache.kafka.streams.StreamsConfig;
import org.apache.kafka.streams.Topology;
import java.util.Properties;

public class KafkaStreamsExample {

    public static void main(String[] args) {
        Properties props = new Properties();
        props.put(StreamsConfig.APPLICATION_ID_CONFIG, "my-streams-application");
        props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "kafka1.example.com:9092,kafka2.example.com:9092,kafka3.example.com:9092");
        props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, org.apache.kafka.common.serialization.Serdes.String().getClass());
        props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, org.apache.kafka.common.serialization.Serdes.String().getClass());
        props.put(StreamsConfig.PROCESSING_GUARANTEE_CONFIG, StreamsConfig.EXACTLY_ONCE_V2);
        props.put(StreamsConfig.NUM_STREAM_THREADS_CONFIG, 4);
        props.put(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG, 10 * 1024 * 1024L); // 10MB
        props.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 2000);
        props.put(StreamsConfig.STATE_DIR_CONFIG, "/data/kafka-streams/state");

        StreamsBuilder builder = new StreamsBuilder();
        // Define your stream processing topology here
        // Example:
        // KStream<String, String> textLines = builder.stream("input-topic");
        // textLines.to("output-topic");

        Topology topology = builder.build();

        KafkaStreams streams = new KafkaStreams(topology, props);
        streams.start();

        // Add shutdown hook for graceful termination
        Runtime.getRuntime().addShutdownHook(new Thread(streams::close));
    }
}
```

## Monitoring and Alerting

Proper monitoring and alerting are essential for maintaining a healthy Kafka cluster. Monitor the following metrics:

- **Broker Metrics:**
  - CPU utilization
  - Disk I/O
  - Network I/O
  - Memory usage
  - Under-replicated partitions
  - Offline partitions
  - Request latency
  - Active Controller Count (should be 1)
- **Producer Metrics:**
  - Message latency
  - Send rate
  - Error rate
- **Consumer Metrics:**
  - Lag (the difference between the latest offset and the consumer's current offset)
  - Fetch rate
  - Commit latency
  - Bytes consumed per second.
- **Kafka Connect Metrics:**
  - Connector status
  - Task status
  - Throughput
  - Latency
- **Kafka Streams Metrics:**
  - Application status
  - Processing latency
  - State store size

Set up alerts for critical metrics to proactively identify and address potential issues. Tools like Prometheus, Grafana, and Datadog can be used for monitoring Kafka.

## Conclusion

Optimizing Kafka settings for production is an ongoing process that requires careful consideration of your specific workload, hardware resources, and business requirements. By understanding the key configuration parameters and monitoring your cluster effectively, you can build a reliable, scalable, and high-performing Kafka deployment. Remember to test your configurations thoroughly in a staging environment before deploying them to production. Regularly review and adjust your settings as your data volumes and application requirements evolve.
