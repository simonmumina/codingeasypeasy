---
title: 'Testing Gatsby Page Performance in CI Pipelines: A Comprehensive Guide'
date: '2024-02-29'
lastmod: '2024-10-27'
tags:
  [
    'gatsby',
    'performance testing',
    'ci/cd',
    'lighthouse',
    'web vitals',
    'javascript',
    'testing',
    'optimization',
  ]
draft: false
summary: 'Learn how to integrate Gatsby page performance testing into your CI/CD pipelines to catch regressions early and ensure optimal website performance. This comprehensive guide covers Lighthouse integration, Web Vitals monitoring, and practical code examples for setting up performance tests in popular CI environments like GitHub Actions and GitLab CI.'
authors: ['default']
---

# Testing Gatsby Page Performance in CI Pipelines: A Comprehensive Guide

Maintaining a fast and performant website is crucial for user experience, SEO rankings, and overall business success. Gatsby, with its emphasis on performance optimizations like pre-rendering and code splitting, helps in building fast sites. However, changes during development can inadvertently introduce performance regressions. Integrating performance testing into your Continuous Integration (CI) pipelines allows you to automatically detect and address these issues before they reach production.

This guide provides a comprehensive overview of how to test Gatsby page performance in your CI pipelines, covering key metrics, tools, and practical code examples.

## Why Test Performance in CI?

Integrating performance testing into your CI pipeline offers several significant benefits:

- **Early Detection of Regressions:** Identifies performance bottlenecks and regressions early in the development cycle, preventing them from impacting live users.
- **Improved Website Quality:** Ensures consistent performance across releases, leading to a better user experience and improved SEO.
- **Reduced Manual Effort:** Automates performance testing, freeing up developers to focus on building new features.
- **Data-Driven Optimization:** Provides concrete data to guide optimization efforts, allowing you to prioritize areas with the greatest impact.
- **Increased Confidence in Deployments:** Gives developers confidence that new deployments won't negatively impact performance.

## Key Performance Metrics to Monitor

Before diving into the implementation, it's essential to understand the key performance metrics that you should be monitoring:

- **Lighthouse Scores:** Google Lighthouse provides a comprehensive analysis of website performance, accessibility, best practices, SEO, and Progressive Web App (PWA) capabilities. Focus on the performance score, which is based on key metrics like:
  - **First Contentful Paint (FCP):** The time it takes for the first piece of content to be rendered on the page.
  - **Largest Contentful Paint (LCP):** The time it takes for the largest content element to be rendered. This is a crucial Web Vital.
  - **First Input Delay (FID):** The time it takes for the browser to respond to the first user interaction. This is now replaced by INP.
  - **Interaction to Next Paint (INP):** Measures the time it takes for a page to respond to all user interactions. This is a Web Vital replacing FID.
  - **Cumulative Layout Shift (CLS):** Measures the visual stability of the page. This is a crucial Web Vital.
  - **Time to Interactive (TTI):** The time it takes for the page to become fully interactive.
  - **Speed Index:** Measures how quickly the contents of a page are visually populated.
- **Web Vitals:** A set of metrics defined by Google that are considered essential for a great user experience. These include LCP, INP, and CLS. These metrics have a direct impact on SEO.
- **Bundle Size:** The size of your JavaScript and CSS bundles. Larger bundles can significantly impact load times.
- **Image Optimization:** Properly optimized images are crucial for performance. Check for image sizes, formats, and lazy loading.

## Tools for Performance Testing

Several tools can be used to test Gatsby page performance in your CI pipelines:

- **Lighthouse CLI:** A command-line interface for running Lighthouse audits. This is the most common and straightforward way to integrate performance testing into CI.
- **Lighthouse CI:** A suite of tools and services for automating Lighthouse audits in CI. It provides features like budget tracking and reporting.
- **PageSpeed Insights API:** Google's API for programmatically running PageSpeed Insights audits.
- **WebPageTest:** A powerful online tool for measuring website performance. It offers detailed waterfall charts and various testing options.
- **Bundle Analyzers:** Tools like `webpack-bundle-analyzer` help you visualize the contents of your JavaScript bundles and identify opportunities for optimization.

## Setting up Performance Testing in Your CI Pipeline

Here's a step-by-step guide to setting up performance testing in your CI pipeline using Lighthouse CLI, with examples for GitHub Actions and GitLab CI.

**1. Install Lighthouse CLI:**

```plaintext
npm install -g lighthouse
```

or

```plaintext
yarn global add lighthouse
```

**2. Create a Performance Test Script:**

Create a script (e.g., `scripts/test-performance.js`) that runs Lighthouse audits and checks for performance regressions. This script will be executed in your CI pipeline. Here's an example:

```plaintext
// scripts/test-performance.js
const lighthouse = require('lighthouse')
const chromeLauncher = require('chrome-launcher')
const fs = require('fs')

async function runLighthouse(url) {
  const chrome = await chromeLauncher.launch({ chromeFlags: ['--headless'] })
  const options = {
    port: chrome.port,
    onlyCategories: ['performance'], // Focus on performance category
    output: 'json', // Output results in JSON format
  }
  const runnerResult = await lighthouse(url, options)

  await chrome.kill()
  return runnerResult.lhr
}

async function checkPerformance(url, thresholds) {
  try {
    const lhr = await runLighthouse(url)

    console.log(`Lighthouse report for ${url}:`)
    console.log(`Performance Score: ${lhr.categories.performance.score * 100}`)

    const errors = []
    for (const metric in thresholds) {
      const expected = thresholds[metric]
      const actual = lhr.categories.performance.score * 100 // Assuming score represents %
      if (actual < expected) {
        errors.push(`Performance score for ${url} is ${actual}, expected at least ${expected}.`)
      }
    }

    if (errors.length > 0) {
      console.error('Performance tests failed:')
      errors.forEach((error) => console.error(error))
      process.exit(1) // Exit with an error code to fail the CI build
    } else {
      console.log('Performance tests passed!')
    }
  } catch (error) {
    console.error('Error running Lighthouse:', error)
    process.exit(1)
  }
}

async function main() {
  const url = process.env.GATSBY_SITE_URL || 'http://localhost:9000' // Use env var for the deployed site URL
  const thresholds = {
    performance: 90, // Example threshold for performance score
  }

  if (!url) {
    console.error('GATSBY_SITE_URL environment variable not set.')
    process.exit(1)
  }

  await checkPerformance(url, thresholds)
}

main()
```

**Explanation:**

- **Requires necessary modules:** `lighthouse` for running the audit, `chrome-launcher` for launching Chrome in headless mode, and `fs` for file system operations.
- **`runLighthouse(url)`:** Launches Chrome in headless mode, runs a Lighthouse audit on the specified URL, and returns the Lighthouse results in JSON format. Only focuses on the 'performance' category to speed things up.
- **`checkPerformance(url, thresholds)`:** Compares the Lighthouse scores against predefined thresholds. If the scores fall below the thresholds, it logs an error and exits with a non-zero exit code, causing the CI build to fail.
- **`main()`:** Sets the target URL (using an environment variable, defaulting to localhost), defines the performance thresholds, and calls `checkPerformance`.
- **Error Handling:** Includes robust error handling to catch potential issues during the Lighthouse run.
- **Environment Variable:** Uses the `GATSBY_SITE_URL` environment variable to allow you to test against a deployed staging environment. Defaults to `http://localhost:9000` for local testing. This is crucial for CI pipelines where your Gatsby site is built and served on a specific URL.

**3. Configure Your CI Pipeline:**

Here are examples for configuring GitHub Actions and GitLab CI:

**GitHub Actions (`.github/workflows/performance-test.yml`):**

```plaintext
name: Performance Tests

on:
  pull_request:
    branches: [main]
  push:
    branches: [main]

jobs:
  performance_test:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Setup Node.js
        uses: actions/setup-node@v3
        with:
          node-version: 18 # Or your preferred Node.js version

      - name: Install dependencies
        run: npm install # Or yarn install

      - name: Build Gatsby site
        run: npm run build # Or yarn build

      - name: Serve Gatsby site
        run: |
          npx serve -s public -p 9000 &  # Start a web server in the background
          sleep 5 # Wait for the server to start

      - name: Run Performance Tests
        env:
          GATSBY_SITE_URL: http://localhost:9000 # Set the URL to the served site
        run: node scripts/test-performance.js
```

**Explanation:**

- **`on`:** Triggers the workflow on pull requests and pushes to the `main` branch.
- **`jobs`:** Defines the `performance_test` job, which runs on an Ubuntu runner.
- **`steps`:**
  - **Checkout code:** Checks out the code from the repository.
  - **Setup Node.js:** Sets up the Node.js environment.
  - **Install dependencies:** Installs the project dependencies.
  - **Build Gatsby site:** Builds the Gatsby site using `npm run build`.
  - **Serve Gatsby site:** Starts a simple static server (`npx serve`) to serve the built Gatsby site on port 9000. **Crucially, the `&` runs the server in the background, and `sleep 5` waits for it to start.**
  - **Run Performance Tests:** Sets the `GATSBY_SITE_URL` environment variable to `http://localhost:9000` (the URL of the served site) and then executes the `scripts/test-performance.js` script.

**GitLab CI (`.gitlab-ci.yml`):**

```plaintext
stages:
  - test

performance_test:
  image: node:18 # Or your preferred Node.js version
  stage: test
  before_script:
    - npm install # Or yarn install
  script:
    - npm run build # Or yarn build
    - npx serve -s public -p 9000 &
    - sleep 5
    - GATSBY_SITE_URL=http://localhost:9000 node scripts/test-performance.js
  artifacts:
    paths:
      - public # Artifacts are not strictly necessary here but could be useful for debugging
```

**Explanation:**

- **`image`:** Specifies the Docker image to use for the job (Node.js 18).
- **`stage`:** Defines the `test` stage.
- **`before_script`:** Installs the project dependencies.
- **`script`:**
  - Builds the Gatsby site.
  - Starts a static server.
  - Runs the performance tests script, setting the `GATSBY_SITE_URL` environment variable.
- **`artifacts`:** (Optional) Saves the `public` directory as an artifact, which can be downloaded for debugging.

**4. Commit and Push Your Changes:**

Commit your changes, including the `scripts/test-performance.js` file and the CI configuration file (`.github/workflows/performance-test.yml` or `.gitlab-ci.yml`), and push them to your repository.

**5. Monitor Your CI Pipeline:**

Your CI pipeline will now automatically run the performance tests whenever a pull request is created or code is pushed to the `main` branch. Monitor the pipeline results to identify any performance regressions.

## Advanced Configuration and Optimizations

- **Multiple URLs:** Modify the `scripts/test-performance.js` script to test multiple URLs. This is especially useful for testing different page types on your site.
- **Dynamic Thresholds:** Use environment variables or configuration files to dynamically adjust the performance thresholds based on the specific environment or page type.
- **Custom Lighthouse Configuration:** Customize the Lighthouse configuration options (e.g., throttling, onlyCategories) to tailor the tests to your specific needs. Refer to the Lighthouse documentation for available options.
- **Lighthouse CI Integration:** Explore using Lighthouse CI for more advanced features like budget tracking and reporting. This requires setting up a Lighthouse CI server.
- **Visual Regression Testing:** Combine performance testing with visual regression testing to catch visual changes that might impact performance.
- **Bundle Analysis:** Integrate bundle analysis into your CI pipeline to monitor bundle sizes and identify opportunities for code splitting and optimization.

## Example: Testing Multiple URLs

Here's how to modify the `scripts/test-performance.js` script to test multiple URLs:

```plaintext
// scripts/test-performance.js (modified)
const lighthouse = require('lighthouse')
const chromeLauncher = require('chrome-launcher')

async function runLighthouse(url) {
  const chrome = await chromeLauncher.launch({ chromeFlags: ['--headless'] })
  const options = {
    port: chrome.port,
    onlyCategories: ['performance'],
    output: 'json',
  }
  const runnerResult = await lighthouse(url, options)

  await chrome.kill()
  return runnerResult.lhr
}

async function checkPerformance(url, thresholds) {
  try {
    const lhr = await runLighthouse(url)

    console.log(`Lighthouse report for ${url}:`)
    console.log(`Performance Score: ${lhr.categories.performance.score * 100}`)

    const errors = []
    for (const metric in thresholds) {
      const expected = thresholds[metric]
      const actual = lhr.categories.performance.score * 100
      if (actual < expected) {
        errors.push(`Performance score for ${url} is ${actual}, expected at least ${expected}.`)
      }
    }

    if (errors.length > 0) {
      console.error('Performance tests failed for ${url}:')
      errors.forEach((error) => console.error(error))
      return false // Indicate failure for this URL
    } else {
      console.log('Performance tests passed for ${url}!')
      return true // Indicate success for this URL
    }
  } catch (error) {
    console.error('Error running Lighthouse for ${url}:', error)
    return false // Indicate failure
  }
}

async function main() {
  const urls = (process.env.GATSBY_SITE_URLS || 'http://localhost:9000').split(',') // Split comma-separated URLs
  const thresholds = {
    performance: 90,
  }

  if (!urls || urls.length === 0) {
    console.error('GATSBY_SITE_URLS environment variable not set.')
    process.exit(1)
  }

  let allPassed = true // Flag to track overall success

  for (const url of urls) {
    const trimmedUrl = url.trim() // Remove whitespace
    const result = await checkPerformance(trimmedUrl, thresholds)
    if (!result) {
      allPassed = false // If any test fails, set the flag to false
    }
  }

  if (!allPassed) {
    console.error('One or more performance tests failed.')
    process.exit(1) // Exit with an error code if any test failed
  } else {
    console.log('All performance tests passed!')
  }
}

main()
```

**Changes:**

- **`GATSBY_SITE_URLS` Environment Variable:** Now uses `GATSBY_SITE_URLS` (plural) to accept a comma-separated list of URLs. The `split(',')` method splits the string into an array of URLs.
- **Looping through URLs:** The `main()` function now iterates through the array of URLs and calls `checkPerformance()` for each URL.
- **`checkPerformance` returns a boolean:** The `checkPerformance` function now returns `true` if the tests pass for a given URL and `false` if they fail.
- **Overall Success Flag:** The `allPassed` flag tracks whether all tests passed. If any test fails, the flag is set to `false`, and the script exits with an error code.
- **Whitespace trimming:** Added `trimmedUrl = url.trim()` to remove any whitespace around the URLs, which can cause errors.

**CI Configuration Update:**

Update your CI configuration to set the `GATSBY_SITE_URLS` environment variable with a comma-separated list of URLs:

**GitHub Actions:**

```plaintext
- name: Run Performance Tests
  env:
    GATSBY_SITE_URLS: http://localhost:9000,http://localhost:9000/page-2 # Example URLs
  run: node scripts/test-performance.js
```

**GitLab CI:**

```plaintext
- GATSBY_SITE_URLS=http://localhost:9000,http://localhost:9000/page-2 node scripts/test-performance.js # Example URLs
```

## Best Practices

- **Test Realistic Scenarios:** Configure your CI environment to mimic your production environment as closely as possible, including network conditions and device emulation.
- **Set Realistic Thresholds:** Establish performance thresholds that are aligned with your business goals and user expectations. Start with reasonable thresholds and gradually adjust them based on your site's performance and user feedback.
- **Monitor Performance Trends:** Track performance metrics over time to identify patterns and trends. This will help you proactively address potential performance issues before they impact users.
- **Optimize Images:** Ensure that your images are properly optimized for the web. Use appropriate image formats (e.g., WebP), compress images, and use lazy loading.
- **Code Splitting:** Implement code splitting to reduce the size of your JavaScript bundles and improve initial load times.
- **Caching:** Leverage browser caching and CDN caching to improve performance for returning users.
- **Regularly Review and Update Your Tests:** As your website evolves, regularly review and update your performance tests to ensure they are still relevant and effective.

## Conclusion

Integrating Gatsby page performance testing into your CI pipelines is a critical step in ensuring a fast, reliable, and user-friendly website. By following the steps outlined in this guide and adopting the best practices discussed, you can proactively identify and address performance regressions, optimize your website for speed, and deliver a superior user experience. Remember to tailor the tests and thresholds to your specific needs and continuously monitor performance trends to maintain optimal website performance over time.
