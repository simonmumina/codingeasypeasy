---
title: 'AI-Powered Rate Limiting with FastAPI: Detect and Prevent Abnormal Traffic Patterns'
date: '2024-10-26'
lastmod: '2024-10-26'
tags:
  [
    'fastapi',
    'rate limiting',
    'ai',
    'machine learning',
    'anomaly detection',
    'python',
    'security',
    'traffic analysis',
  ]
draft: false
summary: 'Implement advanced rate limiting in FastAPI using AI to detect and prevent abnormal traffic patterns, protecting your API from abuse and attacks. Learn how to integrate machine learning models for real-time anomaly detection and adaptively adjust rate limits.'
authors: ['default']
---

# AI-Powered Rate Limiting with FastAPI: Detect and Prevent Abnormal Traffic Patterns

Rate limiting is a crucial component of any robust API, protecting it from abuse, denial-of-service (DoS) attacks, and general overuse. Traditional rate limiting relies on fixed thresholds, such as allowing only a certain number of requests per IP address within a given timeframe. However, these static rules can be easily circumvented by attackers and may even inadvertently block legitimate users during periods of high activity.

This blog post explores how to implement **AI-powered rate limiting** in FastAPI, enabling your API to dynamically adapt to traffic patterns and detect abnormal behavior that traditional methods might miss. We'll leverage machine learning (ML) techniques to identify anomalies in real-time and adjust rate limits accordingly, providing a more sophisticated and effective defense against malicious activity.

## Why AI-Based Rate Limiting?

Here's why AI-based rate limiting offers a significant advantage over traditional methods:

- **Adaptive Protection:** AI models can learn normal traffic patterns and automatically adjust rate limits based on the current context. This avoids over-limiting legitimate users while providing tighter controls during suspicious activity.
- **Anomaly Detection:** ML algorithms excel at identifying unusual patterns and deviations from the norm. This allows you to detect sophisticated attacks and malicious bots that might bypass simple threshold-based rules.
- **Improved Accuracy:** By considering various factors like request frequency, payload size, user behavior, and more, AI models can make more informed decisions about whether to throttle a request.
- **Reduced False Positives:** AI can differentiate between legitimate spikes in traffic and malicious activity, minimizing the risk of blocking genuine users.
- **Proactive Defense:** By identifying anomalies early on, AI-based rate limiting can prevent attacks before they cause significant damage.

## Implementing AI-Powered Rate Limiting with FastAPI

Here's a step-by-step guide to integrating AI-based rate limiting into your FastAPI application:

**1. Project Setup & Dependencies**

First, create a new FastAPI project and install the necessary dependencies. We'll need:

- `fastapi`: The core FastAPI framework.
- `uvicorn`: An ASGI server for running the FastAPI application.
- `scikit-learn`: For building our machine learning model.
- `redis`: A popular in-memory data store for storing request data and model state. (Alternative: any suitable database like PostgreSQL, MySQL etc.)
- `redis-om`: Object mapping to connect and manage Redis data.
- `python-dotenv`: (Optional) For loading environment variables.

```bash
pip install fastapi uvicorn scikit-learn redis redis-om python-dotenv
```

**2. Define a Simple FastAPI Application**

Let's start with a basic FastAPI application:

```plaintext
# main.py
from fastapi import FastAPI, Depends, HTTPException
from fastapi.responses import JSONResponse
from typing import Dict, Any
from redis_om import get_redis_connection, HashModel
import os
from dotenv import load_dotenv

load_dotenv()

REDIS_HOST = os.getenv("REDIS_HOST", "localhost")
REDIS_PORT = os.getenv("REDIS_PORT", "6379")
REDIS_PASSWORD = os.getenv("REDIS_PASSWORD", "")

app = FastAPI()

redis = get_redis_connection(
    host=REDIS_HOST,
    port=REDIS_PORT,
    password=REDIS_PASSWORD,
    decode_responses=True
)


class RequestLog(HashModel):
    ip_address: str
    timestamp: float
    uri: str
    user_agent: str

    class Meta:
        database = redis

@app.get("/")
async def read_root():
    return {"Hello": "World"}

@app.get("/items/{item_id}")
async def read_item(item_id: int, q: str = None):
    return {"item_id": item_id, "q": q}

```

**3. Data Collection and Feature Engineering**

Before building our AI model, we need to collect data about API requests. We'll log the following features for each request:

- **IP Address:** The client's IP address.
- **Timestamp:** The time of the request.
- **URI:** The requested URL path.
- **User Agent:** The client's user agent string.

These features can provide valuable insights into traffic patterns. We use redis-om to store each request in Redis. The `RequestLog` model defines the schema for our logs.

**4. Anomaly Detection Model**

We'll use an Isolation Forest model from scikit-learn to detect anomalies. Isolation Forest is well-suited for this task because it can effectively identify outliers in high-dimensional data without requiring labeled training data.

Here's the code to train and use the Isolation Forest model:

```plaintext
# ai_rate_limiter.py
import time
import numpy as np
from sklearn.ensemble import IsolationForest
from typing import List
from main import RequestLog
import logging

logging.basicConfig(level=logging.INFO)

class AIRateLimiter:
    def __init__(self, model_path="isolation_forest.joblib", retraining_interval=60):
        self.model_path = model_path
        self.retraining_interval = retraining_interval
        self.last_trained = time.time()
        self.model = self.load_or_train_model()  # Load or train on startup

    def load_or_train_model(self):
        """Loads the model from disk or trains a new one if it doesn't exist or is outdated."""
        try:
            import joblib
            model = joblib.load(self.model_path)
            logging.info("Loaded existing model from disk.")
            return model
        except FileNotFoundError:
            logging.info("No existing model found. Training a new one.")
            return self.train_model()  # Train immediately if no model exists
        except Exception as e:
             logging.error(f"Error loading model: {e}, Training a new one.")
             return self.train_model()

    def train_model(self, data: List[RequestLog] = None):
        """Trains the Isolation Forest model with historical data."""
        import joblib

        if data is None:
          data = RequestLog.find().all()


        if not data:
            logging.warning("No historical data available. Returning a default untrained model.")
            self.model = IsolationForest(random_state=42)  # Default, untrained
            return self.model

        features = self.extract_features(data)  # Directly use features from existing logs

        logging.info(f"Training model with {len(features)} data points.")
        self.model = IsolationForest(random_state=42)  # Initialize the model
        try:
            self.model.fit(features)  # Train the model
            joblib.dump(self.model, self.model_path)  # Save the trained model
            self.last_trained = time.time() # update last trained
            logging.info("Model trained and saved successfully.")
            return self.model
        except Exception as e:
            logging.error(f"Error during model training: {e}")
            return IsolationForest(random_state=42)  # Return a default untrained model

    def extract_features(self, data: List[RequestLog]):
        """Extracts relevant features from the request logs."""
        features = []
        for log in data:
          # For now, just use timestamp.  Enhance later with more features
          features.append([log.timestamp])
        return np.array(features)


    def is_anomalous(self, request_data):
        """Predicts whether the request is anomalous."""
        feature = np.array([request_data['timestamp']]).reshape(1, -1)  # Prepare the feature
        if self.model is None:
            logging.warning("Model not initialized. Assuming normal traffic.")
            return False

        prediction = self.model.predict(feature)[0]
        return prediction == -1

    def check_rate_limit(self, request_data: Dict[str, Any]):
        """Checks if the request exceeds the rate limit based on AI analysis."""
        if time.time() - self.last_trained > self.retraining_interval:
            logging.info("Retraining model due to retraining interval")
            self.train_model()


        is_anomalous = self.is_anomalous(request_data)

        if is_anomalous:
            logging.warning(f"Anomalous request detected: {request_data}")
            return True  # Rate limit exceeded
        else:
            return False  # Rate limit not exceeded
```

**Explanation of `ai_rate_limiter.py`:**

- **`AIRateLimiter` Class:** Encapsulates the AI-based rate limiting logic.
- **`__init__`:**
  - Initializes the rate limiter with the path to the saved model and a retraining interval.
  - Loads the trained Isolation Forest model from disk or trains a new one if it doesn't exist on startup.
- **`load_or_train_model`:** Attempts to load a previously trained model from disk using `joblib`. If the file doesn't exist, or if an error occurs during loading, it calls `self.train_model()` to train a new model. It handles potential exceptions and logs informative messages.
- **`train_model`:**
  - Fetches historical request data from Redis using `RequestLog.find().all()`.
  - If no data is available, logs a warning and returns a default untrained Isolation Forest Model.
  - Extracts relevant features from the request logs using `extract_features`. (Currently only using `timestamp` but you can easily add more features)
  - Trains the Isolation Forest model using the extracted features.
  - Saves the trained model to disk using `joblib`.
- **`extract_features`:** Extracts features from a list of `RequestLog` objects. Currently, it only extracts the timestamp of each request. This is the most important part to customize. You'll need to experiment with different feature combinations to find what works best for your API.
- **`is_anomalous`:** Takes a request data dictionary as input, extracts the features and then uses the trained Isolation Forest model to predict whether the request is anomalous (an outlier). Returns `True` if the request is anomalous (rate limit exceeded), and `False` otherwise. Handles the case where the model hasn't been initialized.
- **`check_rate_limit`:**
  - Checks if the retraining interval has elapsed. If so, retrains the model by calling `self.train_model()`.
  - Calls `is_anomalous` to determine if the request is anomalous.
  - Returns `True` if the rate limit has been exceeded (anomalous request detected) and `False` otherwise.

**5. Integrate the AI Rate Limiter into FastAPI**

Now, let's integrate the `AIRateLimiter` into our FastAPI application:

```plaintext
# main.py
from fastapi import FastAPI, Depends, HTTPException
from fastapi.responses import JSONResponse
from typing import Dict, Any
from redis_om import get_redis_connection, HashModel
import os
from dotenv import load_dotenv
import time
from ai_rate_limiter import AIRateLimiter # Import the AI rate limiter


load_dotenv()

REDIS_HOST = os.getenv("REDIS_HOST", "localhost")
REDIS_PORT = os.getenv("REDIS_PORT", "6379")
REDIS_PASSWORD = os.getenv("REDIS_PASSWORD", "")

app = FastAPI()

redis = get_redis_connection(
    host=REDIS_HOST,
    port=REDIS_PORT,
    password=REDIS_PASSWORD,
    decode_responses=True
)


class RequestLog(HashModel):
    ip_address: str
    timestamp: float
    uri: str
    user_agent: str

    class Meta:
        database = redis

# Initialize the AI rate limiter
ai_rate_limiter = AIRateLimiter(retraining_interval=60)  # Retrain every 60 seconds.  Adjust as needed.

async def rate_limit_dependency(request: Dict[str, Any]):
    """Dependency to check the rate limit using the AI rate limiter."""

    request_data = {
        "ip_address": request.client.host, # Correctly get the IP
        "timestamp": time.time(),
        "uri": request.url.path, # Correctly get the URL
        "user_agent": request.headers.get("user-agent", "")
    }

    if ai_rate_limiter.check_rate_limit(request_data):
        raise HTTPException(status_code=429, detail="Rate limit exceeded")

    # Store the request data in Redis for future training
    RequestLog(ip_address=request_data["ip_address"], timestamp=request_data["timestamp"], uri=request_data["uri"], user_agent=request_data["user_agent"]).save()


@app.get("/", dependencies=[Depends(rate_limit_dependency)])
async def read_root(request: Dict[str, Any]):
    return {"Hello": "World"}

@app.get("/items/{item_id}", dependencies=[Depends(rate_limit_dependency)])
async def read_item(item_id: int, q: str = None, request: Dict[str, Any] = None):
    return {"item_id": item_id, "q": q}
```

**Explanation of the Integration:**

1.  **Import `AIRateLimiter`:** Import the `AIRateLimiter` class from `ai_rate_limiter.py`.
2.  **Initialize `AIRateLimiter`:** Create an instance of the `AIRateLimiter`, configuring the `retraining_interval`. Adjust this interval based on your API traffic volume and the desired responsiveness of the rate limiting system.
3.  **`rate_limit_dependency`:** This function acts as a dependency for your API endpoints. It's called before the endpoint logic is executed.
    - Extracts request data like IP address, timestamp, URI, and user agent.
    - Calls `ai_rate_limiter.check_rate_limit()` to determine if the request exceeds the rate limit.
    - If the rate limit is exceeded, raises an `HTTPException` with a 429 status code (Too Many Requests).
    - Stores the request data in Redis using the `RequestLog` model. This data will be used to retrain the AI model.
4.  **Add Dependencies to Endpoints:** Use the `dependencies` parameter of the `@app.get` decorator to apply the `rate_limit_dependency` to your API endpoints. This ensures that the rate limit check is performed before each request is processed.

**6. Running the Application**

Save the files and run the FastAPI application:

```bash
uvicorn main:app --reload
```

Now, send requests to your API. The AI rate limiter will analyze the traffic patterns and dynamically adjust the rate limits. Try simulating an attack by sending a large number of requests in a short period. The AI model should detect this anomalous behavior and start blocking requests.

**7. Further Enhancements and Considerations**

- **More Sophisticated Features:** Enhance the `extract_features` function to include more relevant features like:
  - Request size
  - Request method (GET, POST, etc.)
  - Specific API endpoint being accessed
  - User's past behavior
  - HTTP status codes returned
- **Alternative Models:** Experiment with other anomaly detection algorithms, such as:
  - One-Class SVM
  - Local Outlier Factor (LOF)
  - Autoencoders (for more complex feature extraction)
- **Adaptive Rate Limiting Policies:** Instead of just blocking requests, you can implement more nuanced rate limiting policies based on the severity of the anomaly:
  - Gradually increase the delay between requests.
  - Return a CAPTCHA challenge.
  - Throttle specific endpoints.
- **Real-time Retraining:** Consider retraining the model in real-time as new data becomes available. This can improve the model's accuracy and responsiveness. Be cautious about the computational cost of frequent retraining.
- **Monitoring and Alerting:** Implement monitoring and alerting to track the performance of the AI rate limiter and detect potential issues.
- **Explainability:** It's often helpful to understand why the AI model classified a request as anomalous. Techniques like SHAP (SHapley Additive exPlanations) can help you understand the feature importance and make the model more transparent.
- **Testing:** Thoroughly test your implementation under various traffic conditions, including normal usage, simulated attacks, and edge cases.
- **Model Persistence:** Use a more robust method for saving and loading your model, such as cloud storage (e.g., AWS S3, Google Cloud Storage) or a dedicated model registry.
- **Scalability:** For high-traffic APIs, consider using a distributed system for storing request data and running the AI model. Redis Cluster or a similar solution can provide the necessary scalability.
- **Database Choice:** Redis is good for speed, but lacks persistence and could be costly for high traffic scenarios. Consider using a persistent DB for request logging and only use Redis to store rate-limiting flags or short term data.

## Conclusion

AI-powered rate limiting provides a more sophisticated and effective defense against API abuse compared to traditional methods. By leveraging machine learning to detect abnormal traffic patterns, you can protect your API from attacks, minimize false positives, and dynamically adapt to changing traffic conditions. This blog post provided a practical guide to implementing AI-based rate limiting in FastAPI, demonstrating how to integrate machine learning models for real-time anomaly detection and adaptive rate limiting. Remember to tailor the implementation to your specific API requirements and continuously monitor and improve the performance of your AI rate limiter.

```

```
