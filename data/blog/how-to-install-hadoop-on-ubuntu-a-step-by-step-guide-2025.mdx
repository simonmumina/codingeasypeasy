---
title: 'How to Install Hadoop on Ubuntu: A Step-by-Step Guide (2025)'
date: '2025-01-27'
lastmod: '2025-01-27'
tags:
  [
    'hadoop',
    'ubuntu',
    'installation',
    'big data',
    'tutorial',
    'distributed computing',
    'setup',
    'guide',
  ]
draft: false
summary: 'A comprehensive guide to installing Hadoop on Ubuntu, covering prerequisites, configuration, and verification. Learn how to set up a single-node or multi-node Hadoop cluster on Ubuntu with detailed instructions and code examples.'
authors: ['default']
---

# How to Install Hadoop on Ubuntu: A Step-by-Step Guide (2025)

This guide provides a comprehensive, step-by-step walkthrough on installing Apache Hadoop on an Ubuntu system. Hadoop is a powerful open-source framework used for distributed storage and processing of large datasets. This tutorial caters to both beginners and experienced users who want to set up Hadoop on Ubuntu, whether for development, testing, or production environments. We'll cover everything from prerequisites to verification, ensuring you have a working Hadoop installation at the end.

## Prerequisites

Before we begin, ensure you have the following prerequisites in place:

- **Ubuntu Operating System:** A fresh installation of Ubuntu (preferably the latest LTS version) is recommended. This guide is tested with Ubuntu 22.04, but it should work with other recent versions as well.
- **Java Development Kit (JDK):** Hadoop requires Java to run. We recommend using Oracle JDK 8 or OpenJDK 8. Other JDK versions may work, but compatibility is best with version 8.
- **SSH Access:** You'll need SSH access to your Ubuntu server/machine for remote administration.
- **Basic Linux Command Line Knowledge:** Familiarity with common Linux commands is essential.
- **Sudo Privileges:** You'll need a user account with sudo privileges to perform system-level installations.

## Step 1: Installing Java (JDK)

First, let's install the Java Development Kit (JDK). We will use OpenJDK 8 for this guide.

1.  **Update Package Lists:**

    ```plaintext
    sudo apt update
    ```

2.  **Install OpenJDK 8:**

    ```plaintext
    sudo apt install openjdk-8-jdk
    ```

3.  **Verify Installation:**

    ```plaintext
    java -version
    ```

    This command should output the installed Java version, confirming a successful installation. It should show something like:

    ```
    openjdk version "1.8.0_392"
    OpenJDK Runtime Environment (build 1.8.0_392-8u392-ga-1~22.04.1)
    OpenJDK 64-Bit Server VM (build 25.392-b08, mixed mode)
    ```

4.  **Set JAVA_HOME:**

    Hadoop requires the `JAVA_HOME` environment variable to be set. Find the installation path of your Java. Usually, it is in `/usr/lib/jvm/java-8-openjdk-amd64`. If you are unsure where java is installed run:

    ```plaintext
    sudo update-alternatives --config java
    ```

    Copy the path, and then edit your `.bashrc` file (or `.zshrc` if you use Zsh):

    ```plaintext
    nano ~/.bashrc
    ```

    Add the following lines to the end of the file (replace `/usr/lib/jvm/java-8-openjdk-amd64` with your actual path if different):

    ```plaintext
    export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
    export PATH=$PATH:$JAVA_HOME/bin
    ```

    Save the file and source it to apply the changes:

    ```plaintext
    source ~/.bashrc
    ```

    Verify the `JAVA_HOME` variable:

    ```plaintext
    echo $JAVA_HOME
    ```

    This should output the path you set earlier.

## Step 2: Setting up SSH Access

Hadoop uses SSH for inter-node communication. You need to set up SSH access to your localhost.

1.  **Install SSH:**

    ```plaintext
    sudo apt install openssh-server
    ```

2.  **Generate SSH Key Pair:**

    ```plaintext
    ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa
    ```

    This command generates an RSA key pair without a passphrase. The `-P ''` option ensures there is no passphrase, allowing Hadoop to automatically connect.

3.  **Add Public Key to Authorized Keys:**

    ```plaintext
    cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
    chmod 0600 ~/.ssh/authorized_keys
    ```

4.  **Verify SSH Access to Localhost:**

    ```plaintext
    ssh localhost
    ```

    You should be able to connect to localhost without being prompted for a password. If you are prompted for a password, review the steps above to ensure the key setup is correct. If you get a warning about host authenticity, type "yes" to continue.

## Step 3: Downloading and Extracting Hadoop

Now, let's download and extract the Apache Hadoop distribution. We will download version 3.3.6, but you can download the latest stable version from the Apache Hadoop website ([https://hadoop.apache.org/releases.html](https://hadoop.apache.org/releases.html)).

1.  **Download Hadoop:**

    First, create a directory to store downloaded files

    ```plaintext
    mkdir ~/downloads
    cd ~/downloads
    ```

    Use `wget` to download the binary distribution:

    ```plaintext
    wget https://dlcdn.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz
    ```

    You can also use `curl`

    ```plaintext
    curl -O https://dlcdn.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz
    ```

    **Important:** Always verify the integrity of the downloaded file using checksums. You can find the checksums on the Apache Hadoop website.

2.  **Extract Hadoop:**

    ```plaintext
    tar -xzf hadoop-3.3.6.tar.gz
    ```

3.  **Move Hadoop to a Desirable Location:**

    Move the extracted Hadoop directory to `/usr/local/hadoop`:

    ```plaintext
    sudo mv hadoop-3.3.6 /usr/local/hadoop
    sudo chown -R $USER:$USER /usr/local/hadoop
    ```

## Step 4: Configuring Hadoop

Now comes the crucial part: configuring Hadoop. We'll modify several configuration files in the `/usr/local/hadoop/etc/hadoop` directory.

1.  **`hadoop-env.sh`:**

    Edit `hadoop-env.sh` and set the `JAVA_HOME` variable.

    ```plaintext
    nano /usr/local/hadoop/etc/hadoop/hadoop-env.sh
    ```

    Add the following line to the file (replace `/usr/lib/jvm/java-8-openjdk-amd64` with the actual path to your Java installation):

    ```plaintext
    export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
    ```

2.  **`core-site.xml`:**

    Edit `core-site.xml` and configure the `fs.defaultFS` property.

    ```plaintext
    nano /usr/local/hadoop/etc/hadoop/core-site.xml
    ```

    Add the following configuration within the `<configuration>` tags:

    ```plaintext
    <configuration>
        <property>
            <name>fs.defaultFS</name>
            <value>hdfs://localhost:9000</value>
        </property>
    </configuration>
    ```

    This specifies the default file system for Hadoop. `hdfs://localhost:9000` indicates that we're using the Hadoop Distributed File System (HDFS) running on the local machine at port 9000.

3.  **`hdfs-site.xml`:**

    Edit `hdfs-site.xml` to configure the namenode and datanode directories.

    ```plaintext
    nano /usr/local/hadoop/etc/hadoop/hdfs-site.xml
    ```

    Add the following configuration within the `<configuration>` tags:

    ```plaintext
    <configuration>
        <property>
            <name>dfs.replication</name>
            <value>1</value>
        </property>
        <property>
            <name>dfs.namenode.name.dir</name>
            <value>file:/usr/local/hadoop/tmp/dfs/namenode</value>
        </property>
        <property>
            <name>dfs.datanode.data.dir</name>
            <value>file:/usr/local/hadoop/tmp/dfs/datanode</value>
        </property>
    </configuration>
    ```

    - `dfs.replication`: Set to 1 because we are running a single node cluster. In a multi-node cluster, you'd typically set this to 3 for data redundancy.
    - `dfs.namenode.name.dir`: Specifies the directory where the namenode stores its metadata.
    - `dfs.datanode.data.dir`: Specifies the directory where the datanode stores data blocks.

    **Important:** Create the directories specified in `hdfs-site.xml`:

    ```plaintext
    mkdir -p /usr/local/hadoop/tmp/dfs/namenode
    mkdir -p /usr/local/hadoop/tmp/dfs/datanode
    ```

    And give proper permissions:

    ```plaintext
    sudo chown -R $USER:$USER /usr/local/hadoop/tmp
    ```

4.  **`mapred-site.xml`:**

    Copy the template file `mapred-site.xml.template` to `mapred-site.xml` and then edit it.

    ```plaintext
    cp /usr/local/hadoop/etc/hadoop/mapred-site.xml.template /usr/local/hadoop/etc/hadoop/mapred-site.xml
    nano /usr/local/hadoop/etc/hadoop/mapred-site.xml
    ```

    Add the following configuration within the `<configuration>` tags:

    ```plaintext
    <configuration>
        <property>
            <name>mapreduce.framework.name</name>
            <value>yarn</value>
        </property>
    </configuration>
    ```

    This specifies that we'll use YARN (Yet Another Resource Negotiator) as the MapReduce framework.

5.  **`yarn-site.xml`:**

    Edit `yarn-site.xml`.

    ```plaintext
    nano /usr/local/hadoop/etc/hadoop/yarn-site.xml
    ```

    Add the following configuration within the `<configuration>` tags:

    ```plaintext
    <configuration>
        <property>
            <name>yarn.nodemanager.aux-services</name>
            <value>mapreduce_shuffle</value>
        </property>
        <property>
            <name>yarn.nodemanager.env-whitelist</name>
            <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH,PATH,HADOOP_YARN_HOME</value>
        </property>
    </configuration>
    ```

## Step 5: Formatting the Namenode

Before starting Hadoop, you need to format the namenode. **This step should only be done _once_ when you first set up your Hadoop cluster.** Formatting the namenode clears its metadata and prepares it for operation.

```plaintext
/usr/local/hadoop/bin/hdfs namenode -format
```

**Important:** If you see "Storage directory /usr/local/hadoop/tmp/dfs/namenode does not exist" errors, make sure you created the directories in `hdfs-site.xml` and gave proper permissions.

## Step 6: Starting Hadoop

Now you're ready to start Hadoop.

1.  **Start HDFS:**

    ```plaintext
    /usr/local/hadoop/sbin/start-dfs.sh
    ```

    This script starts the namenode and datanode daemons.

2.  **Start YARN:**

    ```plaintext
    /usr/local/hadoop/sbin/start-yarn.sh
    ```

    This script starts the resourcemanager and nodemanager daemons.

## Step 7: Verifying the Installation

Let's verify that Hadoop is running correctly.

1.  **Check JPS (Java Process Status):**

    Run the `jps` command:

    ```plaintext
    jps
    ```

    You should see processes named `NameNode`, `DataNode`, `ResourceManager`, and `NodeManager` running. If any of these processes are missing, check the logs in the `/usr/local/hadoop/logs` directory for error messages.

2.  **Access the Hadoop Web UI:**

    Open your web browser and navigate to the following URLs:

    - **Namenode:** `http://localhost:9870`
    - **ResourceManager:** `http://localhost:8088`

    These web UIs provide information about the status of your Hadoop cluster. You can monitor the health of the nodes, view the file system, and track running jobs.

## Step 8: Running a Sample MapReduce Job

Let's run a sample MapReduce job to test the Hadoop installation. Hadoop provides several example jobs in the `/usr/local/hadoop/share/hadoop/mapreduce` directory.

1.  **Create an Input Directory in HDFS:**

    ```plaintext
    /usr/local/hadoop/bin/hdfs dfs -mkdir /input
    ```

2.  **Upload Input Data to HDFS:**

    Create a sample text file named `input.txt`:

    ```plaintext
    echo "This is a test file for Hadoop." > input.txt
    echo "Another line in the test file." >> input.txt
    ```

    Upload the file to the `/input` directory in HDFS:

    ```plaintext
    /usr/local/hadoop/bin/hdfs dfs -put input.txt /input
    ```

3.  **Run the WordCount Example:**

    ```plaintext
    /usr/local/hadoop/bin/hadoop jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.6.jar wordcount /input /output
    ```

    This command runs the WordCount example, reading data from the `/input` directory and writing the output to the `/output` directory. Replace `hadoop-mapreduce-examples-3.3.6.jar` with the correct name of the example JAR file if you downloaded a different version of Hadoop.

4.  **View the Output:**

    ```plaintext
    /usr/local/hadoop/bin/hdfs dfs -cat /output/part-r-00000
    ```

    This command displays the output of the WordCount job, which should show the frequency of each word in the input file.

## Step 9: Stopping Hadoop

When you're finished using Hadoop, you can stop the daemons.

1.  **Stop YARN:**

    ```plaintext
    /usr/local/hadoop/sbin/stop-yarn.sh
    ```

2.  **Stop HDFS:**

    ```plaintext
    /usr/local/hadoop/sbin/stop-dfs.sh
    ```

## Conclusion

Congratulations! You have successfully installed and configured Hadoop on your Ubuntu system. You can now start exploring the capabilities of Hadoop for big data processing and analysis. Remember to refer to the Apache Hadoop documentation for more advanced configurations and features. This guide provides a solid foundation for further exploration and experimentation with Hadoop. Good luck!
