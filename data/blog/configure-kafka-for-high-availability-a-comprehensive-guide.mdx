---
title: 'Configure Kafka for High Availability: A Comprehensive Guide'
date: '2024-02-29'
lastmod: '2024-02-29'
tags:
  [
    'kafka',
    'high availability',
    'fault tolerance',
    'distributed systems',
    'zookeeper',
    'replication',
    'cluster configuration',
    'kafka configuration',
  ]
draft: false
summary: 'Learn how to configure Apache Kafka for high availability. This comprehensive guide covers essential concepts, configuration settings, and best practices to ensure your Kafka cluster remains resilient and available even in the face of failures.'
authors: ['default']
---

# Configuring Kafka for High Availability: A Comprehensive Guide

Apache Kafka is a powerful distributed streaming platform used for building real-time data pipelines and streaming applications. One of the key features that makes Kafka so robust is its ability to be configured for high availability (HA). This means ensuring your Kafka cluster remains operational and continues to process messages even when individual brokers or Zookeeper nodes fail.

This guide will walk you through the essential concepts and configuration settings required to set up a highly available Kafka cluster. We'll cover topics like replication, ZooKeeper setup, broker configuration, and best practices to ensure your Kafka infrastructure is resilient and fault-tolerant.

## Understanding the Key Concepts

Before diving into the configuration, let's establish a firm understanding of the core concepts that underpin Kafka's HA capabilities:

- **Brokers:** Kafka brokers are the nodes that form the Kafka cluster. They are responsible for storing and serving data.
- **Topics:** Topics are logical categories used to organize messages. Each topic is partitioned into one or more partitions.
- **Partitions:** Partitions are the fundamental units of parallelism and data distribution in Kafka. Each partition is an ordered, immutable sequence of messages.
- **Replication:** Replication is the process of copying data from one partition (the leader) to multiple other partitions (followers). This ensures that if a leader fails, a follower can take over without data loss.
- **ZooKeeper:** ZooKeeper is a distributed coordination service that Kafka uses to manage the cluster state, including broker leadership, partition assignments, and configuration.
- **Leader and Followers:** For each partition, one broker is designated as the leader, and the others are followers. The leader handles all read and write requests for that partition. Followers replicate the data from the leader.
- **ISR (In-Sync Replicas):** The ISR set consists of followers that are currently up-to-date with the leader. Only members of the ISR are eligible to become leader if the current leader fails, ensuring data consistency.

## Setting up a Highly Available Kafka Cluster: Step-by-Step

Here's a step-by-step guide to configuring Kafka for high availability:

**1. ZooKeeper Configuration:**

ZooKeeper is critical for Kafka's operation and must also be configured for high availability. A ZooKeeper ensemble consists of multiple ZooKeeper servers (typically an odd number like 3, 5, or 7). This ensures that even if some ZooKeeper servers fail, the cluster can still function.

- **Configuration (zoo.cfg):** On each ZooKeeper server, configure the `zoo.cfg` file. Here's an example for a 3-node ZooKeeper ensemble:

  ```properties
  tickTime=2000
  dataDir=/var/lib/zookeeper
  clientPort=2181
  initLimit=10
  syncLimit=5
  server.1=zoo1:2888:3888
  server.2=zoo2:2888:3888
  server.3=zoo3:2888:3888
  ```

  - `tickTime`: The basic time unit in milliseconds.
  - `dataDir`: The directory where ZooKeeper stores its data.
  - `clientPort`: The port on which ZooKeeper listens for client connections (Kafka brokers).
  - `initLimit`: The maximum number of ticks the followers have to connect and sync to the leader.
  - `syncLimit`: The maximum number of ticks a follower can fall behind a leader.
  - `server.X`: Defines each ZooKeeper server. `X` is a unique integer. `zooX` is the hostname (or IP address) of the server. `2888` is the port used for follower to leader communication, and `3888` is the port used for leader election.

- **myid file:** On each ZooKeeper server, create a file named `myid` in the `dataDir` directory containing the server's ID (the `X` from `server.X` in the `zoo.cfg` file). For example, on `zoo1`, the `myid` file would contain:

  ```
  1
  ```

- **Start ZooKeeper:** Start the ZooKeeper service on each server.

**2. Kafka Broker Configuration:**

The key to Kafka's HA lies in the broker configuration, particularly the replication factor.

- **Configuration (server.properties):** Configure the `server.properties` file on each Kafka broker. Here are the crucial settings for HA:

  ```properties
  broker.id=0  # Unique ID for each broker (e.g., 0, 1, 2)
  listeners=PLAINTEXT://your_broker_hostname:9092 # or replace with your actual hostname/IP
  advertised.listeners=PLAINTEXT://your_broker_hostname:9092 # Brokers advertise this to clients
  zookeeper.connect=zoo1:2181,zoo2:2181,zoo3:2181  # List of ZooKeeper servers
  num.partitions=3  # Default number of partitions per topic
  default.replication.factor=3  # Default replication factor for topics
  min.insync.replicas=2  # Minimum number of in-sync replicas required for a write to be considered successful

  log.dirs=/tmp/kafka-logs # Directory where Kafka stores its logs
  log.retention.hours=168  # Retention policy (in hours)
  log.segment.bytes=1073741824 # 1 GB
  ```

  - `broker.id`: Assign a unique integer to each broker in the cluster.
  - `listeners`: Specifies the interfaces and ports on which the broker listens for connections.
  - `advertised.listeners`: The external listeners that clients will use to connect to the broker. This is especially important when using Kafka with Docker or in cloud environments. The brokers advertise these addresses to clients.
  - `zookeeper.connect`: A comma-separated list of ZooKeeper servers (host:port).
  - `num.partitions`: Sets the default number of partitions for new topics. More partitions generally allow for higher throughput.
  - `default.replication.factor`: Specifies the default number of replicas for each partition. A replication factor of 3 means each partition will have three copies of its data, providing good fault tolerance.
  - `min.insync.replicas`: This is arguably one of the MOST important configurations for data safety. It specifies the minimum number of in-sync replicas (ISR) that must be present before a producer can successfully write to a partition. Setting this to a value greater than 1 ensures that data is written to multiple replicas before being acknowledged, preventing data loss if the leader fails before replicating the data. When setting this, consider the formula `replication.factor > min.insync.replicas`. Usually `replication.factor = min.insync.replicas + 1` or greater is recommended. For example, if your `replication.factor` is `3`, `min.insync.replicas` should be `2`. This means that even if one broker goes down, you still have two in-sync replicas to serve the data.
  - `log.dirs`: The directory where Kafka stores its log data. This can be a comma-separated list of directories for better performance and storage capacity. Make sure the user Kafka runs under has read and write permissions.
  - `log.retention.hours`: Sets the amount of time Kafka will retain log data before deleting it.
  - `log.segment.bytes`: Sets the maximum size of a log segment file.

- **Start Kafka Brokers:** Start the Kafka service on each broker.

**3. Topic Creation:**

When creating topics, ensure you specify the desired replication factor and number of partitions.

- **Using kafka-topics.sh:**

  ```plaintext
  ./kafka-topics.sh --create --topic my-topic --partitions 6 --replication-factor 3 --zookeeper zoo1:2181,zoo2:2181,zoo3:2181
  ```

  This creates a topic named "my-topic" with 6 partitions and a replication factor of 3. This means each partition will be replicated across three different brokers.

**4. Producer Configuration:**

Configure your Kafka producers to be aware of the `min.insync.replicas` setting on the brokers. You can do this by setting the `acks` configuration.

- **acks Configuration:**

  - `acks=0`: The producer does not wait for any acknowledgment from the broker. This offers the highest throughput but the lowest guarantee of delivery (data loss is possible).
  - `acks=1`: The producer waits for acknowledgment from the leader broker. This offers a balance between throughput and reliability. Data loss is still possible if the leader fails before followers have replicated the data.
  - `acks=all` or `acks=-1`: The producer waits for acknowledgment from all in-sync replicas (as defined by `min.insync.replicas`). This provides the strongest guarantee of delivery (no data loss) but has the lowest throughput. This is the recommended setting for HA.

  Here's a Java producer example:

  ```plaintext
  import org.apache.kafka.clients.producer.*;
  import java.util.Properties;

  public class KafkaProducerExample {
      public static void main(String[] args) {

          Properties props = new Properties();
          props.put("bootstrap.servers", "your_broker_hostname:9092");
          props.put("acks", "all");  // Ensure all in-sync replicas acknowledge
          props.put("retries", 3);  // Retry failed sends
          props.put("batch.size", 16384);
          props.put("linger.ms", 1);
          props.put("buffer.memory", 33554432);
          props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
          props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");

          Producer<String, String> producer = new KafkaProducer<>(props);
          for (int i = 0; i < 100; i++) {
              producer.send(new ProducerRecord<>("my-topic", Integer.toString(i), "message-" + i),
                      (metadata, exception) -> {
                          if (exception != null) {
                              System.err.println("Failed to send message: " + exception.getMessage());
                          } else {
                              System.out.println("Sent message: " + i + " to partition: " + metadata.partition());
                          }
                      });
          }

          producer.close();
      }
  }
  ```

  Key points in the code:

  - `acks = "all"`: Specifies that the producer waits for acknowledgment from all in-sync replicas.
  - `retries = 3`: Configures the producer to retry sending messages in case of transient errors.

**5. Consumer Configuration:**

Consumer configuration is less directly related to high availability but can impact overall system resilience.

- **Group ID:** Consumers should be part of a consumer group to allow for parallel processing of partitions. Kafka ensures that each partition is consumed by only one consumer within a group.
- **Enable Auto Commit:** Set `enable.auto.commit` to `true` (default) or control commits manually for more fine-grained control over message processing. If set to `true`, use `auto.offset.reset` to control where to start consuming from if no offset exists or the offset is out of range:
  - `auto.offset.reset="earliest"`: automatically reset the offset to the earliest offset
  - `auto.offset.reset="latest"`: automatically reset the offset to the latest offset
  - `auto.offset.reset="none"`: throw exception to the consumer if no initial offset is found for the consumer's group.
- **Configure Max Poll Records:** Tweak `max.poll.records` to tune your consumer's retrieval and processing abilities.

  ```plaintext
  import org.apache.kafka.clients.consumer.ConsumerConfig;
  import org.apache.kafka.clients.consumer.ConsumerRecord;
  import org.apache.kafka.clients.consumer.ConsumerRecords;
  import org.apache.kafka.clients.consumer.KafkaConsumer;
  import org.apache.kafka.common.serialization.StringDeserializer;

  import java.time.Duration;
  import java.util.Arrays;
  import java.util.Properties;

  public class KafkaConsumerExample {
      public static void main(String[] args) {

          String topicName = "my-topic";
          String groupId = "my-group";

          Properties props = new Properties();
          props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "your_broker_hostname:9092");
          props.put(ConsumerConfig.GROUP_ID_CONFIG, groupId);
          props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
          props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
          props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest"); // or "latest"
          props.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, 500); // Adjust as needed

          KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props);
          consumer.subscribe(Arrays.asList(topicName));

          try {
              while (true) {
                  ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(100));
                  for (ConsumerRecord<String, String> record : records) {
                      System.out.printf("offset = %d, key = %s, value = %s%n", record.offset(), record.key(), record.value());
                  }
              }
          } catch (Exception e) {
              System.err.println("Error during consumption: " + e.getMessage());
          } finally {
              consumer.close();
          }
      }
  }
  ```

  Key points in the code:

  - `ConsumerConfig.GROUP_ID_CONFIG`: Consumers sharing the same group ID will consume messages in parallel.
  - `ConsumerConfig.AUTO_OFFSET_RESET_CONFIG`: Determines what to do when the consumer starts reading a partition for the first time or when the latest offset is no longer available on the server.
  - `ConsumerConfig.MAX_POLL_RECORDS_CONFIG`: Maximum number of records that the consumer will poll at one time.

**6. Monitoring and Alerting:**

Implement monitoring and alerting to detect failures and performance issues proactively. Monitor key metrics like:

- **Broker availability:** Ensure brokers are online and responsive.
- **Replication lag:** Monitor the time it takes for followers to catch up with the leader. High replication lag can indicate problems with network connectivity or broker performance.
- **Under-replicated partitions:** Track the number of partitions that don't have the desired number of replicas.
- **ZooKeeper health:** Monitor the health of the ZooKeeper ensemble.
- **Message throughput:** Track the rate at which messages are being produced and consumed.
- **CPU and memory usage:** Monitor CPU and memory utilization on brokers and ZooKeeper servers.

Tools like Prometheus and Grafana can be used to visualize these metrics and set up alerts. Kafka also exposes metrics through JMX, which can be monitored using tools like JConsole or VisualVM.

**7. Testing Failover:**

Regularly test your Kafka cluster's failover capabilities by simulating failures (e.g., stopping a broker, ZooKeeper node, or network partition). This helps to verify that your HA configuration is working as expected.

**8. Best Practices:**

- **Sizing Your Cluster:** Properly size your Kafka cluster based on your expected throughput, storage requirements, and fault tolerance needs.
- **Network Configuration:** Ensure you have a reliable and high-bandwidth network connection between brokers and ZooKeeper nodes.
- **Disk Configuration:** Use fast and reliable disks for Kafka's log directories. Consider using RAID configurations for increased redundancy.
- **Regular Backups:** Implement a backup and recovery strategy for your Kafka data.
- **Rolling Restarts:** When performing maintenance or upgrades, use rolling restarts to minimize downtime. Bring down one broker at a time, upgrade it, and then bring it back online before moving on to the next broker.
- **Security:** Secure your Kafka cluster with authentication and authorization to prevent unauthorized access.

## Conclusion

Configuring Kafka for high availability is crucial for ensuring the reliability and resilience of your data streaming applications. By following the steps and best practices outlined in this guide, you can build a robust Kafka infrastructure that can withstand failures and continue to deliver data reliably. Remember to continuously monitor your cluster and adapt your configuration as your needs evolve. By focusing on replication, ZooKeeper configuration, producer `acks`, and rigorous monitoring, you can ensure a highly available and dependable Kafka platform.
