---
title: 'Build a Full-Featured, Hackable Next.js AI Chatbot with OpenAI and Vercel AI SDK'
date: '2024-10-27'
lastmod: '2024-10-27'
tags:
  [
    'nextjs',
    'ai chatbot',
    'openai',
    'vercel ai sdk',
    'react',
    'javascript',
    'natural language processing',
    'chatbot tutorial',
    'custom chatbot',
  ]
draft: false
summary: 'Learn how to create a customizable and powerful AI chatbot using Next.js, OpenAI, and the Vercel AI SDK. This comprehensive guide covers everything from setting up your environment to deploying a fully functional AI assistant.'
authors: ['default']
---

# Build a Full-Featured, Hackable Next.js AI Chatbot with OpenAI and Vercel AI SDK

In this comprehensive guide, we'll walk you through building a powerful and customizable AI chatbot using Next.js, OpenAI's GPT models, and the Vercel AI SDK. This combination allows us to create a robust, real-time conversational AI experience that's easy to deploy and scale. We'll cover everything from setting up your development environment to deploying a fully functional AI assistant. This tutorial assumes you have a basic understanding of Next.js, React, and JavaScript.

## Why Next.js, OpenAI, and Vercel AI SDK?

- **Next.js:** Provides server-side rendering, API routes, and optimized performance for a smooth user experience. Its file-based routing makes creating and managing your chatbot's pages straightforward.
- **OpenAI:** Offers powerful language models like GPT-3.5 Turbo and GPT-4, capable of generating human-quality text, understanding context, and responding intelligently to user input. We'll leverage their API for our chatbot's core intelligence.
- **Vercel AI SDK:** Simplifies the integration of AI models into your Next.js applications. It provides helpful hooks and components for streaming responses, handling errors, and managing the chat state, allowing you to focus on the chatbot's logic and user interface.

## Prerequisites

Before we begin, ensure you have the following:

- **Node.js (v18.0 or higher):** Download and install the latest stable version from [nodejs.org](https://nodejs.org/).
- **npm or yarn:** Package managers for JavaScript. npm comes with Node.js; yarn can be installed via npm (`npm install -g yarn`).
- **Vercel CLI:** For deployment. Install globally with `npm install -g vercel` or `yarn global add vercel`.
- **OpenAI API Key:** You'll need an API key from OpenAI. Create an account at [openai.com](https://openai.com/) and generate a key from your dashboard. **Treat this key securely and never expose it in your client-side code!**
- **Vercel Account (Optional, but recommended):** For easy deployment and hosting. Create a free account at [vercel.com](https://vercel.com/).

## Step 1: Setting Up Your Next.js Project

Let's start by creating a new Next.js project:

```plaintext
npx create-next-app my-ai-chatbot
cd my-ai-chatbot
```

This command sets up a basic Next.js project named `my-ai-chatbot`. Feel free to choose a different name.

Next, install the necessary dependencies:

```plaintext
npm install openai @vercel/ai react-markdown react-syntax-highlighter
# or
yarn add openai @vercel/ai react-markdown react-syntax-highlighter
```

- **openai:** The official OpenAI Node.js library for interacting with the OpenAI API.
- **@vercel/ai:** The Vercel AI SDK, providing tools for streaming AI responses and simplifying integration.
- **react-markdown:** For rendering Markdown formatted responses from the AI model. This allows the chatbot to format code snippets and provide clear, readable answers.
- **react-syntax-highlighter:** For beautifully syntax highlighting code blocks within the markdown output.

## Step 2: Creating the API Route

We'll create an API route to handle the communication with the OpenAI API. This keeps your API key secure and allows you to perform server-side logic.

Create a file named `app/api/chat/route.ts` (or `app/api/chat/route.js` if you prefer JavaScript). This file will handle the incoming requests, interact with the OpenAI API, and stream the responses back to the client.

```plaintext
// app/api/chat/route.ts
import OpenAI from 'openai'
import { OpenAIStream, StreamingTextResponse } from 'ai'

// Optional, but recommended: Define the runtime to be edge for best performance
export const runtime = 'edge'

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
})

export async function POST(req: Request): Promise<Response> {
  const { messages } = await req.json()

  // Ask OpenAI for a streaming chat completion given the prompt
  const response = await openai.chat.completions.create({
    model: 'gpt-3.5-turbo', // Or gpt-4 if you have access
    stream: true,
    messages,
  })

  // Convert the response into a friendly text-stream
  const stream = OpenAIStream(response)
  // Respond with the stream
  return new StreamingTextResponse(stream)
}
```

**Explanation:**

- **`runtime = 'edge';`**: This line configures the API route to run on Vercel's Edge Functions for faster performance and lower latency.
- **`import OpenAI from 'openai';`**: Imports the OpenAI library.
- **`import { OpenAIStream, StreamingTextResponse } from 'ai';`**: Imports the necessary functions from the Vercel AI SDK for streaming responses.
- **`const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });`**: Creates an OpenAI client instance, using your API key from environment variables. **Remember to set your `OPENAI_API_KEY` environment variable!**
- **`export async function POST(req: Request): Promise<Response> { ... }`**: Defines the `POST` handler for the API route. It receives the chat messages from the client in the request body.
- **`const { messages } = await req.json();`**: Extracts the `messages` array from the request body. This array will contain the conversation history.
- **`const response = await openai.chat.completions.create({ ... });`**: Calls the OpenAI API to generate a chat completion. We specify the model to use (`gpt-3.5-turbo` or `gpt-4`), enable streaming (`stream: true`), and pass the conversation history (`messages`).
- **`const stream = OpenAIStream(response);`**: Converts the OpenAI API response into a readable stream using `OpenAIStream` from the Vercel AI SDK.
- **`return new StreamingTextResponse(stream);`**: Returns the stream as a `StreamingTextResponse`, allowing the client to receive the AI's response in real-time.

**Important:** Set your `OPENAI_API_KEY` environment variable. You can do this in your `.env.local` file (remember to add `.env.local` to your `.gitignore` file to prevent accidental commits!) or through your hosting provider's environment variables settings (e.g., Vercel).

```
// .env.local
OPENAI_API_KEY=YOUR_OPENAI_API_KEY
```

## Step 3: Building the Chatbot UI

Now, let's create the user interface for the chatbot. We'll use React components and hooks to manage the chat state and display the messages.

Create a new component called `components/Chat.tsx` (or `components/Chat.jsx` if using JavaScript).

```plaintext jsx
// components/Chat.tsx
'use client';

import { useState, useEffect, useRef } from 'react';
import { useChat } from 'ai/react';
import ReactMarkdown from 'react-markdown';
import { Prism as SyntaxHighlighter } from 'react-syntax-highlighter';
import { dracula } from 'react-syntax-highlighter/dist/esm/styles/prism';

const Chat = () => {
  const { messages, input, handleInputChange, handleSubmit, isLoading, setMessages } = useChat({
    api: '/api/chat',
  });

  const [mounted, setMounted] = useState(false);
  const chatContainerRef = useRef<HTMLDivElement>(null);


  useEffect(() => {
    setMounted(true);
    // Scroll to bottom on new messages
    if (chatContainerRef.current) {
      chatContainerRef.current.scrollTop = chatContainerRef.current.scrollHeight;
    }
  }, [messages]);

  if (!mounted) {
    return null; // or a loading indicator
  }

  const markdownRenderers = {
    code({ node, inline, className, children, ...props }) {
      const match = /language-(\w+)/.exec(className || '')
      return !inline && match ? (
        <SyntaxHighlighter
          style={dracula}
          language={match[1]}
          PreTag="div"
          children={String(children).replace(/\n$/, '')}
          {...props}
        />
      ) : (
        <code className={className} {...props}>
          {children}
        </code>
      )
    }
  }

  return (
    <div className="flex flex-col h-screen">
      <div className="flex-1 overflow-y-auto p-4" ref={chatContainerRef}>
        {messages.map((message) => (
          <div key={message.id} className={`mb-2 ${message.role === 'user' ? 'text-right' : 'text-left'}`}>
            <span className={`inline-block p-2 rounded-lg ${message.role === 'user' ? 'bg-blue-500 text-white' : 'bg-gray-200 text-gray-800'}`}>
              <ReactMarkdown
                components={markdownRenderers}
              >
                {message.content}
              </ReactMarkdown>
            </span>
          </div>
        ))}
        {isLoading && <div className="text-left">Thinking...</div>}
      </div>

      <form onSubmit={handleSubmit} className="p-4">
        <div className="flex">
          <input
            className="flex-1 p-2 border border-gray-300 rounded-md focus:outline-none focus:ring-2 focus:ring-blue-500"
            value={input}
            placeholder="Ask me anything..."
            onChange={handleInputChange}
          />
          <button
            type="submit"
            className="ml-2 px-4 py-2 bg-blue-500 text-white rounded-md hover:bg-blue-600 focus:outline-none focus:ring-2 focus:ring-blue-500"
            disabled={isLoading}
          >
            {isLoading ? 'Sending...' : 'Send'}
          </button>
        </div>
      </form>
    </div>
  );
};

export default Chat;
```

**Explanation:**

- **`'use client';`**: Marks this component as a client-side component, meaning it will be rendered in the browser.
- **`import { useChat } from 'ai/react';`**: Imports the `useChat` hook from the Vercel AI SDK.
- **`const { messages, input, handleInputChange, handleSubmit, isLoading, setMessages } = useChat({ api: '/api/chat' });`**: Uses the `useChat` hook to manage the chat state. It provides:
  - `messages`: An array of chat messages.
  - `input`: The current input text in the input field.
  - `handleInputChange`: A function to update the input text.
  - `handleSubmit`: A function to submit the message to the API.
  - `isLoading`: A boolean indicating whether the API is currently processing a request.
  - `setMessages`: Allows directly manipulating the messages array.
- **`api: '/api/chat'`**: Configures the `useChat` hook to use the `/api/chat` API route we created earlier.
- **`messages.map((message) => ...)`**: Iterates over the `messages` array and renders each message.
- **`ReactMarkdown`**: Renders the message content as Markdown, allowing for formatted text, lists, and code snippets.
- **`react-syntax-highlighter`**: Render code blocks with syntax highlighting. The `markdownRenderers` object tells ReactMarkdown how to render `code` blocks, using syntax highlighter.
- **Input Field and Send Button**: Provides the user interface for entering and sending messages.
- **`isLoading` State**: Displays "Thinking..." or "Sending..." while the API is processing the request and disables the send button.
- **`useEffect` hook for scrolling:** Scrolls the chat window to the bottom whenever a new message is added, ensuring the user sees the latest content.
- **`mounted` state for hydration:** Prevents rendering before the component is properly mounted on the client. This avoids hydration errors.

**Styling:**

The code also includes some basic styling using Tailwind CSS classes. Make sure you have Tailwind CSS configured in your Next.js project. You can install it using:

```plaintext
npm install -D tailwindcss postcss autoprefixer
npx tailwindcss init -p
```

And configure your `tailwind.config.js` and `globals.css` files according to the Tailwind CSS documentation ([tailwindcss.com](https://tailwindcss.com/)).

## Step 4: Integrating the Chat Component into Your Page

Now, let's integrate the `Chat` component into your main page. Open `app/page.tsx` (or `app/page.jsx`) and replace its content with the following:

```plaintext jsx
// app/page.tsx
import Chat from './components/Chat';

export default function Home() {
  return (
    <main className="flex flex-col items-center justify-center min-h-screen bg-gray-100">
      <h1 className="text-4xl font-bold mb-8">Next.js AI Chatbot</h1>
      <div className="w-full max-w-2xl bg-white rounded-lg shadow-md overflow-hidden">
        <Chat />
      </div>
    </main>
  );
}
```

This code imports the `Chat` component and renders it within a container.

## Step 5: Running Your Chatbot

Now you can run your chatbot locally:

```plaintext
npm run dev
# or
yarn dev
```

Open your browser and navigate to `http://localhost:3000`. You should see your chatbot UI. Type a message and send it. You should see the AI's response appear in real-time.

## Step 6: Deployment (Optional but Recommended)

To deploy your chatbot to Vercel, follow these steps:

1.  **Commit your code to a Git repository (e.g., GitHub, GitLab, Bitbucket).**
2.  **Run `vercel` in your project directory.** This will prompt you to log in to your Vercel account and link your project.
3.  **Vercel will automatically deploy your application.**

Make sure you have set the `OPENAI_API_KEY` environment variable in your Vercel project settings.

## Customization and Hacking

This is just a basic implementation. The real power lies in the ability to customize and extend this chatbot. Here are some ideas:

- **Context Management:** Implement more sophisticated context management beyond just the message history. You could store user preferences, past interactions, or external data to provide more personalized and relevant responses.
- **Function Calling:** Utilize OpenAI's function calling capabilities to allow the chatbot to perform actions on behalf of the user, such as fetching data from an API, sending emails, or scheduling appointments.
- **Custom Prompts:** Fine-tune the prompts used to interact with the OpenAI API to control the chatbot's personality, tone, and behavior. You could create a chatbot that specializes in a particular domain, such as customer service or technical support.
- **Authentication:** Add authentication to allow users to save their chat history and personalize their experience.
- **Styling:** Customize the appearance of the chatbot to match your brand using Tailwind CSS or other styling libraries.
- **Error Handling:** Implement more robust error handling to gracefully handle API errors, network issues, and unexpected input.
- **Moderation:** Integrate content moderation to filter out inappropriate or harmful content.
- **Database Integration:** Store conversation history and user data in a database for persistence and analysis.

## Conclusion

In this guide, we've built a full-featured AI chatbot using Next.js, OpenAI, and the Vercel AI SDK. This is a powerful foundation that you can build upon to create a truly personalized and intelligent conversational AI experience. Experiment with different models, customize the prompts, and integrate other services to create a chatbot that meets your specific needs. Happy hacking!
